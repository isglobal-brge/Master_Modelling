[["index.html", "Data Visualisation and Modelling 1 Introducción", " Data Visualisation and Modelling Juan R González 2021-10-14 1 Introducción This bookdown is written as a supporting material for some of the lectures belonging to the subject Data Visualisation and Modelling given at Master for Modelling in Science and Engineering from Autonomous University of Barelona (UAB). The contents are: Dealing with big data analysis in R Parallelization in R MapReduce Linear regression for Big Data Model fitting Multivariate linear regression General rules for variable selection Stepwise variable selection Comparing models Automatic variable selection Cross validation K-fold and bootstrap cross validation Missing data imputation Regularization (Lasso and Elastic Net) DataSHIELD Some parts of this material are inspired in vignettes that are referenced at each chapter. Examples corresponding to model fitting are based on material from Jeff Webb. This material is licensed under Creative Commons Attribution 4.0 International License. "],["dealing-with-big-data-in-r.html", "2 Dealing with Big Data in R 2.1 Nodes, cores, processes and threads 2.2 Paralelización 2.3 MapReduce 2.4 Linear regression for Big Data", " 2 Dealing with Big Data in R In the era of big data, we can have access to large volumes of data obtained from our population of interest. Traditional algorithms implemented for even the simplest statistical methods (descriptive, linear regression, ) require a lot of computing time. To address this issue, one of the approaches we have is to divide our data into smaller sets that do not require as much computational cost and to combine these results in a clever way that allows us to solve the largest problem. This can be done by parallelizing the calculations since even laptops already have multiple computing cores, and/or combining the calculations with paradigms such as MapReduce that has been designed to deal with Big Data efficiently. In this section we will learn how to: Parallelize in R Use MapReduce Implement multiple linear regression in a distributed system using MapReduce paradigm 2.1 Nodes, cores, processes and threads The terminology of nodes, cores, processes and threads is not universal. Depending on the computer, software (etc.), they can have various meanings. Typical examples: socket instead of node; cpu instead of core; task instead of process. Supercomputers have complex architectures, mainly due to their processors capability to work together on the same memory space. More precisely, the smallest computing units, called cores, are grouped in nodes. All the cores in one node share the same memory space. In other terms, the cores of the same node can operate on the same data, at the same time; no need for sending the data back and forth. This hardware architecture is summarized in this figure that shows a simple schematic of a 16-core node. The node contains two CPUs and each CPU consists of 8 cores. The schematic also shows the attached memory and the connections between the CPUs and memory. Nodes, cores and processors Nodes: Refers to the physical machine/server. In current systems, a node would typically include one or more processors, as well as memory and other hardware. Processor: Refers to the central processing unit (CPU), which contains one or more cores. Cores: Refers to the basic computation unit of the CPU. This is unit that carries out the actual computations. So in essence, each compute node contains one or more processors/CPUs and each CPU will typically consist of one or more cores. 2.2 Paralelización When using a single CPU, or serial computing, the problem size is limited by the available memory and performance. As data sizes become larger and systems become more complex, programs/applications rapidly run out of resources. Effectively utilising parallel computing can overcome these limitations. Parallel computing has become essential to solving big problems (high resolution, lots of timesteps, etc.) in science and engineering. Parallel computing can be simply defined as the simultaneous use of multiple processors/computers, i.e. parallel computers, to solve a computational problem. The general pattern is: The problem is broken down into discrete sections or separate tasks. Each processor works on its task or section of the problem. With multiple processors this means that several tasks can be processed at any given time. Processors exchange information with other processors, when required. It allows leveraging the resources of multiple processors/computers, increasing the resources available to the application/software. This enables the execution of tasks that do not fit on a single CPU and the completion of the tasks in a reasonable time. This has many benefits. For instance, we can add more data points which can translate to the use of bigger domains, improved spatial resolution or the inclusion of more particles in a simulation. Faster execution time can translate to increased number of solutions in a given time or a faster time to solution. 2.2.1 Shared Memory Programming Parallel programming for shared memory machines is easier since the all cores have access to the same memory address space and so all have access to the same data structures. This greatly simplifies the task of parallelisation. Use can be made of auto-parallelisation via compiler options, loop-level parallelism through compiler directives or OpenMP. On the other hand, speedup and scalability are limited by the number of cores in the shared memory machine, and this is generally a relatively small number. In addition, code can only be used on a shared memory machine. 2.2.2 Distributed Memory Programming Programming for distributed memory machines provides a means to take advantage of more resources than those available on a shared memory machine. In addition, code developed for distributed memory machines can be used on shared memory machines. However, this type of programming is generally more difficult than shared memory programming. Since each processor only has access to its local memory, the programmer is responsible for mapping data structures across the separate nodes. In addition, there is a need to coordinate the communications between nodes, i.e. message passing, to ensure that a node can access remote data when it is needed for a local computation. The standard library used for this is MPI. Next figure illustrate the difference between both approaches Memory Organization: (a) Shared Memory, (b) Distributed Memory Doing these tasks in R without strong knowledge in informatics can be hard. However, there are several R packages to perform parallel computing. The reason for using doParallel package, and not parallel, is that the parallel package is not working entirely on Windows and you had to write different code for it to work. The doParallel package is trying to make it happen on all platforms: UNIX, LINUX and WINDOWS, so its a pretty decent wrapper. To me, the most simple way of doing parallelization is to use mclapply() function from parallel but this cannot be used in Window. Let us assume we want to compute \\(f(x)=x^2 + x\\) of 10 numbers stored in a vector called vec. set.seed(1234) f &lt;- function(x) x^2 + x vec &lt;- rpois(10, lambda=200) We can do the following strategies: Looping forFunction &lt;- function(x) { ans &lt;- rep(NA, length(x)) for(i in vec) { ans[i] &lt;- f(i) } ans } Using lapply () or sapply () function lapplyFunction &lt;- function(x) { ans &lt;- sapply(x, f) ans } Using doParallel::parLapply() function We need first to create the cluster library(doParallel) ncores &lt;- detectCores() - 1 registerDoParallel(cores=ncores) cl &lt;- makeCluster(ncores) Then, we can use the parallel implementation of lapply parLapplyFunction &lt;- function(cl, x, f){ result &lt;- parLapply(cl=cl, X=x, fun=f) result } Using doParallel::foreach() function foreachDoParFunction &lt;- function(x) { result &lt;- foreach(i=x, .export=&quot;f&quot;) %dopar% f(i) result } foreachDoFunction &lt;- function(x) { result &lt;- foreach(i=x, .export=&quot;f&quot;) %do% f(i) result } Using parallel::mclapply() function # Only works in Linux (Windows ncores must be set equal to 1) result &lt;- mclapply(x, f, mc.cores=ncores) In order to compare computation time, we can run system.time(result &lt;- lapply(vec, f)) user system elapsed 0.02 0.00 0.02 Nonetheless, rbenchmark function serves as a more accurate replacement of the often seen system.time() function and the more sophisticated system.time(replicate(1000, expr)) expression (that incorporates variability). It tries hard to accurately measure only the time it takes to evaluate expr. To achieved this, the sub-millisecond (supposedly nanosecond) accurate timing functions most modern operating systems provide are used. Additionally all evaluations of the expressions are done in C code to minimize any overhead. In our example: library(rbenchmark) library(doParallel) ncores &lt;- detectCores() - 1 registerDoParallel(cores=ncores) cl &lt;- makeCluster(ncores) testdata1 &lt;- benchmark(&quot;For loop&quot; = forFunction(vec), &quot;lapply&quot; = lapplyFunction(vec), &quot;Foreach dopar&quot; = foreachDoParFunction(vec), &quot;Foreach do&quot; = foreachDoFunction(vec), &quot;parLapply&quot; = parLapplyFunction(cl=cl, x=vec, f=f), columns=c(&#39;test&#39;, &#39;elapsed&#39;, &#39;replications&#39;), replications = c(100, 200, 300, 400, 500)) ggplot() + geom_line(aes(x = replications, y = elapsed, colour = test), data = testdata1) Another example could be to compare the performance of the five methods for matrix multiplication set.seed(12345) A &lt;- matrix(rnorm(20), nrow=4, ncol=5) B &lt;- matrix(rnorm(20), nrow=4, ncol=5) FUN &lt;- function(i, A, B){ crossprod(A,B) } a &lt;- as.list(1:10) testdata2 &lt;- benchmark(&quot;For loop&quot; = for(i in 1:length(a)){FUN(i, A, B)}, &quot;lapply&quot; = lapply(a, FUN = FUN, A=A, B=B), &quot;Foreach dopar&quot; = foreach(i = 1:10) %dopar% FUN(i, A, B), &quot;Foreach do&quot; = foreach(i = 1:10) %do% FUN(i, A, B), &quot;parLapply&quot; = parLapply(cl = cl, X = a, fun = FUN, A=A, B=B), &quot;parSapply&quot; = parSapply(cl = cl, X = a, FUN = FUN, A=A, B=B), columns=c(&#39;test&#39;, &#39;elapsed&#39;, &#39;replications&#39;), replications = c(100, 200, 300, 400, 500)) ggplot() + geom_line(aes(x = replications, y = elapsed, colour = test), data = testdata2) Finally, we could also compare the performance of the five methods for fitting generalized linear model FUN &lt;- function(i) { ind &lt;- sample(100, 100, replace=TRUE) mod &lt;- glm(Species ~ Sepal.Length, family=binomial(logit), data = iris[ind,]) coefficients(mod) } a &lt;- as.list(1:10) testdata3 &lt;- benchmark(&quot;For loop&quot; = for(i in 1:length(a)){ FUN(a[[i]])}, &quot;lapply&quot; = lapply(a, FUN = FUN), &quot;Foreach dopar&quot; = foreach(i = 1:10) %dopar% FUN(i), &quot;Foreach do&quot; = foreach(i = 1:10) %do% FUN(i), &quot;parLapply&quot; = parLapply(cl = cl, X = a, fun = FUN), &quot;parSapply&quot; = parSapply(cl = cl, X = a, FUN = FUN), columns=c(&#39;test&#39;, &#39;elapsed&#39;, &#39;replications&#39;), replications = c(100, 200, 300, 400, 500)) ggplot() + geom_line(aes(x = replications, y = elapsed, colour = test), data = testdata3) stopCluster(cl) To sum up, generally, parLapply () perform better than foreach (). However, for all parallel implementation methods, the increase in terms of efficiency is not proportional to the number of cores being used (Theoretical efficiency). 2.3 MapReduce MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster. A MapReduce program is composed of a map procedure, which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method, which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The MapReduce System (also called infrastructure or framework) orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance. The model is a specialization of the split-apply-combine strategy for data analysis. It is inspired by the map and reduce functions commonly used in functional programming. There are two main frameworks to deal with Big Data and where MapReduce can be applied efficiently Hadoop provides support to perform MapReduce operations over a distributed file system of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Spark provides a richer set of verbs beyond MapReduce to facilitate optimizing code running in multiple machines. Spark also loaded data in-memory, making operations much faster than Hadoops on-disk storage. Thre are some pacakges to connect R and both Hadoop and Spark, but they use is beyond the scope of this short introduction to Big Data analysis. Next figure illustrate how to use MapReduce to count words in two different text files stored in different machines. The map operation splits each word in the original file and outputs a new word-counting file with a mapping of words and counts. The reduce operation can be defined to take two word-counting files and combine them by aggregating the totals for each word; this last file will contain a list of word counts across all the original files. Counting words is often the most basic MapReduce example, but we can also use MapReduce for much more sophisticated and interesting applications in statistics. The MapReduce paradigm has long been a staple of big data computational strategies. However, properly leveraging MapReduce in R can be a challenge, even for experienced users. To get the most out of MapReduce, it is helpful to understand its relationship to functional programming. Functional programming, in a broad sense, is the one that some functions allows to have another function in one of its arguments. For instance, the function sapply(): ff &lt;- function(x) if(x &gt; 0) log(x) else log(-x)^2 sapply(-4:10, ff) [1] 1.9218121 1.2069490 0.4804530 0.0000000 Inf 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 [11] 1.7917595 1.9459101 2.0794415 2.1972246 2.3025851 Functional programming is a very powerful tool that allows you to program in the following way: Create small and simple functions that solve a small and bounded problem Apply these functions to homogeneous groups of values. In the previous example, we have built an ff function and through the sapply () function we have applied it to a list of homogeneous values: the numbers from -4 to 10. There are many functions in R, some of which you have already seen, that take others as arguments. Some of the most common are: sapply y lapply tapply apply y mapply Las funciones ddply, ldply, etc. del paquete plyr A very common example of this type of functions is used to inspect the type of columns in a table since they take advantage of the fact that a table is a list of columns and go through them one by one lapply(iris, class) $Sepal.Length [1] &quot;numeric&quot; $Sepal.Width [1] &quot;numeric&quot; $Petal.Length [1] &quot;numeric&quot; $Petal.Width [1] &quot;numeric&quot; $Species [1] &quot;factor&quot; sapply(iris, length) Sepal.Length Sepal.Width Petal.Length Petal.Width Species 150 150 150 150 150 One of the advantages of this type of programming is that the code is shorter and more readable. We must remember that these functions include the argument ... that allows to pass additional arguments to the function they call. For example this function would do the same as the previous one, but it would be more generic since it would allow calculations by varying the s argument. ff2 &lt;- function(x, s) { if(x &gt; s) log(x) else log(-x)^2 } sapply(-4:10, ff2, s=0) [1] 1.9218121 1.2069490 0.4804530 0.0000000 Inf 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 [11] 1.7917595 1.9459101 2.0794415 2.1972246 2.3025851 2.3.1 Map The MapReduce methodology is also implemented in base R (Map () and Reduce () functions) as well as in tidyverse. Function Map () applies one function to all elements from a list o vector: map(YOUR_LIST, YOUR_FUNCTION) Previous operations could also be executed by usint this code (sapply () es un caso especial de la función Map ()): Map(ff, c(9, 16, 25)) [[1]] [1] 2.197225 [[2]] [1] 2.772589 [[3]] [1] 3.218876 y Map(ff2, c(9, 16, 25), s=0) [[1]] [1] 2.197225 [[2]] [1] 2.772589 [[3]] [1] 3.218876 Another advantage appears when we want to vary more than one argument. With lapply (), only one argument varies; the others are fixed. For example, how would you find a weighted mean when you have two lists, one of observations and the other of weights? # Generate some sample data xs &lt;- replicate(5, runif(10), simplify = FALSE) ws &lt;- replicate(5, rpois(10, 5) + 1, simplify = FALSE) str(xs) List of 5 $ : num [1:10] 0.767 0.671 0.909 0.198 0.436 ... $ : num [1:10] 0.296 0.306 0.126 0.518 0.831 ... $ : num [1:10] 0.607 0.6956 0.1348 0.0715 0.1021 ... $ : num [1:10] 0.655 0.107 0.925 0.557 0.742 ... $ : num [1:10] 0.149 0.47 0.668 0.194 0.451 ... str(ws) List of 5 $ : num [1:10] 6 3 5 4 8 13 5 5 3 5 $ : num [1:10] 3 9 6 5 6 4 6 6 6 5 $ : num [1:10] 3 14 12 4 8 8 4 8 3 4 $ : num [1:10] 7 7 8 5 9 7 6 6 8 4 $ : num [1:10] 4 5 7 4 3 4 8 4 4 7 # compute the weighted.mean unlist(Map(weighted.mean, xs, ws)) [1] 0.4676235 0.5447004 0.3928258 0.4760168 0.5505697 If some of the arguments should be fixed and constant, use an anonymous function: Map(function(x, w) weighted.mean(x, w, na.rm = TRUE), xs, ws) [[1]] [1] 0.4676235 [[2]] [1] 0.5447004 [[3]] [1] 0.3928258 [[4]] [1] 0.4760168 [[5]] [1] 0.5505697 2.3.2 Reduce Another way of thinking about functionals is as a set of general tools for altering, subsetting, and collapsing lists. Every functional programming language has three tools for this: Map(), Reduce(), and Filter(). We have seen Map() already, and next we describe Reduce(), a powerful tool for extending two-argument functions. Filter() is a member of an important class of functionals that work with predicates, functions that return a single TRUE or FALSE (we will not cover that). Reduce() reduces a vector, x, to a single value by recursively calling a function, f, two arguments at a time. It combines the first two elements with f, then combines the result of that call with the third element, and so on. Calling Reduce(f, 1:3) is equivalent to f(f(1, 2), 3). Reduce is also known as fold, because it folds together adjacent elements in the list. The following two examples show what Reduce does with an infix and prefix function: Reduce(`+`, 1:3) # -&gt; ((1 + 2) + 3) [1] 6 Reduce(sum, 1:3) # -&gt; sum(sum(1, 2), 3) [1] 6 The essence of Reduce() can be described by a simple for loop: Reduce2 &lt;- function(f, x) { out &lt;- x[[1]] for(i in seq(2, length(x))) { out &lt;- f(out, x[[i]]) } out } Reduce() is also an elegant way of extending a function that works with two inputs into a function that can deal with any number of inputs. It is useful for implementing many types of recursive operations, like merges and intersections. Imagine you have a list of numeric vectors, and you want to find the values that occur in every element: l &lt;- replicate(5, sample(1:10, 15, replace = T), simplify = FALSE) str(l) List of 5 $ : int [1:15] 9 7 9 6 6 6 10 4 3 8 ... $ : int [1:15] 8 3 7 1 9 5 3 8 10 7 ... $ : int [1:15] 8 6 9 8 9 3 9 10 9 4 ... $ : int [1:15] 7 4 10 8 6 6 3 7 6 4 ... $ : int [1:15] 10 7 8 10 5 1 6 9 6 8 ... You could do that by intersecting each element in turn: intersect(intersect(intersect(intersect(l[[1]], l[[2]]), l[[3]]), l[[4]]), l[[5]]) [1] 9 6 10 8 Thats hard to read. With Reduce(), the equivalent is: Reduce(intersect, l) [1] 9 6 10 8 2.4 Linear regression for Big Data In this section, we describe a very nice application of MapReduce framework to a Big Data problem. There are several problems that require the analysis of large volumes of information. The analysis at very large scale of data is a challenging task since the available information cannot be practically analyzed on a single machine due to the sheer size of the data to fit in memory. In order to overcome this difficulty, high-performance analytical systems running on distributed environments can be used. To this end standard analytics algorithms need to be adapted to take advantage of cloud computing models which provide scalability and flexibility. Here, we describe an approach that introduces a new distributed training method for Multiple Linear Regression which will be based on the QR decomposition and the ordinary least squares method adapted to MapReduce framework. The method is called MLR-MR and is described in (Moufida Adjout Rehab and Faouzi Boufares, 2105). The paper is also available in our Moodle. In this figure we can observe as the model fitting using MLR-MR algorithm is dramatically reduced when the number of MapReduce processors increases. Speedup training MLR-MR with different MapReduce working nodes Let us start by recalling how to describe linear regression using the classical matrix notation: \\[\\mathbf{Y}=\\mathbf{X}\\mathbf{\\beta}+\\mathbf{\\varepsilon}.\\] The ordinary least square (OLS) estimate of \\(\\mathbf{\\beta}\\) is \\[\\widehat{\\mathbf{\\beta}}=[\\mathbf{X}^T\\mathbf{X}]^{-1}\\mathbf{X}^T\\mathbf{y}\\]. To illustrate, let us consider the mtcars example, and run this regression: data(mtcars) mod &lt;- (lm(mpg ~ wt + cyl, data = mtcars)) The algorithm implemented in the lm () function uses the QR decomposition of \\(\\mathbf{X},\\) \\[\\mathbf{X}=\\mathbf{Q}\\mathbf{R},\\] where \\(\\mathbf{Q}\\) is an orthogonal matrix (i.e. \\(\\mathbf{Q}^T\\mathbf{Q}=\\mathbb{I}\\)).Then, \\[\\widehat{\\mathbf{\\beta}}=[\\mathbf{X}^T\\mathbf{X}]^{-1}\\mathbf{X}^T\\mathbf{y}=\\mathbf{R}^{-1}\\mathbf{Q}^T\\mathbf{y}\\] Y &lt;- mtcars$mpg #similar to cbind(1, mtcars$wt, mtcars$cyl) X &lt;- model.matrix(~ wt + cyl, data=mtcars) QR &lt;- qr(as.matrix(X)) R &lt;- qr.R(QR) Q &lt;- qr.Q(QR) solve(R) %*% t(Q) %*% Y [,1] (Intercept) 39.686261 wt -3.190972 cyl -1.507795 We can parallelise computations using the MLR-MR method as follows: Consider \\(m\\) blocks, for instance 3 (given tha we have 4 cores) m &lt;- 3 and split vectors and matrices \\[\\mathbf{y}=\\left[\\begin{matrix}\\mathbf{y}_1\\\\\\mathbf{y}_2\\\\\\vdots \\\\\\mathbf{y}_m\\end{matrix}\\right]\\] and \\[\\mathbf{X}=\\left[\\begin{matrix}\\mathbf{X}_1\\\\\\mathbf{X}_2\\\\\\vdots\\\\\\mathbf{X}_m\\end{matrix}\\right]=\\left[\\begin{matrix}\\mathbf{Q}_1^{(1)}\\mathbf{R}_1^{(1)}\\\\\\mathbf{Q}_2^{(1)}\\mathbf{R}_2^{(1)}\\\\\\vdots \\\\\\mathbf{Q}_m^{(1)}\\mathbf{R}_m^{(1)}\\end{matrix}\\right]\\] In R to split vectors and matrices we can use these two functions: chunk &lt;- function(x,n) split(x, cut(seq_along(x), n, labels = FALSE)) splitMatByRow = function(mat, size){ row.index &lt;- chunk(1:nrow(mat), size) lapply(row.index, function(val) mat[val, ]) } We can do that with our data x.block &lt;- splitMatByRow(X, m) y.block &lt;- splitMatByRow(matrix(Y, ncol = 1), m) Then, we get small QR decomposition (per subset). This step correspond to de Map step in the MapReduce framework # Algorithm 2 Mapper function in step1 x.block.qr &lt;- lapply(x.block, function(val){ qrresult &lt;- qr(val) list(Q=qr.Q(qrresult), R=qr.R(qrresult)) }) Now, consider the QR decomposition of \\(\\mathbf{R}^{(1)}\\) which is the first step of the reduce part \\[\\mathbf{R}^{(1)}=\\left[\\begin{matrix}\\mathbf{R}_1^{(1)}\\\\\\mathbf{R}_2^{(1)}\\\\\\vdots \\\\\\mathbf{R}_m^{(1)}\\end{matrix}\\right]=\\mathbf{Q}^{(2)}\\mathbf{R}^{(2)}\\] where \\[\\mathbf{Q}^{(2)}=\\left[\\begin{matrix}\\mathbf{Q}^{(2)}_1\\\\\\mathbf{Q}^{(2)}_2\\\\\\vdots\\\\\\mathbf{Q}^{(2)}_m\\end{matrix}\\right]\\] that can be computed as # Algorithm 3 Reducer function in step1 Rtemp &lt;- do.call(rbind, lapply(x.block.qr, function (l) l$R)) qrresult &lt;- qr(Rtemp) Rtemp.qr &lt;- list(Q=qr.Q(qrresult), R=qr.R(qrresult)) R.final &lt;- Rtemp.qr$R Rtemp.Q.divide &lt;- splitMatByRow(Rtemp.qr$Q, m) Q.result = list() for (i in 1:m){ Q.result[[i]] &lt;- x.block.qr[[i]]$Q %*% Rtemp.Q.divide[[i]] } Define  as step 2 of the reduce part \\[\\mathbf{Q}^{(3)}_j=\\mathbf{Q}^{(2)}_j\\mathbf{Q}^{(1)}_j\\] and \\[\\mathbf{V}_j=\\mathbf{Q}^{(3)T}_j\\mathbf{y}_j\\] # Algorithm 4 Reduce function in step2 V = list() for (i in 1:m){ V[[i]] &lt;- crossprod(Q.result[[i]], y.block[[i]]) } and finally set  as the step 3 of the reduce part \\[\\widehat{\\mathbf{\\beta}}=[\\mathbf{R}^{(2)}]^{-1}\\sum_{j=1}^m\\mathbf{V}_j\\] # Algorithm 5 Reduce function in step3 V.sum &lt;- Reduce(&#39;+&#39;, V) beta &lt;- as.numeric(solve(R.final) %*% V.sum) Let us compare the results cbind(lm=coef(mod), parallel=beta) lm parallel (Intercept) 39.686261 39.686261 wt -3.190972 -3.190972 cyl -1.507795 -1.507795 # error sum((beta - coef(mod)) ** 2) [1] 2.607678e-28 "],["linear-models.html", "3 Linear models 3.1 OLS estimation in R", " 3 Linear models Consider the study of the effect of a new drug for hemophilia, by analyzing the level of blood coagulation after the administration of various amounts of the new drug. Researchers may be interested in knowing whether the drug affects on the level of blood coagulation. The simplest statistical model to address this scientific question is the linear regression model \\[ y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, \\] where \\(i = 1, \\ldots, n\\) and \\(i\\) denotes the \\(i\\)-th observation. However, the response variable (\\(y\\)) can be explained by a different independent variables (\\(x\\)). In other words, the response variable can be the sum of effects of some independent factors (\\(x=(x_1,\\dots,x_p)\\)). The model assumption implies that the expected response is the sum of the factors effects: \\[ \\begin{align} E[y]=x_1 \\beta_1 + \\dots + x_p \\beta_p = \\sum_{j=1}^p x_j \\beta_j = x&#39;\\beta . \\tag{7.1} \\end{align} \\] Clearly, there may be other factors that affect the the level of blood coagulation. We thus introduce an error term, denoted by \\(\\epsilon\\), to capture the effects of all unmodeled factors and measurement error. The implied generative process of a sample of \\(i = 1, \\ldots, n\\) observations is thus: \\[ \\begin{align} y_i = x_i&#39;\\beta + \\varepsilon_i = \\sum_j x_{i,j} \\beta_j + \\varepsilon_i , i=1,\\dots,n . \\tag{7.2} \\end{align} \\] or in matrix notation \\[ \\begin{align} y = X \\beta + \\varepsilon . \\tag{7.3} \\end{align} \\] This figure illustrates the linear regression fit of our problem $title [1] &quot;Blood coagulation as a function of drug doses&quot; attr(,&quot;class&quot;) [1] &quot;labels&quot; Model parameters can be estimated by solving the Ordinary Least Squares (OLS) problem \\[ \\begin{align} \\hat \\beta= \\text{argmin}_\\beta \\{ \\sum_i (y_i-x_i&#39;\\beta)^2 \\}, \\tag{7.4} \\end{align} \\] and in matrix notation \\[ \\begin{align} \\hat \\beta= \\text{argmin}_\\beta \\{ \\Vert y-X\\beta \\Vert^2_2 \\}. \\tag{7.5} \\end{align} \\] This minimization problem has an unique solution given by: \\[\\widehat{\\mathbf{\\beta}}=[\\mathbf{X}^T\\mathbf{X}]^{-1}\\mathbf{X}^T\\mathbf{y}=\\mathbf{R}^{-1}\\mathbf{Q}^T\\mathbf{y}\\] 3.1 OLS estimation in R We are now ready to estimate some linear models with R. We will use the airquality data from the datasets package (installed by default), that contains daily air quality measurements in New York, May to September 1973. head(airquality) Ozone Solar.R Wind Temp Month Day 1 41 190 7.4 67 5 1 2 36 118 8.0 72 5 2 3 12 149 12.6 74 5 3 4 18 313 11.5 62 5 4 5 NA NA 14.3 56 5 5 6 28 NA 14.9 66 5 6 We carry out the OLS estimation to investigate whether temperature has any influence on ozone levels mod &lt;- lm(Ozone ~ Temp, data=airquality) summary(mod) Call: lm(formula = Ozone ~ Temp, data = airquality) Residuals: Min 1Q Median 3Q Max -40.729 -17.409 -0.587 11.306 118.271 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -146.9955 18.2872 -8.038 9.37e-13 *** Temp 2.4287 0.2331 10.418 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 23.71 on 114 degrees of freedom (37 observations deleted due to missingness) Multiple R-squared: 0.4877, Adjusted R-squared: 0.4832 F-statistic: 108.5 on 1 and 114 DF, p-value: &lt; 2.2e-16 We can visualize our results using ggplot2 as follows: library(ggplot2) p &lt;- ggplot(data = airquality, aes(x = Temp, y = Ozone)) + geom_smooth(method = &quot;lm&quot;, se=FALSE, color=&quot;darkred&quot;) + xlab(&quot;Temperature&quot;) + ylab(&quot;Ozone&quot;) + geom_point() p "],["generalized-linear-models.html", "4 Generalized linear models 4.1 Logistic regression with R", " 4 Generalized linear models Let us consider other real data problems. Problem 1: Consider the relation between cigarettes smoked, and the occurrence of lung cancer. Do we expect the probability of cancer to be linear in the number of cigarettes? I do not think so. In those situations, mainly when the outcome variable does not follow a Normal distribution, generalized linear models (GLMs) are required. Problem 2: Consider the relation between the travel times to the distance traveled. Do you agree that the longer the distance traveled, then not only the travel times get longer, but they also get more variable? In the linear models, we assumed the generative process to be linear in the effects of the predictors. Let us now write that same linear model, slightly differently in order to facilitate its extension: \\[ y|x \\sim \\mathcal{N}(x&#39;\\beta, \\sigma^2). \\] This model not allow for the non-linear relations of Problem 1, nor does it allow for the distribution of \\(\\epsilon\\) to change with \\(x\\) as in the Problem 2. This is how GLMs works by generalizing linear models to address these two limitations. For Problem 1, we would like something like \\[ y|x \\sim Binom(1,p(x)) \\] and for Problem 2 \\[ y|x \\sim \\mathcal{N}(x&#39;\\beta,\\sigma^2(x)), \\] or more generally \\[ y|x \\sim \\mathcal{N}(\\mu(x),\\sigma^2(x)), \\] or maybe not Gaussian \\[ y|x \\sim Pois(\\lambda(x)). \\] Even more generally, for some distribution \\(F(\\theta)\\), with a parameter \\(\\theta\\), we would like to assume that the data is generated via \\[ \\begin{align} \\tag{8.1} y|x \\sim F(\\theta(x)) \\end{align} \\] GLMs assume the data distribution \\(F\\) to be in a well-behaved family known as the Natural Exponential Family of distributions. This family includes the Gaussian, Gamma, Binomial, Poisson, and Negative Binomial distributions. These five include as special cases the exponential, chi-squared, Rayleigh, Weibull, Bernoulli, and geometric distributions. GLMs also assume that the distributions parameter, \\(\\theta\\), is some simple function of a linear combination of the effects. In our cigarettes example this amounts to assuming that each cigarette has an additive effect, but not on the probability of cancer, but rather, on some simple function of it. Formally \\[ g(\\theta(x))=x&#39;\\beta, \\] and we recall that \\[ x&#39;\\beta=\\beta_0 + \\sum_j x_j \\beta_j. \\] that corresponds to the linear predictor, and \\(g\\) function is known as the link function that for Normal data is the identity, for binomial data (i.e. binary) is the logistic function and for count data (Poisson) is the logarithm function. 4.1 Logistic regression with R The predimed data set provides information on the PREDIMED trial (Prevención con Dieta Mediterránea) which is a randomized, parallel and multicentric cohort with more than 7,000 participants who were randomly assigned to three diet groups (olive oil + mediterranean diet, nuts + mediterranean diet, and low-fat diet -control group-). It also includes information about whether the individual suffered from different adverse events (stroke, cardiovascular, myocardial infarction) after a 7 years follow-up. library(compareGroups) data(predimed) head(predimed) group sex age smoke bmi waist wth htn diab hyperchol famhist hormo p14 toevent event 1 Control Male 58 Former 33.53 122 0.7530864 No No Yes No No 10 5.374401 Yes 2 Control Male 77 Current 31.05 119 0.7300614 Yes Yes No No No 10 6.097194 No 4 MedDiet + VOO Female 72 Former 30.86 106 0.6543210 No Yes No Yes No 8 5.946612 No 5 MedDiet + Nuts Male 71 Former 27.68 118 0.6941177 Yes No Yes No No 8 2.907598 Yes 6 MedDiet + VOO Female 79 Never 35.94 129 0.8062500 Yes No Yes No No 9 4.761123 No 8 Control Male 63 Former 41.66 143 0.8033708 Yes Yes Yes No &lt;NA&gt; 9 3.148528 Yes table(predimed$group) Control MedDiet + Nuts MedDiet + VOO 2042 2100 2182 Let us fit a logistic regression model to investigate whether mediterranean diet (variable group) is associated with the risk of having and adverse event (variable event) mod &lt;- glm(event ~ group, data=predimed, family=&quot;binomial&quot;) summary(mod) Call: glm(formula = event ~ group, family = &quot;binomial&quot;, data = predimed) Deviance Residuals: Min 1Q Median 3Q Max -0.3120 -0.3120 -0.2819 -0.2604 2.6081 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.9983 0.1040 -28.820 &lt;2e-16 *** groupMedDiet + Nuts -0.3690 0.1600 -2.306 0.0211 * groupMedDiet + VOO -0.2073 0.1519 -1.365 0.1723 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 2118.1 on 6323 degrees of freedom Residual deviance: 2112.6 on 6321 degrees of freedom AIC: 2118.6 Number of Fisher Scoring iterations: 6 Let us see how the results change after adjusting by other covariates mod.adj &lt;- glm(event ~ group + sex + age + smoke, data=predimed, family=&quot;binomial&quot;) summary(mod.adj) Call: glm(formula = event ~ group + sex + age + smoke, family = &quot;binomial&quot;, data = predimed) Deviance Residuals: Min 1Q Median 3Q Max -0.6351 -0.3138 -0.2526 -0.2012 2.9984 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -8.01858 0.76810 -10.439 &lt; 2e-16 *** groupMedDiet + Nuts -0.37122 0.16170 -2.296 0.02169 * groupMedDiet + VOO -0.19791 0.15327 -1.291 0.19662 sexFemale -0.54522 0.17079 -3.192 0.00141 ** age 0.07418 0.01058 7.010 2.39e-12 *** smokeCurrent 0.63066 0.20707 3.046 0.00232 ** smokeFormer 0.43680 0.18237 2.395 0.01662 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 2118.1 on 6323 degrees of freedom Residual deviance: 2023.0 on 6317 degrees of freedom AIC: 2037 Number of Fisher Scoring iterations: 6 "],["model-fitting.html", "5 Model Fitting 5.1 Getting started 5.2 General rules for variable selection 5.3 Stepwise variable selection 5.4 Comparing models 5.5 Automatic variable selection 5.6 Cross validation 5.7 Cross-validation and Bootstrap 5.8 Example with caret library 5.9 Missing data imputation 5.10 Regularization", " 5 Model Fitting This chapter covers a range of additional topics related to model fitting, such as variable selection, model comparison, cross-validation and missing data imputation. It culminates with a discussion of ridge and LASSO regression, two useful regression-based machine learning techniques for automatically selecting variables in high dimensional data so as to balance the bias-variance trade-off. The concepts and methods discussed here apply to both linear and logistic regression. Additional resource: Introduction to Statistical Learning. See chapters 5 and 6. 5.1 Getting started Before starting showing how to perform data modelling in the context of linear regression (NOTE: everything applies to logistic regression), let us start by implementing some functions that will be required to evaluate model performance. A multivariate linear model with an outcome, \\(y\\), and \\(p\\) predictors \\(x\\) can be written as: \\[ y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, \\] where \\(i = 1, \\ldots, n.\\) The subscript in this equation, \\(i\\), indexes the \\(n\\) observations in the dataset. (Think of \\(i\\) as a row number.) The equation can be read as follows: the value of \\(i^{th}\\) outcome variable, \\(y_i\\), is defined by an intercept, \\(\\beta_0\\), plus a slope, \\(\\beta_1\\), multiplied by the \\(i^{th}\\) predictor variable, \\(x_i\\). These elements define the systematic or deterministic portion of the model. However, because the world is uncertain, containing randomness, we know that the model will be wrong (as George Box said). To fully describe the data we need an error term, \\(\\epsilon_i\\), which is also indexed by row. The error term is the stochastic portion of the model. \\(\\epsilon_i\\) measures the distance between the fitted or expected values of the modelcalculated from the deterministic portion of the modeland the actual values. The errors in a linear modelalso known as model residualsare the part of the data that remains unexplained by the deterministic portion of the model. One of the key assumptions of a linear model is that the residuals are normally distributed with mean = 0 and variance = \\(\\sigma^2\\), which we denote, in matrix notation, as \\(N(0,\\sigma^2)\\). The model performance can be summarized with \\[ \\operatorname{RSS} = \\sum_{i=1}^n ((\\beta_0 + \\beta_1x_i) - y_i)^2 = \\sum_{i=1}^n (\\hat{y}_i - y_i)^2 \\] A related measure is root mean squared error (RMSE), the square root of the average of the squared errors: \\[ \\operatorname{RMSE}= \\sqrt{\\frac{\\sum_{i=1}^n ((\\beta_0 + \\beta_1x_i) - y_i)^2}{n}} \\] \\[ = \\sqrt{\\frac{\\sum_{i=1}^n (\\hat{y}_i - y_i)^2}{n}} \\] The nice thing about RMSE is that, unlike RSS, it returns a value that is on the scale of the outcome. \\(R^2\\) is another measure of model fit that is convenient because it is a standardized measurescaled between 0 and 1and is therefore comparable across contexts. \\[ R^2 = 1 - \\frac{SS_\\text{resid}}{SS_\\text{tot}}, \\] where \\(SS_\\text{tot}=\\sum_i (y_i-\\bar{y})^2\\) and \\(SS_\\text{res}=\\sum_i (y_i - \\hat{y}_i)^2\\). In words: \\(R^2\\) represents the variation in the outcome variable explained by the model as a proportion of the total variation. In the plot below, the left hand panel, TSS, serves as the denominator in calculating \\(R^2\\), and the right hand panel, RSS, is the numerator. Next R code illustrates how to implement such measurements and one example with Hitters dataset which contains information of the performance statistics and salaries of major league baseball players in the 1986 season. It includes information about Salary, which is our outcome, and some predictor: hits, years in the league, home runs, RBIs, walks and assists. library(ISLR) data(Hitters) rss &lt;- function(fitted, actual){ sum((fitted - actual)^2) } rmse &lt;- function(fitted, actual){ sqrt(mean((fitted - actual)^2)) } R2 &lt;- function(fitted, actual){ tss &lt;- sum((actual - mean(actual))^2) rss &lt;- sum((actual - fitted)^2) 1 - rss/tss } display(h &lt;- lm(Salary ~ Hits, data= Hitters)) lm(formula = Salary ~ Hits, data = Hitters) coef.est coef.se (Intercept) 63.05 64.98 Hits 4.39 0.56 --- n = 263, k = 2 residual sd = 406.17, R-Squared = 0.19 rss(fitted(h), na.omit(Hitters$Salary)) [1] 43058621 rmse(fitted(h), na.omit(Hitters$Salary)) [1] 404.6245 R2(fitted(h), na.omit(Hitters$Salary)) [1] 0.1924355 5.2 General rules for variable selection How do we know which variables belong in a model? The short answer is: we often dont. Here are some rules of thumb when thinking about variable selection:1 Think about the data. What variables does it make sense to include given the situation? Does any published literature offer guidance? If we are in descriptive mode then we may only care about certain variables and use the others as controls. If we are in predictive mode then we include all variables that, for substantive reasons, might be important in predicting the outcome. This is very general guidance, however, as different contexts demand different approaches to model fitting. Include quadratic terms if there is evidence from bivariate plots of a non-linear relationship between predictor and outcome. In general, we dont include polynomial terms with degrees greater than 2. To do so risks overfitting. Look for possible interactions among variables with the largest main effects. In general we dont include higher order interactions (greater than 2) unless we have a sensible rationale and can explain it (to ourselves and to our audience). 2 way interactions are hard enough to explain. Consider combining separate predictors into a single predictora total scoreby summing or averaging them. Keep it simple. Parsimonious models are almost always betterthey are more interpretable and tend to have lower variance. 5.3 Stepwise variable selection The traditional technique in statistics for selecting variables is stepwise selection. With forward selection we start with a null model (intercept only) and add one variable at a time. If the added variable improves the model, then we keep it in and add another. We continue until all variables have been tested. See this figure Forward selection With backward selection we start with a full model (all available terms), and serially remove variables. If the model is better after a variable has been removed, then we leave it out. We continue until all variables have been tested. See this figure Backward selection Forward selection followed by backward selection. Select forward then backward. Unfortunately these hand-fitting procedures are flawed. They depend on the order in which variables are added or excluded and often will not select the best model. Furthermore, in the Boston data there are \\(k\\) = 13 predictor variables, which means there are \\(2^k\\) or 8192 possible models we could fit, not even including interactions or polynomial terms. This is an extremely large space to search through to find the best model, and the search is computationally expensive and time consuming. Conducting such a search manually would be impossible. 5.4 Comparing models We are already familiar with \\(R^2\\), RMSE and RSS as tools for comparing models. In general, if we add a variable and \\(R^2\\) goes up and RMSE/RSS goes down, then the model with the additional variable is better. The amount of unexplained variance has decreased. However, there is a danger of overfitting. As weve seen, adjusted \\(R^2\\) penalizes the fit for the number of predictors. Likewise, information criterion methods like AIC (Akaike Information Criterion) penalize the fit for model complexity, defined as the number of predictors. \\[\\mathrm{AIC} = - 2\\ln(L) + 2k\\] where \\(k\\) the number of estimated parameters in the model, \\(L\\) is the maximized value of the likelihood function for the model, and \\(ln\\) is the natural log. Given a set of candidate models for the data, the preferred model is the one with the lowest AIC value. In penalizing for larger \\(k\\) (ensured by the final term, \\(+2k\\)), AIC attempts to guard against overfitting. It is possible, then, to see \\(R^2\\) go up with the addition of predictors, while AIC goes down. We can also compare models with a formal statistic test using the likelihood ratio test (LRT): \\[ 2 \\times [ \\ln(L_{a}) - \\ln(L_{c}) ] \\] where \\(\\ln(L_{c})\\) is the log likelihood of the current model and \\(\\ln(L_{a})\\) is the log likelihood of the alternative model with additional predictors. The lrtest() function in the lmtest package implements the LRT. The anova() function in base R will also compare models using an f-test, with results that will be virtually identical to the LRT. Here are some examples of model comparison using the Hitters data from the ISLR package. We start with a null model of Salary: library(ISLR); data(Hitters) display(null &lt;- lm(Salary ~ 1, data = Hitters)) lm(formula = Salary ~ 1, data = Hitters) coef.est coef.se (Intercept) 535.93 27.82 --- n = 263, k = 1 residual sd = 451.12, R-Squared = 0.00 round(mean(Hitters$Salary, na.rm = T),2) [1] 535.93 A null model consists only in an intercept, the coefficient of which, as we can see, is just the mean of Salary. (Note that in order to calculate the mean of Salary we needed to remove the missing values. lm() silently removes the missing values: display() reports \\(n = 263\\), whereas the dataset has 322 rows.) The key question as we make a model more complex is whether that complexity is justified, whether adding predictors not only lowers the bias but does so without unduly increasing the potential variance. Lets add predictors. library(lmtest) display(h1 &lt;- lm(Salary ~ Hits, data = Hitters)) lm(formula = Salary ~ Hits, data = Hitters) coef.est coef.se (Intercept) 63.05 64.98 Hits 4.39 0.56 --- n = 263, k = 2 residual sd = 406.17, R-Squared = 0.19 lrtest(null, h1) Likelihood ratio test Model 1: Salary ~ 1 Model 2: Salary ~ Hits #Df LogLik Df Chisq Pr(&gt;Chisq) 1 2 -1980.1 2 3 -1952.0 1 56.212 6.508e-14 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(null, h1) Analysis of Variance Table Model 1: Salary ~ 1 Model 2: Salary ~ Hits Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 262 53319113 2 261 43058621 1 10260491 62.194 8.531e-14 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 as.matrix(AIC(null, h1)) df AIC null 2 3964.130 h1 3 3909.918 Hits is statistically significant, since the 95% CI does not include 0 (4.39 \\(\\pm\\) 2 x .56). These three methods agree that the model with Hits is an improvement over the null model. In the case of lrtest() and anova() the p-value represents the results of a statistical test (chi-squared test and f-test, respectively) for whether the second, more complex model is a better fit to the data. Does adding an additional predictor, AtBat, improve the model further? display(h2 &lt;- lm(Salary ~ Hits + AtBat, data = Hitters)) lm(formula = Salary ~ Hits + AtBat, data = Hitters) coef.est coef.se (Intercept) 141.27 76.55 Hits 8.21 2.08 AtBat -1.22 0.64 --- n = 263, k = 3 residual sd = 404.13, R-Squared = 0.20 lrtest(h1, h2) Likelihood ratio test Model 1: Salary ~ Hits Model 2: Salary ~ Hits + AtBat #Df LogLik Df Chisq Pr(&gt;Chisq) 1 3 -1952.0 2 4 -1950.1 1 3.6588 0.05577 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(h1, h2) Analysis of Variance Table Model 1: Salary ~ Hits Model 2: Salary ~ Hits + AtBat Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 261 43058621 2 260 42463750 1 594871 3.6423 0.05743 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 as.matrix(AIC(h1, h2)) df AIC h1 3 3909.918 h2 4 3908.260 The results are ambiguous. R-squared goes up, while AIC, log likelihood and RSS go down, but the decline in the latter two cases is not statistically significant. (This result is consistent with the fact that AtBat is not itself statistically significant, since the 95% CI for AtBat includes 0: -1.22 \\(pm\\) 2 x .64.) Should we leave AtBat in the model? It doesnt improve the fit much, if at all, while adding complexity. So, we should take it out. Unfortunately such choices are often not clear, which is why model fitting sometimes seems more like an art than a science. To implement forward selection, we would keep adding variables and comparing models using lrtest() or anova() trying to find the best possible fit. One problem with this procedure, however, is that the order in which we step through predictors will impact our selection decisions because each predictors impact on model fit is contingent on the presence of the others. For example, suppose we had added AtBat later in the selection process: h3 &lt;- lm(Salary ~ Hits + Years + HmRun + RBI + Walks + Assists, data = Hitters) h4 &lt;- lm(Salary ~ Hits + Years + HmRun + RBI + Walks + Assists + AtBat, data = Hitters) lrtest(h3, h4) Likelihood ratio test Model 1: Salary ~ Hits + Years + HmRun + RBI + Walks + Assists Model 2: Salary ~ Hits + Years + HmRun + RBI + Walks + Assists + AtBat #Df LogLik Df Chisq Pr(&gt;Chisq) 1 8 -1916.7 2 9 -1911.1 1 11.114 0.0008569 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 as.matrix(AIC(h3, h4)) df AIC h3 8 3849.311 h4 9 3840.198 Now AtBat clearly improves the fit, but we would never have discovered that had we already thrown it out. This is troubling. Is there a better way? Perhaps. 5.5 Automatic variable selection Algorithms have been developed to search model space efficiently for the optimal model. A caution about automatic variable selection is in order at the outset, however. Choosing variables should not be a mechanical process. We should, instead, seek to understand the data generating process. Indeed, the greatest benefit of manual stepwise selection consists less in producing a good model than in the understanding gained by fitting many models, and seeing, through trial and error, which predictors are most reactive with the outcome. Especially when it comes to description, automatic variable selection algorithms are just tools for exploring your data and thinking about models. The step() function in base R automates stepwise variable selection using AIC. display(step(lm(Salary ~ ., data = Hitters), trace = F, direction = &quot;forward&quot;)) lm(formula = Salary ~ AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + League + Division + PutOuts + Assists + Errors + NewLeague, data = Hitters) coef.est coef.se (Intercept) 163.10 90.78 AtBat -1.98 0.63 Hits 7.50 2.38 HmRun 4.33 6.20 Runs -2.38 2.98 RBI -1.04 2.60 Walks 6.23 1.83 Years -3.49 12.41 CAtBat -0.17 0.14 CHits 0.13 0.67 CHmRun -0.17 1.62 CRuns 1.45 0.75 CRBI 0.81 0.69 CWalks -0.81 0.33 LeagueN 62.60 79.26 DivisionW -116.85 40.37 PutOuts 0.28 0.08 Assists 0.37 0.22 Errors -3.36 4.39 NewLeagueN -24.76 79.00 --- n = 263, k = 20 residual sd = 315.58, R-Squared = 0.55 Forward selection settled on 19 predictors with model \\(R^2\\) of .55. display(step(lm(Salary ~ ., data = Hitters), trace = F, direction = &quot;backward&quot;)) lm(formula = Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + CRBI + CWalks + Division + PutOuts + Assists, data = Hitters) coef.est coef.se (Intercept) 162.54 66.91 AtBat -2.17 0.54 Hits 6.92 1.65 Walks 5.77 1.58 CAtBat -0.13 0.06 CRuns 1.41 0.39 CRBI 0.77 0.21 CWalks -0.83 0.26 DivisionW -112.38 39.21 PutOuts 0.30 0.07 Assists 0.28 0.16 --- n = 263, k = 11 residual sd = 311.81, R-Squared = 0.54 Backward selection settled on 10 predictors with \\(R^2\\) of .54. (The default setting in step() for direction is both, which returns the same result as the above.) This function certainly simplifies stepwise variable selection, but even the automated stepwise algorithm is not guaranteed to return the optimal model, as the result still depends on the sequence in which variables are entered into the model. Moreover, the fact that backward selection returned such a different model is concerning. Ideally, we do not want our model to depend on a methodological choicewe just want the best model. And in this case, while the larger model has a marginally higher \\(R^2\\), it is also much more complicated: does the better fit justify the additional complication? Probably not. With the bigger model we have likely crossed the line into overfitting, an issue we will take up when we discuss cross-validation. The regsubsets() function in the leaps package performs exhaustive search of the model space using the leaps algorithm for variable selection. library(leaps) plot(regsubsets(Salary ~ ., data = Hitters, method = &quot;exhaustive&quot;, nbest = 1)) The plot presents multiple candidate models organized by BIC on the y-axis. Like AIC, BIC penalizes for model complexity.2 Lower BIC is better. The model with the lowest BIC is the rather simple one at the top of the plot: Intercept, AtBat, Hits, Walks, CRBI, DivisionW and PutOuts. If we refit a model with these predictors using lm() we find it has an \\(R^2\\) of .51. Is this model really better? The algorithm did an exhaustive search of the model space yet returned a model with lower \\(R^2\\)! How could that be better? But it probably is better. While the bias in this model will be higher than in the larger model selected by the step() function, the variance is likely lower. Remember: bias refers to in-sample model performance and variance refers to the out-of-sample model performancehow the model does when it encounters new data. If the model performs poorly on new data, with a big discrepancy between in-sample and out-of-sample performance, then it is overfitting. AIC, BIC, and adjusted R-squared all penalize for model complexity in order to avoid overfitting and will tend to select models with higher bias and lower variance. 5.6 Cross validation Cross validation (CV) is the technique we use to assess whether a model is overfitting and to estimate how it will perform on new data. Overfitting is a major hazard in predictive analytics, especially when using machine learning algorithms like random forest which, without proper tuning, can learn sample data almost perfectly, essentially fitting noise. When such a model is used to predict new data, with different noise, model performance can be shockingly bad. We use CV to help us identify and avoid such situations. How so? Many machine learning algorithms require the user to specify certain parameters. In the case of random forest, for example, we need to specify values for \\(m\\), the number of randomly chosen predictors to be used at each tree split. The lower the \\(m\\), the simpler the tree. We can use CV to choose the value of \\(m\\) that minimizes variance and reduces overfitting. Linear regression has no user-specified parameters, but CV still helps us assess how much a model might be overfitting the sample data. The simplest version of CV is the so-called validation set method, consisting in the following steps: Split the sample data into two parts: a train set and a test set. Researchers use different proportions, but it is common to randomly select 70% of the data as the train set and 30% as the test or validation set. (Obviously, we must have enough data in the sample to fit a model after splitting the data.) Because CV relies on random sampling, our results will vary unless we use set.seed(). We will demonstrate using the Hitters data, using only complete cases. set.seed(123) Hitters_complete &lt;- Hitters[complete.cases(Hitters), ] rows &lt;- sample(nrow(Hitters_complete), .7 * nrow(Hitters_complete)) train &lt;- Hitters_complete[rows, ] test &lt;- Hitters_complete[-rows, ] Fit a model on the training set using an appropriate variable selection procedure. We will create two models for comparison: one with all the variables, then one with just the variables chosen by regsubsets(). full_model &lt;- lm(Salary ~., data = train) select_model &lt;- lm(Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts, data = train) Use that model to predict on the testing set. Performance on the test set is the CV estimate for the models out-of-sample performance. results &lt;- data.frame(Model = c(&quot;Full model in-sample&quot;, &quot;Select model in-sample&quot;, &quot;Full model out-of-sample&quot;, &quot;Select model out-of-sample&quot;), RMSE = round(c(rmse(fitted(full_model), train$Salary), rmse(fitted(select_model), train$Salary), rmse(predict(full_model, newdata = test), test$Salary), rmse(predict(select_model, newdata = test), test$Salary)),1)) results Model RMSE 1 Full model in-sample 297.8 2 Select model in-sample 326.1 3 Full model out-of-sample 368.2 4 Select model out-of-sample 306.4 We can see that the full model is overfittingin-sample RMSE is worse than out-of-sample RMSEwhile the select model chosen by regsubsets() using BIC is not overfitting. In fact, the select model actually does better out-of-sample than in-sample, though this particular result is likely a matter of chance, a function of random split we happen to be using. Generally, though, these results illustrate the danger of model complexity, and why it makes sense to choose predictors using measures of model fit that penalize for complexity. Simple models tend to generalize better. This figure depicts these relationships: As model complexity increases, the in-sample fit will likely keep getting better and better. But the out-of-sample fit starts getting worse at a certain threshold of complexity, as the model begins fitting noise in the sample. CV is designed to identify that threshold. 5.7 Cross-validation and Bootstrap The problem with this train-test CV procedure is that results can be quite variable due to the single random split defining the two sets. \\(K\\)-fold CV is designed to solve this problem. From Statistical Learning: This approach involves randomly dividing the set of observations into \\(k\\) groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining \\(k  1\\) folds. The mean squared error, \\(MSE_1\\),is then computed on the observations in the held-out fold. This procedure is repeated \\(k\\) times; each time, a different group of observations is treated as a validation set. This process results in \\(k\\) estimates of the test error, \\(MSE_1,MSE_2,...,MSE_k\\). The \\(k-fold\\) CV estimate is computed by averaging these values: \\(CV_k = \\sum_{i=1}^{k}MSE_i.\\)(181) There are different types of CV that we briefly describe here. 5.7.1 Leave-one-out cross validation (LOOCV) This method works as follows: Extract one observation from the data and use the rest to train the model Tests the model with the observation that has been extracted in the previous step and save the error associated with that prediction Repeat the process for all observations Calculate the global prediction error using the average of all the errors estimated in step 2. We will see later how to do these calculations with a specific library. For now, for you to learn how this methodology works, you must perform the following exercise EXERCISE (Deliver at Moodle: Exercise-LOOCV): Upload the R function. Create an R function that performs the LOOCV procedure and estimates the LOOCV value for the full model (e.g object full_model) and the selected model (e.g. object select_model) in the train data. HINT: use the function update () to re-evaluate the model in a new dataset. 5.7.2 K-fold cross validation (K-fold CV) The difference with LOOCV is that this method evaluates the behavior of the model in a data set of different size (K). The algorithm is as follows: Separate the data into k-subsets (k-fold) randomly Save one of the subsets of data and train the model with the rest of the individuals Tests the model with the reserved data and saves the average prediction error. Repeat the process until the k subsets have served as test sample. Calculate the average of the k errors that have been saved. This value is the cross-validation error and it helps us to evaluate the behavior of our model as if we were using it in an external database. The main advantage of this method over LOOCV is the computational cost. Another advantage that is not so obvious is that this method often gives better estimates of model error than LOOCV. A typical question is how to choose the optimal value of K. Small values of K give biased estimates. On the other hand, large K values are less skewed, but have a lot of variability. In practice, values of k = 5 or k = 10 are normally used, since these values have, empirically, estimated error rates that are not too biased or with too much variance. As in the previous case, we will see an R package to perform these analyzes efficiently. For now, do the following exercise: EXERECISE (Deliver at Moodle: Exercise-Kfold): Upload the R function. Create an R function that performs the K-fold CV procedure and estimates the value of value K-fold CV for the full model (e.g object full_model) and the selected model (e.g. object select_model) in the train data. The function should have a parameter that depends on K. Give the results for K = 5 and K = 10. HINT: use the function update () to re-evaluate the model in a new dataset. 5.7.3 Bootstrap Instead of dividing our sample into \\(K\\) sub-samples we can carry out a random selection of samples with replacement. These re-samples are called bootstrap tambples. This is a technique widely used in statistics to make inference when the distribution of the statistic is unknown. This will be further explained in the next letures, but here you have a simple description of this methodoloty. Boostrap Boostrap The bootstrap procedure applied to regression would be: Draw a random sample with replacement of size $ n $ from our data (we have $ n $ observations) Save samples that have not been selected (test data) Train the model with the sample * bootstrap * Tests the model with the test data and saves the average prediction error. Repeat the process $ B $ times Calculate the average of the $ B $ errors that have been saved. This value is the * bootstrap * error and it helps us to evaluate the behavior of our model. EXERCISE (Deliver at Moodle: Exercise-bootstrap): Upload the R function. Create a function R that implements the bootstrap procedure and estimate the value of this method for the full model (e.g object full_model) and the selected model (e.g. object select_model) in the train data. The function should have a parameter that depends on \\(B\\). Provide the the results for B = 25, B = 50, and B = 100. HINT: use the function update () to re-evaluate the model in a new dataset. 5.8 Example with caret library The caret R package (short for Classification And REgression Training) contains functions to streamline the model training process for complex regression and classification problems. Here there is an excellent bookdown describing how to do machine learning with R using different methods. By default caret uses 25 bootstrap samples rather than folds to perform model evaluation. Some data points will be left out of each bootstrap sample; caret uses those as the test set for estimating out-of-sample predictive error. library(caret) set.seed(123) train(Salary ~ ., data = train, method = &quot;lm&quot;) Linear Regression 184 samples 19 predictor No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 184, 184, 184, 184, 184, 184, ... Resampling results: RMSE Rsquared MAE 365.7867 0.4751905 261.7278 Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE The output that caret prints to the screen is not in-sample RMSE and \\(R^2\\) but is rather the CV estimate of out-of-sample error. Estimated out-of-sample RMSE for the full model is 391.19. Lets compare this result to the one for the select model. set.seed(123) train(Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts, data = train, method = &quot;lm&quot;) Linear Regression 184 samples 6 predictor No pre-processing Resampling: Bootstrapped (25 reps) Summary of sample sizes: 184, 184, 184, 184, 184, 184, ... Resampling results: RMSE Rsquared MAE 349.7814 0.5100381 246.7548 Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE Estimated out-of-sample RMSE for the select model is 362.17, which roughly agrees with the result we obtained using the validation set method: the simpler model has lower variance. And why do we care about lower variance? Because models that perform better on new data are less yoked to the idiosyncrasies of sample data and presumably doing a better job of describing the characteristics of the population. Such models are better at both inference and prediction. EXERCISE (Deliver Moodle: Exercise-create model): Load Boston data set (from MASS package) in R by executing data(&quot;Boston&quot;, package = &quot;MASS&quot;) Our aim is to create a model to predict the median house value (mdev), in Boston Suburbs, using the following predictor variables: crim: per capita crime rate by town. zn: proportion of residential land zoned for lots over 25,000 sq.ft. indus: proportion of non-retail business acres per town. chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise). nox: nitrogen oxides concentration (parts per 10 million). rm: average number of rooms per dwelling. age: proportion of owner-occupied units built prior to 1940. dis: weighted mean of distances to five Boston employment centres. rad: index of accessibility to radial highways. tax: full-value property-tax rate per $10,000. ptratio: pupil-teacher ratio by town. black: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town. lstat: lower status of the population (percent). medv: median value of owner-occupied homes in $1000s. Split the data in 70% (train) and 30% (test) Create a predictive model using a stepwise procedure Provide a goodness-of-fit value of the model using cross-validation in the train set Validate the model in the test dataset and compare model performance with the value obtained in the previous step TO DELIVER: Do the analyses using R Markdonw and upload the pdf (note: if you can only create the html file, use a web browser to export it to pdf.) 5.9 Missing data imputation Real-world datasets often have missing observations. The lm() function, for better or worse, silently removes rows with missing observations. Should we remove these rows or impute the missing observations? We are almost always better off imputing.3 While we can choose whether to impute in the case of linear regression, many machine learning applications require complete datasets: so we must impute. Missing data imputation is a large and complicated topic; the following discussion is very introductory. Types of missing values: Missing completely at random (MCAR): the probability that an observation is missing is the same for all cases. Deleting missing cases in this instance will not cause bias, though we may lose information. Missing at random (MAR): the probability that an observation is missing depends on a known mechanism. For example, some groups are less likely to answer surveys. If we know group membership we can delete the missing observations provided we include group as a factor in a regression. However, we can generally do better than just deleting such cases. Missing not at random (MNAR): the probability that an observation is missing depends on some unknown mechanisman unobserved variable. Dealing with MNAR problems is difficult or even impossible. In this discussion we we will focus on MAR problems. A simple solution is to fill in or impute the MAR values. There are two major strategies: Single imputation replaces missing values based on a univariate statistic or a multivariable regression model. The caret package will do single imputation with medians, KNN regression or random forest. The missForest package will do single imputation using random forest. In single imputation using medians we impute missing data using the median of the univariate column vector. (The median is better than the mean when the column data are skewed.) In single imputation using KNN or random forest we create a multivariable model of the missing observations using the other column vectors and use that model to predict the missing values. The problem with single imputation, theoretically, is that the variability of the imputed variable is lower than the variability in the actual variable would have been, creating a bias towards 0 in the coefficients. Thus, while deletion loses information, single imputation can cause bias. (It is not clear to me, however, how big a problem this actually is in practice.) Multiple imputation addresses these problems by imputing missing values with a multivariable model but adding the variability back in by re-including the error variation that we would normally see in the data. The multiple in multiple imputation refers to the multiple datasets created in the process of estimating regression coefficients. The steps are as follows: Create \\(m\\) complete datasets with imputed missing values. Imputations are done by randomly drawing from distributions of plausible values for each column vector. Fit a linear model on each imputed dataset,and store \\(\\hat\\beta\\)s and SEs. Average the \\(\\hat\\beta\\)s and combine the SEs to produce coefficients based on multiply imputed datasets.4 Multiple imputation works better for description than for prediction, and is probably preferrable to single imputation if we only want to estimate coefficients. For prediction it will usually be necessary to use single imputation. We will demonstrate imputation methods using the Carseats data from the ISLR package. This is a simulated dataset of carseat sales, from which we will randomly remove 25% of the observations using the prodNA() function in the missForest package (taking care to leave the outcome variable, Sales, intact). data(Carseats) levels(Carseats$ShelveLoc) &lt;- c(&quot;Bad&quot;,&quot;Medium&quot;,&quot;Good&quot;) # Relevel the factor library(missForest) set.seed(123) carseats_missx &lt;- prodNA(Carseats[,-1], noNA=.25) carseats_miss &lt;- cbind(Sales=Carseats[, 1], carseats_missx) glimpse(carseats_miss) Rows: 400 Columns: 11 $ Sales &lt;dbl&gt; 9.50, 11.22, 10.06, 7.40, 4.15, 10.81, 6.63, 11.85, 6.54, 4.69, 9.01, 11.96, 3.98, 10.96~ $ CompPrice &lt;dbl&gt; 138, 111, 113, 117, 141, 124, 115, NA, NA, NA, 121, 117, NA, 115, 107, NA, 118, NA, 110,~ $ Income &lt;dbl&gt; 73, 48, 35, 100, 64, 113, NA, 81, 110, 113, 78, 94, NA, 28, 117, 95, 32, 74, 110, 76, NA~ $ Advertising &lt;dbl&gt; 11, 16, NA, 4, 3, 13, NA, 15, 0, 0, 9, 4, 2, NA, 11, 5, NA, 13, 0, 16, 2, 12, 6, 0, 16, ~ $ Population &lt;dbl&gt; 276, 260, 269, NA, 340, 501, 45, 425, 108, 131, 150, 503, NA, 29, 148, 400, 284, 251, 40~ $ Price &lt;dbl&gt; 120, NA, NA, 97, 128, 72, 108, 120, NA, 124, 100, NA, NA, NA, 118, 144, 110, 131, 68, 12~ $ ShelveLoc &lt;fct&gt; Bad, NA, Good, NA, Bad, Bad, Good, NA, Good, Good, Bad, Medium, NA, Medium, Medium, Good~ $ Age &lt;dbl&gt; 42, 65, NA, 55, 38, NA, 71, 67, 76, 76, 26, 50, NA, 53, 52, 76, 63, 52, 46, 69, NA, NA, ~ $ Education &lt;dbl&gt; NA, 10, 12, NA, 13, 16, 15, 10, 10, 17, 10, 13, NA, NA, NA, 18, 13, 10, 17, 12, 18, NA, ~ $ Urban &lt;fct&gt; NA, Yes, Yes, Yes, Yes, NA, NA, Yes, No, NA, NA, Yes, Yes, Yes, Yes, No, Yes, Yes, No, N~ $ US &lt;fct&gt; Yes, Yes, Yes, Yes, No, Yes, No, Yes, NA, Yes, Yes, Yes, No, Yes, Yes, No, No, NA, Yes, ~ There are now many missing observations. When we fit a regression model of Sales, notice that lm() silently removes the rows with NAs, producing a model based on a very small subset of the data. display(lm(Sales ~ CompPrice + Income + Advertising + Population + Price, data = carseats_miss)) lm(formula = Sales ~ CompPrice + Income + Advertising + Population + Price, data = carseats_miss) coef.est coef.se (Intercept) 6.24 1.99 CompPrice 0.10 0.02 Income 0.01 0.01 Advertising 0.13 0.03 Population 0.00 0.00 Price -0.11 0.01 --- n = 93, k = 6 residual sd = 2.06, R-Squared = 0.59 Out of an original dataset of 400 we now only have 82 rows! We will demonstrate multiple imputation using the mice() function from the mice package. (mice stands for multiple imputation using chained equations.) library(mice) names(Carseats) [1] &quot;Sales&quot; &quot;CompPrice&quot; &quot;Income&quot; &quot;Advertising&quot; &quot;Population&quot; &quot;Price&quot; &quot;ShelveLoc&quot; [8] &quot;Age&quot; &quot;Education&quot; &quot;Urban&quot; &quot;US&quot; mice_imp &lt;- mice(carseats_miss, printFlag = F) The carseats_imp object created by mice() includes (among many other things) \\(m\\) imputed datasets (the default setting in mice is m = 5). The imputed datasets differ because the imputations are randomly drawn from distributions of plausible values. We can visualize the variability of the predictors in these imputed datasets using the densityplot() function. library(lattice) densityplot(mice_imp) The solid blue lines depict the actual distribution of the predictors, while the red lines show the imputed distributions. The next step is to use these imputed datasets to average the \\(\\hat\\beta\\)s and SEs using mices pool() function. mice_model_imp &lt;- with(data = mice_imp, exp = lm(Sales ~ CompPrice + Income + Advertising + Population + Price)) (mi &lt;- summary(pool(mice_model_imp))[, 2:6]) estimate std.error statistic df p.value 1 4.679006056 1.017480171 4.5986214 202.392447 7.486640e-06 2 0.099191783 0.009211103 10.7687199 103.974260 0.000000e+00 3 0.012721625 0.006268441 2.0294720 8.754973 7.387424e-02 4 0.131320342 0.016300491 8.0562203 230.153464 4.241052e-14 5 -0.000864611 0.001226849 -0.7047412 8.478900 4.998692e-01 6 -0.095475945 0.006019753 -15.8604416 85.633736 0.000000e+00 These coefficients are similar to the ones from the earlier model fitted using the non-imputed data, but they should be closer to population values because, rather than just removing the incomplete cases, instead uses distributional information to make educated guesses about missing data. Multiple imputation works best for purposes of descriptionestimating coefficients to report in an academic paper, for examplebut using it for prediction on new data is awkward or impossible, for the following reasons: If the new data is complete then we can use the coefficient estimates derived from multiple imputation in a regression equation for prediction. But this is a pain. We use the original Carseats data for illustration. preds &lt;- mi[1, 2] + mi[2, 2]*Carseats$CompPrice + mi[3, 2]*Carseats$Income + mi[4, 2]*Carseats$Advertising + mi[5, 2]*Carseats$Population + mi[6, 2]*Carseats$Price head(preds) [1] 3.986495 3.420226 3.252338 3.942853 3.953984 4.127971 If the new data is not complete then these multiply imputed coefficients are useless for predicting on rows with missing observations. This, for example, is the result of trying to predict using the carseats data with missing observations. preds &lt;- mi[1, 2] + mi[2, 2]*carseats_miss$CompPrice + mi[3, 2]*carseats_miss$Income + mi[4, 2]*carseats_miss$Advertising + mi[5, 2]*carseats_miss$Population + mi[6, 2]*carseats_miss$Price head(preds) [1] 3.986495 NA NA NA 3.953984 4.127971 Multiple imputation thus doesnt solve the major problem we often face with missing data, which is that although we may have successfully fit a model on the train set, the test set may also have missing observations, and our predictions using that data will also therefore be incomplete. We could use one of the imputed datasets produced by mice, but then we are not doing multiple imputation anymore but single imputation. At that point, the methods available in the mice package offer no special advantage over those in the caret and the missForest packages. Indeed, they might be worse since mice() was designed not to produce the single best imputation but rather a range of plausible imputations. Using caret, we can do single imputation using knnImpute, medianImpute, or bagImpute (random forest). While it is possible to impute inside the train() function using preProcess(), it is more straightforward to create a new dataset with imputed observatons. These methods only work for numeric variables, so we will create a custom function to turn the factorsShelveloc, Urban and USinto integers. (When using the imputed dataset for regression we could leave these variables as integers, as long as the integer values correspond to the factor levels.) make_df_numeric &lt;- function(df){ data.frame(sapply(df, function(x) as.numeric(x))) } carseats_miss_num &lt;- make_df_numeric(carseats_miss) med_imp &lt;- predict(preProcess(carseats_miss_num, method = c(&quot;medianImpute&quot;)), carseats_miss_num) knn_imp &lt;- predict(preProcess(carseats_miss_num, method = c(&quot;knnImpute&quot;)), carseats_miss_num) bag_imp &lt;- predict(preProcess(carseats_miss_num, method = c(&quot;bagImpute&quot;)), carseats_miss_num) The missForest package offers yet another single imputation solution, which is simpler than the caret functions because it handles categorical data automatically. While missForest works well for small datasets, and provides good quality imputations using multivariable random forest models, it will be very slow on large datasets. In fact, the same will be true for carets bagImpute() function, which also uses random forest. In such cases it might make sense to use carets medianImpute() function instead. mf_imp &lt;- missForest(carseats_miss, verbose = F) missForest iteration 1 in progress...done! missForest iteration 2 in progress...done! missForest iteration 3 in progress...done! missForest iteration 4 in progress...done! missForest iteration 5 in progress...done! missForest iteration 6 in progress...done! missForest iteration 7 in progress...done! The imputed dataset is stored in a list object (under ximp). Lets compare the errors associated with these different imputation methods. We can do this because, having created the missing observations in the first place, we can compare the imputed observations against the true observations by computing the sum of squares of the difference. For the imputations using mice() we calculate errors for each of the 5 imputed datasets. The results from knnImpute() are not comparable because the function automatically centers and scales variables; they have been omitted. comparison &lt;- data.frame(Method = c(&quot;mice 1&quot;, &quot;mice 2&quot;, &quot;mice 3&quot;, &quot;mice 4&quot;, &quot;mice 5&quot;, &quot;medianImpute&quot;, &quot;bagImpute&quot;, &quot;missForest&quot;), RSS = c(rss(make_df_numeric(complete(mice_imp, 1)), make_df_numeric(Carseats)), rss(make_df_numeric(complete(mice_imp, 2)), make_df_numeric(Carseats)), rss(make_df_numeric(complete(mice_imp, 3)), make_df_numeric(Carseats)), rss(make_df_numeric(complete(mice_imp, 4)), make_df_numeric(Carseats)), rss(make_df_numeric(complete(mice_imp, 5)), make_df_numeric(Carseats)), rss(med_imp, make_df_numeric(Carseats)), rss(bag_imp, make_df_numeric(Carseats)), rss(make_df_numeric(mf_imp$ximp), make_df_numeric(Carseats)))) comparison %&gt;% mutate(RSS = round(RSS)) %&gt;% arrange(RSS) Method RSS 1 missForest 2371284 2 medianImpute 2538059 3 bagImpute 2770171 4 mice 5 4226391 5 mice 4 4406142 6 mice 3 4494585 7 mice 2 4672300 8 mice 1 5180281 Missforest does the best, though medianImpute compares very well! Mice does not do well, probably for the reasons mentioned above: it is designed for multiple, not single, imputation. 5.10 Regularization Selecting variables using AIC or \\(R^2\\) is a discrete process: a variable is either in or out of the model. By contrast, methods are available that regularize or shrink coefficients towards zero and thereby achieve the same objective as discrete variable selection but in a continuous manner. The method works particularly well when there are large numbers of predictors. (In the wrong conditionssmall number of predictors, for exampleregularized models will actually do worse than ordinary least squares regression or OLS regression.) We will discuss two methods: ridge regression, which shrinks coefficients towards each other and towards zero, and lasso, which does the same thing but shrinks some coefficients all the way to zero, effectively taking those predictors out of the model.5 Ridge regression never completely removes predictors. Why would we want to shrink coefficients? Large coefficients tend to be artifacts of chanceof the fact that we happened to get this sample rather than another one. The world is a complex place, with many intersecting influences; it does not abound in strong relationships. Shrinking large coefficients will generally produce a model with higher bias but lower variance. We select a worse model in-sample so as to have a better model out-of-sample. Regularized models are particularly well-suited, consequently, for prediction problems. Ridge regression shrinks regression coefficients towards each other and towards zero by constraining their size. Remember: the least squares line in OLS regression is defined by the \\(\\beta_0\\) and \\(\\beta_j\\) that minimize RSS: \\[ \\min_{ \\beta_0, \\beta_j }\\left\\{ \\frac{1}{N} \\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} )^2 \\right\\} = \\min_{ \\beta_0, \\beta_j }\\left\\{RSS\\right\\} \\] We can think of the least squares algorithm as searching a large space of possibilities for the values of \\(\\beta_0\\) and \\(\\beta_j\\) that produce the lowest RSS. Ridge regression does the same thing thing but imposes a shrinkage penalty on RSS. \\[ \\min_{ \\beta_0, \\beta_j }\\left\\{RSS + \\lambda \\sum_{j=1}^p \\beta{_j^2} \\right\\} \\] where \\(\\lambda\\) is a tuning parameter. From Statistical Learning: As with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the \\(RSS\\) small. However, the second term, \\(\\lambda \\sum_j \\beta{_j^2}\\), called a shrinkage penalty,is small when \\(\\beta_1, ... , \\beta_j\\) are close to zero, and so it has the effect of shrinking the estimates of \\(\\beta_j\\) towards zero. The tuning parameter \\(\\lambda\\) serves to control the relative impact of these two terms on the regression coefficient estimates. When \\(\\lambda\\) = 0, the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as \\(\\lambda \\rightarrow \\infty\\), the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero. Unlike least squares, which generates only one set of coefficient estimates, ridge regression will produce a different set of coefficient estimates, \\(\\hat\\beta^r_\\lambda\\), for each value of \\(\\lambda\\). Selecting a good value for \\(\\lambda\\) is critical. [For that we use cross- validation.] (215) Lets examine how shrinkage works in practice. Consider a simple regression model with \\(\\beta_0\\) = -1 and \\(\\beta_1\\) = 2. x &lt;- c(1,2,3,4) y &lt;- c(1,4,3,8) ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = F) + ggtitle(&quot;OLS line: intercept = -1, slope = 2&quot;) The following table relates \\(\\beta_1\\) to RSS for three models: the OLS model from above (column 2) and then two different ridge models of the same data with different \\(\\lambda\\) (columns 3 and 4): tab &lt;- data.frame(Slope = seq(1.75,2.25,.05), rss = 0, rss2 = 0, rss3 = 0) names(tab)[2:4] &lt;- c(&quot;OLS RSS&quot;, &quot;Ridge RSS (lambda = 1)&quot;, &quot;Ridge RSS (lambda = 2)&quot;) for(i in 1:nrow(tab)){tab[i,2] &lt;- round(sum((-1 + tab$Slope[i]*x - y)^2) + 0*tab$Slope[i]^2, 2)} for(i in 1:nrow(tab)){tab[i,3] &lt;- round(sum((-1 + tab$Slope[i]*x - y)^2) + 1*tab$Slope[i]^2, 2)} for(i in 1:nrow(tab)){tab[i,4] &lt;- round(sum((-1 + tab$Slope[i]*x - y)^2) + 2*tab$Slope[i]^2 , 2)} tab Slope OLS RSS Ridge RSS (lambda = 1) Ridge RSS (lambda = 2) 1 1.75 7.88 10.94 14.00 2 1.80 7.20 10.44 13.68 3 1.85 6.68 10.10 13.52 4 1.90 6.30 9.91 13.52 5 1.95 6.07 9.88 13.68 6 2.00 6.00 10.00 14.00 7 2.05 6.07 10.28 14.48 8 2.10 6.30 10.71 15.12 9 2.15 6.67 11.30 15.92 10 2.20 7.20 12.04 16.88 11 2.25 7.88 12.94 18.00 The \\(\\beta_1\\) that minimizes RSS for the OLS model is 2. (OLS is identical to a ridge model with \\(\\lambda\\) = 0.) For the ridge models we can see that as \\(\\lambda\\) increases from 1 to 2, the shrinkage penalty grows, which has the effect of selecting smaller \\(\\beta_1\\)s. When \\(\\lambda\\) = 1 the optimal \\(\\beta_1\\) is 1.95, and when \\(\\lambda\\) = 2 the optimal \\(\\beta_1\\) somewhere between 1.85 and 1.9. x &lt;- c(1,2,3,4) y &lt;- c(1,4,3,8) ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = F) + geom_abline(slope = 1.95, intercept = -1, lty = 2) + geom_abline(slope = 1.9, intercept = -1, lty = 2) + ggtitle(&quot;OLS line compared to ridge estimates for lambda = 1 and lambda = 2&quot;) Another way to think about ridge regression is that it minimizes RSS subject to a constraint, \\(t\\), on the size of the square root of the squared and summed \\(\\beta\\) coefficients: \\[ \\min_{ \\beta_0, \\beta_j }\\left\\{RSS \\right\\} \\text{ subject to } \\sum_{j=1}^p ||\\beta_j||_2 \\leq t \\] \\(||\\beta_j||_2\\) is the \\(L_2\\) or Euclidean norm: \\(\\left\\| \\boldsymbol{x} \\right\\|_2 := \\sqrt{x_1^2 + \\cdots + x_n^2}\\). The constraint is like a budget that ensures the \\(\\beta\\) coefficients never get larger than a certain size. We pick the optimal \\(t\\), just as we would the optimal \\(\\lambda\\), through cross validation. We seek the value of \\(t\\) that minimizes estimated out-of-sample penalized error. Lasso regression also shrinks regression coefficients by constraining their size, but uses absolute value of \\(\\beta_j\\) in the penalty term. In technical terms: lasso uses the \\(L_1\\) norm instead of the \\(L_2\\) norm. The \\(L_1\\) norm is just the absolute value of the summed \\(\\beta_j\\)s rather than the squares. \\[ \\min_{ \\beta_0, \\beta_j }\\left\\{ \\frac{1}{N} \\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij} )^2 + \\lambda \\sum_{j=1}^p |\\beta{_j}|_1 \\right\\} = \\] \\[ \\min_{ \\beta_0, \\beta_j }\\left\\{RSS + \\lambda \\sum_{j=1}^p|\\beta{_j|_1} \\right\\} \\] where \\(\\lambda \\geq 0\\) is again a tuning parameter, which we choose using CV. Or, just as with ridge, we can think about lasso as minimizing RSS subject to a constraint, \\(t\\), on the size of the absolute value of the summed \\(\\beta\\) coefficients: \\[ \\min_{ \\beta_0, \\beta_j }\\left\\{RSS \\right\\} \\text{ subject to } \\sum_{j=1}^p |\\beta_j|_1 \\leq t \\] The difference between the \\(L_2\\) norm (used for ridge regression) and the \\(L_1\\) norm (used for lasso) may seem trivial but it accounts for the fact that lasso does not just shrink coefficients towards zero but actually sets some coefficients at zero. Say, for example, that the constraint on the coefficients for a model with two predictors is \\(t =1\\). For lasso this means that \\(|\\hat\\beta_1|\\) + \\(|\\hat\\beta_2| \\leq\\) 1. Examples: \\(|1| + |0| = 1\\) \\(|.5| + |.5| = 1\\) \\(|0| + |1| = 1\\) We can generalize and say that the shape of the lasso constraint for any \\(\\hat\\beta_1\\) + \\(\\hat\\beta_2\\) subject to \\(t \\leq 1\\) will be a square, whereas the shape of the ridge constraint will be a circle. Examples: \\(1^2 + 0 = 1\\) \\(.71^2 + .71^2 = .5 + .5 = 1\\) \\(0 + 1^2 = 1\\) Ridge coefficients will never equal 0 because, due to the circular shape of the constraint, they will always intersect the constraint at points where \\(\\hat\\beta_1\\) and \\(\\hat\\beta_2\\) are either greater than or less than 0. Not so for lasso. The following graphic from Statistical Learning shows the difference. The possible values for \\(\\hat\\beta\\) will touch the corners of the square (will equal 0) in the case of lasso, but never for ridge: the constraint will always intersect the possibilities for \\(\\hat\\beta\\) at some non-zero point. Both lasso and ridge regression models are simple to fit in caret using the glmnet() function. We must center and scale variables to use these methods. set.seed(123) (glmnet_model &lt;- train(Salary ~ ., data = train, preProcess = c(&quot;center&quot;, &quot;scale&quot;), method = &quot;glmnet&quot;)) glmnet 184 samples 19 predictor Pre-processing: centered (19), scaled (19) Resampling: Bootstrapped (25 reps) Summary of sample sizes: 184, 184, 184, 184, 184, 184, ... Resampling results across tuning parameters: alpha lambda RMSE Rsquared MAE 0.10 0.6140311 359.6662 0.4869600 256.2830 0.10 6.1403108 355.4081 0.4943610 250.1483 0.10 61.4031085 360.7901 0.4851299 243.6817 0.55 0.6140311 359.6364 0.4869292 256.1222 0.55 6.1403108 357.1621 0.4905829 249.5126 0.55 61.4031085 369.6322 0.4709253 247.5932 1.00 0.6140311 359.8132 0.4865187 256.1048 1.00 6.1403108 360.2626 0.4846552 249.8920 1.00 61.4031085 377.7654 0.4592662 253.8781 RMSE was used to select the optimal model using the smallest value. The final values used for the model were alpha = 0.1 and lambda = 6.140311. There are two user-specified parameters that caret sets using CV: lambda and alpha. Lambda is the shrinkage penalty. Caret searches over a small set of possibilities in this case.5, 5, and 50to find the lambda associated with the best out-of-sample performance, here 48.44. (We can specify a wider grid search for optimal lambda.) Alpha represents the mixing percentage between ridge and lasso. By default, glmnet() combines ridge and lasso in optimal proportions. We can force glmnet() to fit a ridge or lasso regression by specifying alpha = 0 (ridge) or alpha = 1 (lasso). set.seed(156) (ridge_model &lt;- train(Salary ~ ., data = train, preProcess = c(&quot;center&quot;, &quot;scale&quot;), method = &quot;glmnet&quot;, tuneGrid = expand.grid( alpha = 0, lambda = seq(150,200, 10)))) glmnet 184 samples 19 predictor Pre-processing: centered (19), scaled (19) Resampling: Bootstrapped (25 reps) Summary of sample sizes: 184, 184, 184, 184, 184, 184, ... Resampling results across tuning parameters: lambda RMSE Rsquared MAE 150 362.8204 0.4875400 241.9429 160 362.9125 0.4874825 241.8245 170 363.0041 0.4874261 241.7228 180 363.0942 0.4873715 241.6314 190 363.1854 0.4873158 241.5471 200 363.2765 0.4872630 241.4777 Tuning parameter &#39;alpha&#39; was held constant at a value of 0 RMSE was used to select the optimal model using the smallest value. The final values used for the model were alpha = 0 and lambda = 150. Extracting the coefficients for a glmnet() model from caret is sort of a pain. We first need to find the optimal lambda selected through CV, and then use that to pick out the best final model object. Here is the code: glmnet_model$finalModel$tuneValue alpha lambda 2 0.1 6.140311 coef(glmnet_model$finalModel, glmnet_model$finalModel$tuneValue$lambda) 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; s1 (Intercept) 545.463804 AtBat -218.230018 Hits 181.460199 HmRun -46.975785 Runs 5.316087 RBI 47.420469 Walks 134.468437 Years -126.192296 CAtBat -102.958094 CHits 125.279061 CHmRun 134.972795 CRuns 304.040262 CRBI 68.416918 CWalks -162.590400 LeagueN 7.336835 DivisionW -67.236684 PutOuts 54.635291 Assists 62.702663 Errors -52.727974 NewLeagueN 34.938002 Two things are going on here. First, the coefficients for all predictors have been shrunk towards 0, and, second, some predictors have been completely removed from the model by having their coefficients shrunk to all the way to 0. Is this continuous version of automatic variable selection better than the discrete version we used earlier with regsubsets()? Lets compare predictions on the test set. rmse(predict(select_model, newdata = test), test$Salary) [1] 306.4138 rmse(predict(glmnet_model, newdata = test), test$Salary) [1] 351.9049 Unfortunately, glmnet() did not live up to its billing in this case. Regularization tends to work best in high dimensional settings where manual variable selection is not possible, or where automatic discrete variable selection does not provide enough flexibility. Lets try using glmnet() to predict on a more challenging, high-dimensional dataset, the communities and crime dataset from UC Irvines machine learning repository. The data dictionary notes, the data combines socio-economic data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey, and crime data from the 1995 FBI UCR. There are 147 variables in the dataset with 2215 rows. We wont bother to add in predictor names. The final variable in the dataset, ViolentCrimesPerPop, is the outcome. We will exclude the first two columns which function as row names representing the cities and states with crime statistics in this dataset. crime_data &lt;- read.csv(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/00211/CommViolPredUnnormalizedData.txt&quot;, header = F, sep = &quot;,&quot;, quote = &quot;\\&quot;&quot;, dec = &quot;.&quot;, fill = TRUE, comment.char = &quot;&quot;, na.strings = &quot;?&quot;, strip.white=TRUE, stringsAsFactors = F) any(is.na(crime_data)) [1] TRUE There are missing observations. We could use missForest() for imputation, but given the high dimensionality of the data, this method will be very slow, if it works at all. We will instead use carets medianImpute() function for speed. crime_data &lt;- predict(preProcess(crime_data[, -c(1:2)], method = c(&quot;medianImpute&quot;)), crime_data[, -c(1:2)]) all(complete.cases(crime_data)) [1] TRUE set.seed(512) rows &lt;- sample(nrow(crime_data), .7*nrow(crime_data), replace = F) crime_train &lt;- crime_data[rows,] crime_test &lt;- crime_data[-rows,] crime_lm &lt;- lm(V147 ~., data = crime_train) crime_glmnet &lt;- train(V147 ~., data = crime_train, preProcess = c(&quot;center&quot;, &quot;scale&quot;), method = &quot;glmnet&quot;) rmse(predict(crime_lm, newdata = crime_test), crime_test$V147) [1] 542.0005 rmse(predict(crime_glmnet, newdata = crime_test), crime_test$V147) [1] 306.5808 In this case the regularized model outperforms the linear model. But does it outperform a model with discrete automatic variable selection? Exhaustive search using regsubsets() would not be computationally feasible. The model space consists in \\(2^{145}\\) models. From that perspective, lasso and ridge regression seem like pretty good alternatives. Using the step() function, however, remains possible. step_selection &lt;- step(crime_lm, data = crime_train, trace = 0) rmse(predict(step_selection, newdata = crime_test), crime_test$V147) [1] 489.2709 In this instance, regularized regression outperforms step selection also, which in this case is worse than the linear model with all predictors. How, additionally, does the regularized model compare to other popular machine learning algorithms like gradient boosting? crime_gbm &lt;- train(V147 ~., data = crime_train, method = &quot;gbm&quot;, verbose = F) rmse(predict(crime_gbm, newdata = crime_test), crime_test$V147) [1] 854.6857 It does better. In high dimensional settings, then, regularization is a good choice. EXERCISE (Deliver Moodle: Exercise-lasso): DNA methylation (DNAm) is a biological process by which methyl groups are added to the DNA molecule. Methylation can change the activity of a DNA segment without changing the sequence. In mammals, DNAm is almost exclusively found in CpG dinucleotides, with the cytosines on both strands being usually methylated. There are arrays that can measure 27K CpG sites for a given individual (current methods can obtain information up to 450K or 1M). The CpG variables are continuous variables with values between 0 (hypo-methylation) and 1 (hyper-methylation). Horvath hypothesized that DNAm age measures the cumulative effect of an epigenetic maintenance system and that it can predict all-cause mortality in later life. Using healthy individuals we can create a model to estimate age from DNAm data. Then, this model can be used to predict DNAm age in another individual and use the difference of this value with the biological age (i.e. age acceleration) as a biomarker of death. This task aims to illustrate how to perform such data modelling. The file methy_train.txt (espace delimited) contains information of 261 healthy individuals, their biological age and DNAm data corresponding to 25,978 CpG sites (variables starting by cg). Use this data to train a model to predict the age using DNAm data. You can use whatever technique you think is appropriate. You also have access to another file called methy_test.txt having the same information for 172 independent individuals. Both data files are available at: https://github.com/isglobal-brge/Master_Modelling/blob/main/data/methy_data.zip For further discussion, see Gelman chapter 4, page 69. \\(\\mathrm{BIC} = {\\ln(n)k - 2\\ln({L})},\\) where \\(L\\) is the maximum likelihood value, \\(n\\) is the number of observations, \\(k\\) is the number of parameters, and \\(ln\\) is the natural log. Practically speaking, though, imputing a few missing observations may not be worth the trouble since removing them will not usually change the fit at all. Specifically, \\(\\hat\\beta_{j} = \\frac{1}{m} \\sum_{i} \\hat\\beta_{ij}\\) and \\(s^2_j = \\frac{1}{m} \\sum_{i} s^2_{ij} + var \\hat\\beta_{ij} (1 + 1/m)\\), where \\(\\hat\\beta_{ij}\\) and \\(s_{ij}\\) are the estimates of and standard errors for the \\(i^{th}\\) imputed result for \\(i = 1,..., m\\) and for the \\(j^{th}\\) parameter. Lasso stands for Least Absolute Selection and Shrinkage Operator. "],["datashield.html", "6 DataSHIELD 6.1 Introduction 6.2 Opal 6.3 The Resources 6.4 DataSHIELD R Interface (DSI) 6.5 DataSHIELD/Opal Implementation 6.6 Demo: Basic statistical analyses 6.7 Tips and tricks", " 6 DataSHIELD 6.1 Introduction Some research projects require pooling data from several studies to obtain sample sizes large and diverse enough for detecting interactions. Unfortunately, important ethico-legal constraints often prevent or impede the sharing of individual-level data across multiple studies. DataSHIELD aims to address this issue. DataSHIELD is a method that enables advanced statistical analysis of individual-level data from several sources without actually pooling the data from these sources together. DataSHIELD facilitates important research in settings where: a co-analysis of individual-level data from several studies is scientifically necessary but governance restrictions prevent the release or sharing of some of the required data, and/or render data access unacceptably slow, equivalent governance concerns prevent or hinder access to a single dataset, a research group wishes to actively share the information held in its data with others but does not wish to cede control of the governance of those data and/or the intellectual property they represent by physically handing over the data themselves, a dataset which is to be remotely analysed, or included in a multi-study co-analysis, contains data objects (e.g. images) which are too large to be physically transferred to the analysis location. A typical DataSHIELD infrastructure (see Figure 6.1) is composed of one analysis node (the DataSHIELD client) connected to one or several data analysis nodes (the DataSHIELD servers). In each of these server nodes, there is an R server application which can only be accessed through a DataSHIELD-compliant middleware application. This middleware application acts as a broker for managing R server sessions in a multi-user environment, assigning data and launching analysis on the R server. The analysis execution environment is then fully controlled: users must be authenticated, must have the proper permissions to access the data of interest and can only perform some predefined assignment and aggregation operations. Importantly, the operations that are permitted are designed to prevent the user having access to individual data items while still allowing useful work to be done with the data. For example, users can fit a generalised linear model to a dataset and receive information about the model coefficients, but are not given the residuals, as these could be used to reconstruct the original data.The reference implementation of this DataSHIELD infrastructure is based on the Opal data repository. Figure 6.1: Typical DataSHIELD infrastructure, including one central analysis node (client) and several data nodes (servers). The client node interacts programmatically in R with the server nodes using the DataSHIELD Interface implemented as the DSI R package. The DSI defines prototype functions to authenticate the user and to perform assignment and aggregation operations in each of the R servers sitting in the server nodes. The reference implementation of DSI is the DSOpal R package. An alternate implementation of DSI is DSLite, an R package targetted at DataSHIELD developers by offering a lightweight, pure R implementation of the whole DataSHIELD infrastructure. 6.2 Opal 6.2.1 Introduction Opal is OBiBas core database application for epidemiological studies. Participant data, collected by questionnaires, medical instruments, sensors, administrative databases etc. can be integrated and stored in a central data repository under a uniform model. Opal is a web application that can import, process, copy data and has advanced features for cataloging the data (fully described, annotated and searchable data dictionaries) as recommended by the Maelstrom Research group at McGill University, Canada. Opal is typically used by a research center to analyze the data acquired from assessment centres. Its ultimate purpose is to achieve seamless data-sharing among epidemiological studies. Opal is the reference implementation of the DataSHIELD infrastructure. More information on Opal can be found in the Opal description on OBiBa. Opal provides the following main features: Use of MongoDB, Mysql, MariaDB and/or PostgreSQL as database software backends, Import of data from various file formats (CSV, SPSS, SAS, Stata etc.) and from SQL databases, Export of data to various file formats (CSV, SPSS, SAS, Stata etc.) and to SQL databases, Plugin architecture to extend import/export capabilities, for instance by connecting to data source software such as REDCap, LimeSurvey etc., Storage of data about any type of entity, such as subject, sample, geographic area, etc., Storage of data of any type (e.g., texts, numbers, geo-localisation, images, videos, etc.), Advanced authentication and authorization features, Reporting using R markdown, Data analysis plugins using R, Web services can be accessed using R, Python, Java, Javascript, DataSHIELD middleware reference implementation (configuration, access controls, R session management). 6.2.2 Data Management In Opal the data sets are represented by tables, which are grouped by projects. A table has variables (columns) and entity values (rows). Opal also has the concept of views, which are logical tables where the variables are derived from physical tables via scripts. The storage of the data and of the meta-data (data dictionaries) is managed in a database (for example, a SQL database such as MySQL, MariaDB or PostgreSQL, or a document-oriented database such as MongoDB). Detailed concepts and tutorials for tables can be found here: Variables and Data Identifiers Mappings Data Harmonization 6.2.3 Security All Opal operations are accessible through web services that require authentication and proper authorization. The permissions can be granted to a specific user or a group of users, can be applied to a project or to a table and have different levels: read-only meta-data (access to the data dictionary without access to the individual-level data), read-only, or write permissions. The programmatic authentication can make use of username/password credentials, token or 2-way SSL authentication methods. Opal can also integrate with the hosting institutions users registry using the OpenID Connect standard. 6.2.4 R Integration Opal connects to a R server to perform different kind of operations: data import/export (using R packages), data analysis (by transfering data from Opals database into a R server session and using R packages). The R server is based on the Rserve R package. The user R sessions that are running in this R server are managed by Opal. This Opal/R integration works well for small to mid-size datasets (usually less than 10M data points). For bigger datasets, extracting and transferring data from the database to the R server is time, CPU and memory intensive. In this work, we will present a more flexible data description paradigm called resources that enables Opal to manage access to Big Data sets, complex data structures and computation units for analysis purpose, while still having the security and the analysis features provided by Opal. 6.2.4.1 Opal demo site We have set up an Opal demo site to illustrate how to perform some basic analyses using DataSHIELD as well as how to deal with different resources for omic data. The Opal server can be accessed with the credentials: username: administrator password: password In this figure we can see all the projects available. Opal demo site available projects This vignette will mainly make use of the resources available at RSRC project Resources available at Opal demo site of RSRC project In order to make the reader familiar with Opal we recommend visiting the Opal online documentation. 6.3 The Resources Developing and implementing new algorithms to perform advanced data analyses under the DataSHIELD framework is a current active line of research. However, the analysis of big data within DataSHIELD has some limitations. Some of them are related to how data is managed in Opals database and others are related to how to perform statistical analyses of big data within the R environment. Opal databases are for general purpose and do not manage large amounts of information properly and, second, it requires data to be moved from original repositories into Opal which is inefficient (this is a time, CPU and memory intensive operation) and is difficult to maintain when data are updated. We have overcome the problem related to DataSHIELD big data management by developing a new data infrastructure within Opal: the resources. 6.3.1 Concept Resources are datasets or computation units whose location is described by a URL and access is protected by credentials. When assigned to a R/DataSHIELD server session, remote big/complex datasets or high performance computers are made accessible to data analysts. Instead of storing the data in Opals database, only the way to access it is defined: the datasets are kept in their original format and location (a SQL database, a SPSS file, R object, etc.) and are read directly from the R/DataSHIELD server-side session. Then as soon as there is a R reader for the dataset or a connector for the analysis services, a resource can be defined. Opal takes care of the DataSHIELD permissions (a DataSHIELD user cannot see the resources credentials) and of the resources assignment to a R/DataSHIELD session (see Figure 6.2) Figure 6.2: Resources: a new DataSHIELD infrastructure 6.3.2 Types The data format refers to the intrinsic structure of the data. A very common family of data formats is the tabular format which is made of rows (entities, records, observations etc.) and columns (variables, fields, vectors etc.). Examples of tabular formats are the delimiter-separated values formats (CSV, TSV etc.), the spreadsheet data formats (Microsoft Excel, LibreOffice Calc, Google Sheets etc.), some proprietary statistical software data formats (SPSS, SAS, Stata etc.), the database tables that can be stored in structured database management systems that are row-oriented (MySQL, MariaDB, PostgreSQL, Oracle, SQLite etc.) or column-oriented (Apache Cassandra, Apache Parquet, MariaDB ColumnStore, BigTable etc.), or in semi-structured database management systems such as the documented-oriented databases (MongoDB, Redis, CouchDB, Elasticsearch etc.). When the data model is more complex (data types and objects relationships), a domain-specific data format is sometimes designed to handle this complexity so that statistical analysis and data retrieval can be executed as efficiently as possible. Examples of domain-specific data formats that are encountered in the omic or geospatial fields of research that are described in the Workflows section: Omic and Geospatial. A data format can also include some additional features such as data compression, encoding or encryption. Each data format requires an appropriate reader software library or application to extract the information or perform data aggregation or filtering operations. We have prepared a demo environment, with the Opal implementation of Resources and an appropriate R/DataSHIELD configuration that is available at: opal-demo.obiba.org in a project called RSRC. This figure illustrate the resources which are available for this project and can serve as a starting example of the different types of resources that can be dealt with Figure 6.3: Resources from a test enviroment (project called RSRC) available at https://opal-demo.obiba.org As shown in this example, the data storage can simply be a file accessed directly from the hosts file system or downloaded from a remote location. More advanced data storage systems are software applications that expose an interface to query, extract or analyse the data. These applications can make use of a standard programming interface (e.g. SQL) or expose specific web services (e.g. based on the HTTP communication protocol) or provide a software library (in different programming languages) to access the data. These different ways of accessing the data are not exclusive from each other. In some cases the micro-data cannot be extracted, only computation services that return aggregated data are provided. The data storage system can also apply security rules, requiring authentication and proper authorisations to access or analyse the data. 6.3.3 Definition We define a resource to be a data or computation access description. A resource will have the following properties: the location of the data or of the computation services, the data format (if this information cannot be inferred from the location property), the access credentials (if some apply). The resource location description will make use of the web standard described in the RFC 3986 Uniform Resource Identifier (URI): Generic Syntax. More specifically, the Uniform Resource Locator (URL) specification is what we need for defining the location of the data or computation resource: the term Uniform allows to describe the resource the same way, independently of its type, location and usage context; the term Resource does not limit the scope of what might be a resource, e.g. a document, a service, a collection of resources, or even abstract concepts (operations, relationships, etc.); the term Locator both identifies the resource and provides a means of locating it by describing its access mechanism (e.g. the network location). The URL syntax is composed of several parts: a scheme, that describes how to access the resource, e.g. the communication protocols https (secured HTTP communication), ssh (secured shell, for issuing commands on a remote server), or s3 (for accessing Amazon Web Service S3 file store services), an authority (optional), e.g. a server name address, a path that identifies/locates the resource in a hierarchical way and that can be altered by query parameters. The resources data format might be inferred from the path part of the URL, by using the file name suffix for instance. Nevertheless, sometimes it is not possible to identify the data format because the path might only descripbe the data storage system, for example when a file store designates a document using an obfuscated string identifier or when a text-based data format is compressed as a zip archive. The format property can provide this information. Although the authority section of the URL can contain some user information (such as the username and password), it is discouraged to use this capability for security considerations. The resources credentials property will be used instead, and will be composed of an identifier sub-property and a secret sub-property, which can be used for authenticating with a username/password, or an access token, or any other credentials encoded string. The advantage of separating the credentials property from the resource location property is that a user with limited permissions could have access to the resources location information while the credentials are kept secret. Once a resource has been formally defined, it should be possible to build programmatically a connection object that will make use of the data or computation services described. This resource description is not bound to a specific programmatic language (the URL property is a web standard, other properties are simple strings) and does not enforce the use of a specific software application for building, storing and interpreting a resource object. The resourcer package is an R implementation of the data and computation resources description and connection. There the reader can see some examples of how dealing with different resources in DataSHIELD here. 6.4 DataSHIELD R Interface (DSI) The DataSHIELD Interface (DSI) defines a set of S4 classes and generic methods that can be implemented for accessing a data repository supporting the DataSHIELD infrastructure: controlled R commands to be executed on the server side that ensure only non disclosive information is returned to client side. 6.4.1 Class Structures The DSI S4 classes are: Class Description DSObject A common base class for all DSI, DSDriver A class to drive the creation of a connection object, DSConnection Allows the interaction with the remote server; DataSHIELD operations such as aggregation and assignment return a result object; DataSHIELD setup status check can be performed (dataset access, configuration comparision), DSResult Wraps access to the result, which can be fetched either synchronously or asynchronously depending on the capabilities of the data repository server. All classes are virtual: they cannot be instantiated directly and instead must be subclassed. See DSOpal for a reference implementation of DSI based on the Opal data repository. These S4 classes and generic methods are meant to be used for implementing connections to a DataSHIELD-aware data repository. 6.4.2 Higher Level Functions In addition to these S4 classes, DSI provides functions to handle a list of remote data repository servers: Functions Description datashield.login Create DSConnection objects for the data repositories, using the DSDriver specification. datashield.logout Destroy the DSConnections objects. datashield.aggregate, datashield.assign Typical DataSHIELD operations on DSConnection objects; results can be fetched through DSResult objects. datashield.connections, datashield.connections_default, datashield.connections_find Management of the list of DSConnection objects that can be discovered and used by the client-side analytic functions. datashield.workspaces, datashield.workspace_save, datashield.workspace_rm Manage R images of the remote DataSHIELD sessions (to speed up restoration of data analysis sessions). datashield.symbols, datashield.symbol_rm Minimalistic management of the R symbols living in the remote DataSHIELD sessions. datashield.tables, datashield.table_status List the tables and their accessibility across a set of data repositories. datashield.resources, datashield.resource_status List the resources and their accessibility across a set of data repositories. datashield.pkg_status, datashield.method_status, datashield.methods Utility functions to explore the DataSHIELD setup across a set of data repositories. These datashield.* functions are meant to be used by DataSHIELD packages developers and users. 6.4.3 Options Some options can be set to modify the behavior of the DSI: Option Description datashield.env The R environment in which the DSConnection object list is to be looked for. Default value is the Global Environment: globalenv(). datashield.progress A logical to enable visibility of progress bars. Default value is TRUE. datashield.progress.clear A logical to make the progress bar disappear after it has been completed. Default value is FALSE. datashield.error.stop A logical to alter error handling behavior: if TRUE an error is raised when at least one server has failed, otherwise a warning message is issued. Default value is TRUE. 6.5 DataSHIELD/Opal Implementation Opal is a web application that is accessible through web services. It implements DataSHIELD methods thanks to the following built-in features: integration with an R server, where the DataSHIELD operations will take place, secure data management, with fine-grained permissions (to restrict access to individual level data), web services API, that allows Opal operations to be run from an R script. In addition to these features, Opal manages the DataSHIELD configuration which consists of declaring the set of permitted aggregation/assignment R functions and some R options. 6.5.1 Client The opalr R package is a general purpose Opal connection R library (authentication is required) that is used to perform various operations (authorization may be required). The DSOpal R package is an implementation of the DSI, built on top of opalr. All the DataSHIELD operations are transparently applied to one or more Opal server using the DSI higher-level functions. Opal also supports asynchronous function calls (submission of a R operation, then later retrieval of the result) which allows operations on several DataSHIELD analysis nodes in parallel. 6.5.2 Server On the R server managed by Opal, some DataSHIELD-compliant R packages can be managed using the Opals web interface: installation, removal of DataSHIELD-compliant R packages and automatic DataSHIELD configuration discovery. Opal guarantees that only the allowed functions can be called. The DataSHIELD-compliant R package guarantees that only aggregated results are returned to the client. The term aggregated here means that the data in the R server will go through a function that summarizes individual-level data into a non-disclosive form. For example, obtaining the length of a vector, or obtaining the summary statistics of a vector (min, max, mean, etc.). These DataSHIELD functions are customisable. That is, administrators of the Opal server can add, remove, modify or create completely custom aggregating methods that are proposed to DataSHIELD clients. Figure 6.4: DataSHIELD configuration in Opal When performing a DataSHIELD analysis session, a typical workflow on a single Opal analysis node is the following: authentication of the user (requires authorization to use DataSHIELD service), creation and initialization of an R server session, assignment of Opal-managed data into the R server session (requires data access authorization), processing incoming R operation requests (aggregation and assignment function calls requires authorization) that are forwarded to the R server session; non-disclosive aggregated result is then returned to the R client. 6.6 Demo: Basic statistical analyses Let us start by illustrating how to peform simple statistical data analyses using different resources. Here, we will use data from three studies that are available in our Opal demo repository. The three databases are called CNSIM1, CNSIM2, CNSIM3 and are available as three different resources: mySQL database, SPSS file and CSV file (see Figure 6.3). This example mimics real situations where different hospitals or research centers manage their own databases containing harmonized data. Data correspond to three simulated datasets with different numbers of observations of 11 harmonized variables. They contain synthetic data based on a model derived from the participants of the 1958 Birth Cohort, as part of an obesity methodological development project. This dataset does contain some NA values. The available variables are: Variable Description Type Note LAB_TSC Total Serum Cholesterol numeric mmol/L LAB_TRIG Triglycerides numeric mmol/L LAB_HDL HDL Cholesterol numeric mmol/L LAB_GLUC_ADJUSTED Non-Fasting Glucose numeric mmol/L PM_BMI_CONTINUOUS Body Mass Index (continuous) numeric kg/m2 DIS_CVA History of Stroke factor 0 = Never had stroke; 1 = Has had stroke MEDI_LPD Current Use of Lipid Lowering Medication (from categorical assessment item) factor 0 = Not currently using lipid lowering medication; 1 = Currently using lipid lowering medication DIS_DIAB History of Diabetes factor 0 = Never had diabetes; 1 = Has had diabetes DIS_AMI History of Myocardial Infarction factor 0 = Never had myocardial infarction; 1 = Has had myocardial infarction GENDER Gender factor 0 = Female, 1 = Male PM_BMI_CATEGORICAL Body Mass Index (categorical) factor 1 = Less than 25 kg/m2; 2 = 25 to 30 kg/m2; 3 = Over 30 kg/m2 The analyses that are described here, can also be found in the DataSHIELD Tutorial where these resources here uploaded to the Opal server as three tables, an inferior approach since data have to be moved from their original repositories. 6.6.1 Analysis from a single study Let us start by illustrating how to analyze one data set (CNSIM2). library(DSOpal) library(dsBaseClient) # prepare login data and resource to assign builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.CNSIM1&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # login and assign resource conns &lt;- DSI::datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # coerce ResourceClient objects to a data.frame called &#39;D&#39; datashield.assign.expr(conns, symbol = &quot;D&quot;, expr = quote(as.resource.data.frame(res, strict=TRUE))) Then we can inspect the type of data we have ds.class(&quot;D&quot;) $study1 [1] &quot;data.frame&quot; ds.colnames(&quot;D&quot;) $study1 [1] &quot;id&quot; &quot;LAB_TSC&quot; &quot;LAB_TRIG&quot; &quot;LAB_HDL&quot; &quot;LAB_GLUC_ADJUSTED&quot; [6] &quot;PM_BMI_CONTINUOUS&quot; &quot;DIS_CVA&quot; &quot;MEDI_LPD&quot; &quot;DIS_DIAB&quot; &quot;DIS_AMI&quot; [11] &quot;GENDER&quot; &quot;PM_BMI_CATEGORICAL&quot; Perform some data descriptive analyses ds.table(&quot;D$DIS_DIAB&quot;) Data in all studies were valid Study 1 : No errors reported from this study $output.list $output.list$TABLE_rvar.by.study_row.props study D$DIS_DIAB study1 0 1 1 1 $output.list$TABLE_rvar.by.study_col.props study D$DIS_DIAB study1 0 0.98613037 1 0.01386963 $output.list$TABLE_rvar.by.study_counts study D$DIS_DIAB study1 0 2133 1 30 $output.list$TABLES.COMBINED_all.sources_proportions D$DIS_DIAB 0 1 0.9860 0.0139 $output.list$TABLES.COMBINED_all.sources_counts D$DIS_DIAB 0 1 2133 30 $validity.message [1] &quot;Data in all studies were valid&quot; ds.table(&quot;D$DIS_DIAB&quot;, &quot;D$GENDER&quot;) Data in all studies were valid Study 1 : No errors reported from this study $output.list $output.list$TABLE.STUDY.study1_row.props D$GENDER D$DIS_DIAB 0 1 0 0.502 0.498 1 0.700 0.300 $output.list$TABLE.STUDY.study1_col.props D$GENDER D$DIS_DIAB 0 1 0 0.9810 0.9920 1 0.0192 0.0084 $output.list$TABLES.COMBINED_all.sources_row.props D$GENDER D$DIS_DIAB 0 1 0 0.502 0.498 1 0.700 0.300 $output.list$TABLES.COMBINED_all.sources_col.props D$GENDER D$DIS_DIAB 0 1 0 0.9810 0.9920 1 0.0192 0.0084 $output.list$TABLE_STUDY.study1_counts D$GENDER D$DIS_DIAB 0 1 0 1071 1062 1 21 9 $output.list$TABLES.COMBINED_all.sources_counts D$GENDER D$DIS_DIAB 0 1 0 1071 1062 1 21 9 $validity.message [1] &quot;Data in all studies were valid&quot; Or even some statistical modelling. In this case we want to assess whether sex (GENDER) or triglycerides (LAB_TRIG) are risk factors for diabetes (DIS_DIAB) mod &lt;- ds.glm(DIS_DIAB ~ LAB_TRIG + GENDER, data = &quot;D&quot; , family=&quot;binomial&quot;) mod$coeff Estimate Std. Error z-value p-value low0.95CI.LP high0.95CI.LP P_OR (Intercept) -5.1696619 0.4549328 -11.363572 6.349427e-30 -6.0613138 -4.2780099 0.005654338 LAB_TRIG 0.3813891 0.1037611 3.675647 2.372471e-04 0.1780211 0.5847570 1.464317247 GENDER -0.2260851 0.4375864 -0.516664 6.053908e-01 -1.0837387 0.6315685 0.797650197 low0.95CI.P_OR high0.95CI.P_OR (Intercept) 0.002325913 0.01368049 LAB_TRIG 1.194850574 1.79455494 GENDER 0.338328242 1.88055787 As usual the connection must be closed datashield.logout(conns) 6.6.2 Analysis from a multiple studies Now, let us illustrate a similar analysis with multiple studies. In this case we see results aggregated across all three studies. library(DSOpal) library(dsBaseClient) # prepare login data and resources to assign builder &lt;- DSI::newDSLoginBuilder() builder$append(server = &quot;study1&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.CNSIM1&quot;, driver = &quot;OpalDriver&quot;) builder$append(server = &quot;study2&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.CNSIM2&quot;, driver = &quot;OpalDriver&quot;) builder$append(server = &quot;study3&quot;, url = &quot;https://opal-demo.obiba.org&quot;, user = &quot;dsuser&quot;, password = &quot;password&quot;, resource = &quot;RSRC.CNSIM3&quot;, driver = &quot;OpalDriver&quot;) logindata &lt;- builder$build() # login and assign resources conns &lt;- datashield.login(logins = logindata, assign = TRUE, symbol = &quot;res&quot;) # assigned objects are of class ResourceClient (and others) ds.class(&quot;res&quot;) $study1 [1] &quot;SQLResourceClient&quot; &quot;ResourceClient&quot; &quot;R6&quot; $study2 [1] &quot;TidyFileResourceClient&quot; &quot;FileResourceClient&quot; &quot;ResourceClient&quot; &quot;R6&quot; $study3 [1] &quot;TidyFileResourceClient&quot; &quot;FileResourceClient&quot; &quot;ResourceClient&quot; &quot;R6&quot; # coerce ResourceClient objects to data.frames # (DataSHIELD config allows as.resource.data.frame() assignment function for the purpose of the demo) datashield.assign.expr(conns, symbol = &quot;D&quot;, expr = quote(as.resource.data.frame(res, strict = TRUE))) ds.class(&quot;D&quot;) $study1 [1] &quot;data.frame&quot; $study2 [1] &quot;data.frame&quot; $study3 [1] &quot;data.frame&quot; # do usual dsBase analysis ds.summary(&#39;D$LAB_HDL&#39;) $study1 $study1$class [1] &quot;numeric&quot; $study1$length [1] 2163 $study1$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 0.875240 1.047400 1.300000 1.581000 1.844500 2.090000 2.210900 1.569416 $study2 $study2$class [1] &quot;numeric&quot; $study2$length [1] 3088 $study2$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 0.850280 1.032200 1.294000 1.563000 1.840000 2.077000 2.225000 1.556648 $study3 $study3$class [1] &quot;numeric&quot; $study3$length [1] 4128 $study3$`quantiles &amp; mean` 5% 10% 25% 50% 75% 90% 95% Mean 0.876760 1.039200 1.304000 1.589000 1.856000 2.098800 2.244200 1.574687 # vector types are not necessarily the same depending on the data reader that was used ds.class(&#39;D$GENDER&#39;) $study1 [1] &quot;integer&quot; $study2 [1] &quot;haven_labelled&quot; &quot;vctrs_vctr&quot; &quot;double&quot; $study3 [1] &quot;numeric&quot; ds.asFactor(&#39;D$GENDER&#39;, &#39;GENDER&#39;) $all.unique.levels [1] &quot;0&quot; &quot;1&quot; $return.message [1] &quot;Data object &lt;GENDER&gt; correctly created in all specified data sources&quot; ds.summary(&#39;GENDER&#39;) $study1 $study1$class [1] &quot;factor&quot; $study1$length [1] 2163 $study1$categories [1] &quot;0&quot; &quot;1&quot; $study1$`count of &#39;0&#39;` [1] 1092 $study1$`count of &#39;1&#39;` [1] 1071 $study2 $study2$class [1] &quot;factor&quot; $study2$length [1] 3088 $study2$categories [1] &quot;0&quot; &quot;1&quot; $study2$`count of &#39;0&#39;` [1] 1585 $study2$`count of &#39;1&#39;` [1] 1503 $study3 $study3$class [1] &quot;factor&quot; $study3$length [1] 4128 $study3$categories [1] &quot;0&quot; &quot;1&quot; $study3$`count of &#39;0&#39;` [1] 2091 $study3$`count of &#39;1&#39;` [1] 2037 mod &lt;- ds.glm(&quot;DIS_DIAB ~ LAB_TRIG + GENDER&quot;, data = &quot;D&quot; , family=&quot;binomial&quot;) mod$coeff Estimate Std. Error z-value p-value low0.95CI.LP high0.95CI.LP P_OR (Intercept) -4.7792110 0.21081170 -22.670521 8.755236e-114 -5.1923944 -4.36602770 0.00833261 LAB_TRIG 0.3035931 0.05487436 5.532514 3.156737e-08 0.1960414 0.41114488 1.35471774 GENDER -0.4455989 0.20797931 -2.142516 3.215202e-02 -0.8532309 -0.03796695 0.64044060 low0.95CI.P_OR high0.95CI.P_OR (Intercept) 0.005527953 0.01254229 LAB_TRIG 1.216577226 1.50854390 GENDER 0.426036242 0.96274475 datashield.logout(conns) 6.7 Tips and tricks The user can see in this link how to create and install an Opal server. Next we illustrate how to deal with some of the basics for setting up a server to be used within DataSHIELD environment. In order to do that, we are using our Opal demo server available here: https://opal-demo.obiba.org/ This is the how an Opal looks like once the user enters the credentials: username: administrator password: password Opal demo main page 6.7.1 How to create a new project into OPAL 6.7.1.1 Manually The tab Projects (top-left) goes to the projects available in the Opal demo Opal demo projects A new project can be created by clicking on +Add Project tab. Then this information must be filled in Adding a new project to Opal 6.7.1.2 Using R A project can be created using the following R code: library(opalr) o &lt;- opal.login(username=&quot;administrator&quot;, password=&quot;password&quot;, url=&quot;https://opal-demo.obiba.org&quot;) # A general purpose project, with a data storage opal.project_create(o, &quot;projectName&quot;, database=TRUE) # A project that can hold only resources and views (no persisted data) opal.project_create(o, &quot;projectName&quot;) opal.logout(o) See also the other opal.project_* functions. 6.7.2 How to upload a new resource into OPAL 6.7.2.1 Manually Once a new project has been created, a new resource can be uploaded by clicking on projects name. In this case, let us assume that we are working on RSRC project that has been created to illustrate the main examples in this bookdown. After clicking on that project this window will appear Tables, variables and resources from an Opal project Here we can observe that this project contains 16 resources a no tables or variables. We can add a new resource by clicking on the link tab (see red circle) Going to the resources of a given project Then a new resource can be added by clicking on the +Add Resource tab Adding a resource of a given project Then this window will appear and information for your resource must be filled in Adding a resource of a given project The different types of resources have been described Chapter ?? 6.7.2.2 Using R A resource can be added to a project by a simple function call, assuming that you know how to express the URL to the resource: library(opalr) o &lt;- opal.login(&quot;administrator&quot;,&quot;password&quot;, url=&quot;https://opal-demo.obiba.org&quot;) opal.resource_create(o, &quot;RSRC&quot;, &quot;CNSIM3&quot;, url = &quot;opal+https://opal-demo.obiba.org/ws/files/projects/RSRC/CNSIM3.zip&quot;, format = &quot;csv&quot;, secret = &quot;EeTtQGIob6haio5bx6FUfVvIGkeZJfGq&quot;) # to test the resource assignment opal.assign.resource(o, &quot;client&quot;, &quot;RSRC.CNSIM3&quot;) opal.execute(o, &quot;class(client)&quot;) opal.logout(o) See also the other opal.resource_*functions. 6.7.3 How to install DataSHIELD packages into OPAL server 6.7.3.1 Manually Administration tab at Opal The tab Administration (red circle in the previous figure) allows the user access to the administration page Administration tab at Opal The DataSHIELD tab goes to the DataSHIELD administration details Managing DataSHIELD packages in Opal The tab +Add Package allow the user to install a DataSHIELD package either from the DataSHIELD repository or any other GitHub site Install DataSHIELD packages in Opal 6.7.3.2 Using R The user can install DataSHIELD R packages from CRAN or GitHub using the following R code: library(opalr) o &lt;- opal.login(username=&quot;administrator&quot;, password=&quot;password&quot;, url=&quot;https://opal-demo.obiba.org&quot;) # CRAN dsadmin.install_package(o, &quot;dsBase&quot;) # GitHub dsadmin.install_github_package(o, &quot;packageName&quot;, username=&quot;orgOrUserName&quot;) opal.logout(o) When developing a new DataSHIELD package, it can be convenient to directly upload the built package archive for being installed on the R server side. This can be done as follow: library(opalr) o &lt;- opal.login(username=&quot;administrator&quot;, password=&quot;password&quot;, url=&quot;https://opal-demo.obiba.org&quot;) # build and install the package archive packageArchivePath &lt;- devtools::build(pkg=&quot;/path/to/the/packageName&quot;) dsadmin.install_local_package(o, path=packageArchivePath) opal.logout(o) 6.7.4 How to install R packages into OPAL server 6.7.4.1 Manually All the dependencies in a DataSHIELD package are automatically installed when installing it on the Opal Server. If necessary the user can also use the +Install button from the Administration/R tab Install DataSHIELD packages in Opal 6.7.4.2 Using R The user can install R packages from CRAN, Bioconductor or GitHub using the following R code: library(opalr) o &lt;- opal.login(username=&quot;administrator&quot;, password=&quot;password&quot;, url=&quot;https://opal-demo.obiba.org&quot;) # CRAN oadmin.install_package(o, &quot;cranPackageName&quot;) # Bioconductor oadmin.install_bioconductor_package(o, &quot;biocPackageName&quot;) # GitHub oadmin.install_github_package(o, &quot;packageName&quot;, username=&quot;orgOrUserName&quot;, ref = &quot;branchOrTagName&quot;) opal.logout(o) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
