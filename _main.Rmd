--- 
title: "Data Visualisation and Modelling"
author: "Juan R González"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::html_gitbook
documentclass: book
link-citations: yes
encoding: latin1
---


# Introducción

This bookdown is written as a supporting material for some of the lectures belonging to the subject [Data Visualisation and Modelling](https://guies.uab.cat/guies_docents/public/portal/html/2020/assignatura/43482/en) given at [Master for Modelling in Science and Engineering](https://www.uab.cat/web/estudiar/official-master-s-degrees/general-information/modelling-for-science-and-engineering-1096480962610.html?param1=1307112830469) from [Autonomous University of Barelona](https://www.uab.cat/web/universitat-autonoma-de-barcelona-1345467954774.html) (UAB).

The contents are:

* Dealing with big data analysis in R
     - Parallelization in R
     - MapReduce
     - Linear regression for Big Data

* Model fitting
     - Multivariate linear regression
     - General rules for variable selection
     - Stepwise variable selection
     - Comparing models
     - Automatic variable selection
     - Cross validation
     - K-fold and bootstrap cross validation
     - Missing data imputation
     - Regularization (Lasso and Elastic Net)
     
* DataSHIELD     

Some parts of this material are inspired in vignettes that are referenced at each chapter. Examples corresponding to model fitting are based on material from [Jeff Webb](https://github.com/jefftwebb). This material is licensed under [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).


![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFgAAAAfCAMAAABUFvrSAAAAIGNIUk0AAHolAACAgwAA+f8AAIDpAAB1MAAA6mAAADqYAAAXb5JfxUYAAAAEZ0FNQQAAsY58+1GTAAAAAXNSR0IB2cksfwAAAW5QTFRF////////////////7+/v39/f1tXV09bS0tXS0tXR0dTR0dTQ0NTQ0NPPz9PPztLOztHNzdHNzdHMz8/PzdDMzNDMzNDLzM/Ly8/Ly8/Ky87Kys3Jyc3Jyc3IyMzIyMzHx8vHxsrGxsrFxcnFxcnExMnExMjDw8jDxMfDw8fCwsfCwcXAwMXAwMW/wMS/v8S+v8O+vsO+vsK9vcK9vcK8v7+/vMG8vMG7vMC8u8C7u8C6ur+6ur+5ub65ub64uL23t7y2tru1tbq0tLqztLmzs7iysrixsrexsbewsbawsLavsLWvr7Wur7SusLOvrrStrrOtr7KvrbOsrLKrr6+vq7Gqn6OenqCdn5+flpmWk5iTkZSRkZORj4+PiYyJhIaEhIWEgoWCgICAfX98fH98eXx5cHJvcHBwYGBgXV5dUFFQUFBQQ0RDQEBAPj8+NTY1MjMxMDAwKSkpKCkoICAgGxsbEBAQDg4ODQ4NAAAAlzoSDQAAAAN0Uk5TAAoO5yEBUwAAAvhJREFUeNq1lutX2kAQxWmXFDVGYy1EIjQ2VZDiu1CsRQQURYvV+qSKj6II8rANYOT+9z0JqIASo9Y5ydkP2f2d2Ts7d2N4jRcJgwEIBwO+SbdTFGw8ZzZz1n5BdLgnfLPBcCT6fW1jY3P78QEYEA76PWMu0W5lGbrNZGrrYNg+u+ga9fgVcmxtY/NJZAOCfs+IY4Bn6eN8RdlEJX9Ed1uFIfdnfzC8uBJbv5tyqqhMLKa0wQHPiEOwMInLW4Eu9xmzfdDtmQ0uLK3cSXmvBBTS6QJQ2tMC+8YcgpnOApAzSa83mZEBZIff2odGfYFQJNqc8s4VchQhhFA5XO1pgCddAxaFKyeNpBpxGSgNmwXXxMxcWE25fkkJGUIIoExESQPsFnkmC0gUuQmjBGQZq+j2BEKR5dUGLVLIvbkGkxxSrcHO92wCkIyENJL3u+2O8Zng/FJsvR5cRF0GFIqtwaKVvoTcSxrCKOOS7hPdXwLhxUYtUFC+Z6AKQgpoDRZ6joEkaYo4cMQKril/KLLcCE4TVYmqFmkNsK0rD9lIiDdXKCSrwwEhREae6Ve0WIiuPg3M0xVlW171BBe21CGjbLbSYR0c/To3H409TQquHTggREKZ8pbjEiRqqxxXtWjjRLdvLrzUAK4Vr5qwZvEsJsCrzExWF9Tk9gIm84e74BRyRN9xeyS4vkHSmg1yK4Wxt5yUIClDayn0t3SteLWq3RQvjQrN31O87e2dEiBl0tJDJmTrykImN8dtq6AOpIw8Y3OMf2s+bvptU+hJqFrc1yCfpmZDkWYX0mv0H9WWpvS2tH6w8z27e58JJVi7c2ImuNBkQvrBOOWZc0CqsyFKtU3+97OuaQBnXGe90RuTMvCHtpziuWCcmDvPm64m+t2vlmuq/YHqqwnGCcfs1l+mCcbSmgtSe8iDGQNnPEsnrq//fZrltXS4tk3oAOPvT2tPF91uMrXTDNv340JrjQ4hbsHAxeE0z1ksHD99eKFdl0dl/P//Cl+9EPcfS+yBAoqk3eUAAAAASUVORK5CYII=)

 
```{r include=FALSE}
knitr::opts_chunk$set(cache=TRUE, warning=FALSE, message=FALSE, 
                      fig.path = "fig/", comment="")
library(arm)
library(ggplot2)
library(tidyr)
library(dplyr)
options(scipen = 0)
```
 

<!--chapter:end:index.Rmd-->

# Dealing with Big Data in R

In the era of big data, we can have access to large volumes of data obtained from our population of interest. Traditional algorithms implemented for even the simplest statistical methods (descriptive, linear regression, ...) require a lot of computing time. To address this issue, one of the approaches we have is to divide our data into smaller sets that do not require as much computational cost and to combine these results in a clever way that allows us to solve the largest problem. This can be done by parallelizing the calculations since even laptops already have multiple computing cores, and/or combining the calculations with paradigms such as *MapReduce* that has been designed to deal with Big Data efficiently. In this section we will learn how to:

- Parallelize in R
- Use MapReduce
- Implement multiple linear regression in a distributed system using MapReduce paradigm


## Nodes, cores, processes and threads

The terminology of nodes, cores, processes and threads is not universal. Depending on the computer, software (etc.), they can have various meanings. Typical examples: socket instead of node; cpu instead of core; task instead of process. Supercomputers have complex architectures, mainly due to their processors capability to work together on the same memory space. More precisely, the smallest computing units, called cores, are grouped in nodes. All the cores in one node share the same memory space. In other terms, the cores of the same node can operate on the same data, at the same time; no need for sending the data back and forth. This hardware architecture is summarized in this figure that shows a simple schematic of a 16-core node. The node contains two CPUs and each CPU consists of 8 cores. The schematic also shows the attached memory and the connections between the CPUs and memory.

![Nodes, cores and processors](figures/hpc_node-cpu-core.png){width=50%}



**Nodes**: Refers to the physical machine/server. In current systems, a node would typically include one or more processors, as well as memory and other hardware.

**Processor**: Refers to the central processing unit (CPU), which contains one or more cores.

**Cores**: Refers to the basic computation unit of the CPU. This is unit that carries out the actual computations.

So in essence, each compute node contains one or more processors/CPUs and each CPU will typically consist of one or more cores.



## Paralelización

When using a single CPU, or serial computing, the problem size is limited by the available memory and performance. As data sizes become larger and systems become more complex, programs/applications rapidly run out of resources. Effectively utilising parallel computing can overcome these limitations. Parallel computing has become essential to solving big problems (high resolution, lots of timesteps, etc.) in science and engineering.

Parallel computing can be simply defined as the simultaneous use of multiple processors/computers, i.e. parallel computers, to solve a computational problem. The general pattern is:

* The problem is broken down into discrete sections or separate tasks.
* Each processor works on its task or section of the problem. With multiple processors this means that several tasks can be processed at any given time.
* Processors exchange information with other processors, when required.

It allows leveraging the resources of multiple processors/computers, increasing the resources available to the application/software. This enables the execution of tasks that do not fit on a single CPU and the completion of the tasks in a reasonable time. This has many benefits. For instance, we can add more data points which can translate to the use of bigger domains, improved spatial resolution or the inclusion of more particles in a simulation. Faster execution time can translate to increased number of solutions in a given time or a faster time to solution.


### Shared Memory Programming
Parallel programming for shared memory machines is easier since the all cores have access to the same memory address space and so all have access to the same data structures. This greatly simplifies the task of parallelisation. Use can be made of auto-parallelisation via compiler options, loop-level parallelism through compiler directives or OpenMP. On the other hand, speedup and scalability are limited by the number of cores in the shared memory machine, and this is generally a relatively small number. In addition, code can only be used on a shared memory machine.

### Distributed Memory Programming
Programming for distributed memory machines provides a means to take advantage of more resources than those available on a shared memory machine. In addition, code developed for distributed memory machines can be used on shared memory machines. However, this type of programming is generally more difficult than shared memory programming. Since each processor only has access to its local memory, the programmer is responsible for mapping data structures across the separate nodes. In addition, there is a need to coordinate the communications between nodes, i.e. message passing, to ensure that a node can access remote data when it is needed for a local computation. The standard library used for this is MPI.

Next figure illustrate the difference between both approaches

![Memory Organization: (a) Shared Memory, (b) Distributed Memory](figures/shared_distributed_memory.jpg){width=70%}



Doing these tasks in R without strong knowledge in informatics can be hard. However, there are several R packages to perform parallel computing. The reason for using `doParallel` package, and not `parallel`, is that the  `parallel` package is not working entirely on Windows and you had to write different code for it to work. The `doParallel` package is trying to make it happen on all platforms: UNIX, LINUX and WINDOWS, so it’s a pretty decent wrapper. To me, the most simple way of doing parallelization is to use `mclapply()` function from `parallel` but this cannot be used in Window. 


Let us assume we want to compute $f(x)=x^2 + x$ of 10 numbers stored in a vector called `vec`. 

```{r}
set.seed(1234)
f <- function(x) x^2 + x
vec <- rpois(10, lambda=200)
```


We can do the following strategies:

1. Looping

```{r loop}
forFunction <- function(x) {  
  ans <- rep(NA, length(x))
  for(i in vec)
   {
    ans[i] <- f(i)
  }
  ans
}
```

2. Using `lapply ()` or `sapply ()` function

```{r lapply}
lapplyFunction <- function(x) {
  ans <- sapply(x, f)
  ans
}
```




3. Using  `doParallel::parLapply()` function

We need first to create the cluster

```{r createCluster, eval=FALSE}
library(doParallel)
ncores <- detectCores() - 1  
registerDoParallel(cores=ncores)  
cl <- makeCluster(ncores) 
```

Then, we can use the parallel implementation of `lapply` 

```{r doParallel_parLapply}
parLapplyFunction <- function(cl, x, f){
  result <- parLapply(cl=cl, X=x, fun=f)  
  result
}
```  


4. Using  `doParallel::foreach()` function

```{r foreach}
foreachDoParFunction <- function(x) {
  result <- foreach(i=x, .export="f") %dopar% f(i)
  result
}  
```

```{r foreach2}
foreachDoFunction <- function(x) {
  result <- foreach(i=x, .export="f") %do% f(i)
  result
}  
```


5. Using  `parallel::mclapply()` function

```{r mclapply, eval=FALSE}
# Only works in Linux (Windows ncores must be set equal to 1)
result <- mclapply(x, f, mc.cores=ncores)
```

In order to compare computation time, we can run

```{r system}
system.time(result <- lapply(vec, f))
```


Nonetheless, `rbenchmark` function serves as a more accurate replacement of the often seen `system.time()` function and the more sophisticated `system.time(replicate(1000, expr))` expression (that incorporates variability). It tries hard to accurately measure only the time it takes to evaluate expr. To achieved this, the sub-millisecond (supposedly nanosecond) accurate timing functions most modern operating systems provide are used. Additionally all evaluations of the expressions are done in C code to minimize any overhead. In our example:


```{r }
library(rbenchmark)
library(doParallel)

ncores <- detectCores() - 1  
registerDoParallel(cores=ncores)  
cl <- makeCluster(ncores) 

testdata1 <- benchmark("For loop" = forFunction(vec),
                      "lapply" = lapplyFunction(vec),
                      "Foreach dopar" = foreachDoParFunction(vec),
                      "Foreach do" = foreachDoFunction(vec),
                      "parLapply" = parLapplyFunction(cl=cl, x=vec, f=f),
                      columns=c('test', 'elapsed', 'replications'),
                      replications = c(100, 200, 300, 400, 500))




ggplot() +
  geom_line(aes(x = replications, y = elapsed, colour = test), data = testdata1)
```


Another example could be to compare the performance of the five methods for matrix multiplication

```{r}
set.seed(12345)
A <- matrix(rnorm(20), nrow=4, ncol=5)
B <- matrix(rnorm(20), nrow=4, ncol=5)

FUN <- function(i, A, B){
  crossprod(A,B)
}

a <- as.list(1:10)
testdata2 <- benchmark("For loop" = for(i in 1:length(a)){FUN(i, A, B)},
                       "lapply" = lapply(a, FUN = FUN, A=A, B=B), 
                       "Foreach dopar" = foreach(i = 1:10) %dopar% FUN(i, A, B),
                       "Foreach do" = foreach(i = 1:10) %do% FUN(i, A, B),
                       "parLapply" = parLapply(cl = cl, X = a, fun = FUN, A=A, B=B),
                       "parSapply" = parSapply(cl = cl, X = a, FUN = FUN, A=A, B=B),
                       columns=c('test', 'elapsed', 'replications'),
                       replications = c(100, 200, 300, 400, 500))

ggplot() +
  geom_line(aes(x = replications, y = elapsed, colour = test), data = testdata2)
```


Finally, we could also compare the performance of the five methods for fitting generalized linear model

```{r}
FUN <- function(i) {
  ind <- sample(100, 100, replace=TRUE)
  mod <- glm(Species ~ Sepal.Length, family=binomial(logit), data = iris[ind,])
  coefficients(mod)
}

a <- as.list(1:10)
testdata3 <- benchmark("For loop" = for(i in 1:length(a)){ FUN(a[[i]])},
                       "lapply" = lapply(a, FUN = FUN), 
                       "Foreach dopar" = foreach(i = 1:10) %dopar% FUN(i),
                       "Foreach do" = foreach(i = 1:10) %do% FUN(i),
                       "parLapply" = parLapply(cl = cl, X = a, fun = FUN),
                       "parSapply" = parSapply(cl = cl, X = a, FUN = FUN),
                       columns=c('test', 'elapsed', 'replications'),
                       replications = c(100, 200, 300, 400, 500))

ggplot() +
  geom_line(aes(x = replications, y = elapsed, colour = test), data = testdata3)

stopCluster(cl)
```

To sum up, generally, `parLapply ()` perform better than `foreach ()`. However, for all parallel implementation methods, the increase in terms of efficiency is not proportional to the number of cores being used (Theoretical efficiency).



## MapReduce

MapReduce is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster. A MapReduce program is composed of a map procedure, which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method, which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The "MapReduce System" (also called "infrastructure" or "framework") orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance. The model is a specialization of the split-apply-combine strategy for data analysis. It is inspired by the map and reduce functions commonly used in [functional programming](https://en.wikipedia.org/wiki/Functional_programming). There are two main frameworks to deal with Big Data and where MapReduce can be applied efficiently 

- **[Hadoop](https://hadoop.apache.org/)** provides support to perform MapReduce operations over a distributed file system of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. 
- **[Spark](https://spark.apache.org/)** provides a richer set of verbs beyond MapReduce to facilitate optimizing code running in multiple machines. Spark also loaded data in-memory, making operations much faster than Hadoop’s on-disk storage. 


Thre are some pacakges to connect R and both [Hadoop](https://datascienceplus.com/integrating-r-with-apache-hadoop/) and [Spark](https://spark.apache.org/docs/latest/sparkr.html), but they use is beyond the scope of this short introduction to Big Data analysis.


Next figure illustrate how to use MapReduce to count words in two different text files stored in different machines. The map operation splits each word in the original file and outputs a new word-counting file with a mapping of words and counts. The reduce operation can be defined to take two word-counting files and combine them by aggregating the totals for each word; this last file will contain a list of word counts across all the original files. Counting words is often the most basic MapReduce example, but we can also use MapReduce for much more sophisticated and interesting applications in statistics.

![MapReduce example counting words across files](figures/mapreduce.png){width=60%}
The MapReduce paradigm has long been a staple of big data computational strategies. However, properly leveraging MapReduce in R can be a challenge, even for experienced  users. To get the most out of MapReduce, it is helpful to understand its relationship to functional programming. Functional programming, in a broad sense, is the one that some functions allows to have another function in one of its arguments. For instance, the function `sapply()`:


```{r}
ff <- function(x) if(x > 0) log(x) else log(-x)^2
sapply(-4:10, ff)
```


Functional programming is a very powerful tool that allows you to program in the following way:

- Create small and simple functions that solve a small and bounded problem
- Apply these functions to homogeneous groups of values.

In the previous example, we have built an `ff` function and through the `sapply ()` function we have applied it to a list of homogeneous values: the numbers from -4 to 10.

There are many functions in R, some of which you have already seen, that take others as arguments. Some of the most common are:

- sapply y lapply
- tapply
- apply y mapply
- Las funciones ddply, ldply, etc. del paquete `plyr`


A very common example of this type of functions is used to inspect the type of columns in a table since they take advantage of the fact that a table is a list of columns and go through them one by one

```{r}
lapply(iris, class)
sapply(iris, length)
```

One of the advantages of this type of programming is that the code is shorter and more readable. We must remember that these functions include the argument `...` that allows to pass additional arguments to the function they call. For example this function would do the same as the previous one, but it would be more generic since it would allow calculations by varying the `s` argument.


```{r}
ff2 <- function(x, s) {
  if(x > s) log(x) else log(-x)^2
}
sapply(-4:10, ff2, s=0)
```


### Map

The MapReduce methodology is also implemented in base R (`Map ()` and `Reduce ()` functions) as well as in `tidyverse`. Function `Map ()` applies one function to all elements from a list o vector:

```map(YOUR_LIST, YOUR_FUNCTION)```

Previous operations could also be executed by usint this code (`sapply ()` es un caso especial de la función `Map ()`):

```{r}
Map(ff, c(9, 16, 25))
```

y

```{r}
Map(ff2, c(9, 16, 25), s=0)
```

Another advantage appears when we want to **vary more than one argument**. With `lapply ()`, only one argument varies; the others are fixed. For example, how would you find a weighted mean when you have two lists, one of observations and the other of weights?


```{r}
# Generate some sample data
xs <- replicate(5, runif(10), simplify = FALSE)
ws <- replicate(5, rpois(10, 5) + 1, simplify = FALSE)
str(xs)
str(ws)

# compute the weighted.mean
unlist(Map(weighted.mean, xs, ws))
```

If some of the arguments should be fixed and constant, use an anonymous function:

```{r}
Map(function(x, w) weighted.mean(x, w, na.rm = TRUE), xs, ws)
```

### Reduce

Another way of thinking about functionals is as a set of general tools for altering, subsetting, and collapsing lists. Every functional programming language has three tools for this: `Map()`, `Reduce()`, and `Filter()`. We have seen `Map()` already, and next we describe `Reduce()`, a powerful tool for extending two-argument functions. `Filter()` is a member of an important class of functionals that work with predicates, functions that return a single TRUE or FALSE (we will not cover that).

`Reduce()` reduces a vector, x, to a single value by recursively calling a function, f, two arguments at a time. It combines the first two elements with f, then combines the result of that call with the third element, and so on. Calling `Reduce(f, 1:3)` is equivalent to `f(f(1, 2), 3)`. Reduce is also known as fold, because it folds together adjacent elements in the list.

The following two examples show what Reduce does with an infix and prefix function:

```{r}
Reduce(`+`, 1:3) # -> ((1 + 2) + 3)
Reduce(sum, 1:3) # -> sum(sum(1, 2), 3)
```

The essence of `Reduce()` can be described by a simple for loop:

```
Reduce2 <- function(f, x) {
  out <- x[[1]]
  for(i in seq(2, length(x))) {
    out <- f(out, x[[i]])
  }
  out
}
```


`Reduce()` is also an elegant way of extending a function that works with two inputs into a function that can deal with any number of inputs. It is useful for implementing many types of recursive operations, like merges and intersections. Imagine you have a list of numeric vectors, and you want to find the values that occur in every element:

```{r} 
l <- replicate(5, sample(1:10, 15, replace = T), simplify = FALSE)
str(l)
```

You could do that by intersecting each element in turn:

```{r}
intersect(intersect(intersect(intersect(l[[1]], l[[2]]),
                              l[[3]]), l[[4]]), l[[5]])
``` 

That’s hard to read. With `Reduce()`, the equivalent is:

```{r}
Reduce(intersect, l)
```




## Linear regression for Big Data

In this section, we describe a very nice application of MapReduce framework to a Big Data problem. 

There are several problems that require the analysis of large volumes of information. The analysis at very large scale of data is a challenging task since the available information cannot be practically analyzed on a single machine due to the sheer size of the data to fit in memory. In order to overcome this difficulty, high-performance analytical systems running on distributed environments can be used. To this end standard analytics algorithms need to be adapted to take advantage of cloud computing models which provide scalability and flexibility. Here, we describe an approach that introduces a new distributed training method for Multiple Linear Regression which will be based on the QR decomposition and the ordinary least squares method adapted to MapReduce framework. The method is called MLR-MR and is described in ([Moufida Adjout Rehab and Faouzi Boufares, 2105](https://ieeexplore.ieee.org/document/7345473)). The paper is also available in our Moodle.

In this figure we can observe as the model fitting using MLR-MR algorithm is dramatically reduced when the number of MapReduce processors increases. 



![Speedup training MLR-MR with different MapReduce working nodes](figures/MLR-MR_time.png){width=50%}




Let us start by recalling how to describe linear regression using the classical matrix notation: 

$$\mathbf{Y}=\mathbf{X}\mathbf{\beta}+\mathbf{\varepsilon}.$$  

The ordinary least square (OLS) estimate of $\mathbf{\beta}$ is $$\widehat{\mathbf{\beta}}=[\mathbf{X}^T\mathbf{X}]^{-1}\mathbf{X}^T\mathbf{y}$$. To illustrate, let us consider the "mtcars" example, and run this regression:

```{r}
data(mtcars)
mod <- (lm(mpg ~ wt + cyl, data = mtcars))
```


The algorithm implemented in the `lm ()` function uses the QR decomposition of $\mathbf{X},$ $$\mathbf{X}=\mathbf{Q}\mathbf{R},$$ where $\mathbf{Q}$ is an orthogonal matrix (i.e. $\mathbf{Q}^T\mathbf{Q}=\mathbb{I}$).Then,

$$\widehat{\mathbf{\beta}}=[\mathbf{X}^T\mathbf{X}]^{-1}\mathbf{X}^T\mathbf{y}=\mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}$$

```{r}
Y <- mtcars$mpg
#similar to cbind(1, mtcars$wt, mtcars$cyl)
X <- model.matrix(~ wt + cyl, data=mtcars)
QR <- qr(as.matrix(X))
R <- qr.R(QR)
Q <- qr.Q(QR)
solve(R) %*% t(Q) %*% Y
```

We can parallelise computations using the MLR-MR method as follows:


Consider $m$ blocks, for instance 3 (given tha we have 4 cores)

```{r}
m <- 3
```


and split vectors and matrices

$$\mathbf{y}=\left[\begin{matrix}\mathbf{y}_1\\\mathbf{y}_2\\\vdots \\\mathbf{y}_m\end{matrix}\right]$$

and 

$$\mathbf{X}=\left[\begin{matrix}\mathbf{X}_1\\\mathbf{X}_2\\\vdots\\\mathbf{X}_m\end{matrix}\right]=\left[\begin{matrix}\mathbf{Q}_1^{(1)}\mathbf{R}_1^{(1)}\\\mathbf{Q}_2^{(1)}\mathbf{R}_2^{(1)}\\\vdots \\\mathbf{Q}_m^{(1)}\mathbf{R}_m^{(1)}\end{matrix}\right]$$

In R to split vectors and matrices we can use these two functions:


```{r}
chunk <- function(x,n) split(x, cut(seq_along(x), n, labels = FALSE)) 

splitMatByRow = function(mat, size){
  row.index <- chunk(1:nrow(mat), size)
  lapply(row.index, function(val) mat[val, ])
}
```


We can do that with our data


```{r}
x.block <- splitMatByRow(X, m)
y.block <- splitMatByRow(matrix(Y, ncol = 1), m)
```

Then, we get small QR decomposition (per subset). This step correspond to de **Map** step in the `MapReduce` framework

```{r}
# Algorithm 2 Mapper function in step1
x.block.qr <- lapply(x.block, function(val){
  qrresult <- qr(val)
  list(Q=qr.Q(qrresult), R=qr.R(qrresult))
})
```

Now, consider the QR decomposition of $\mathbf{R}^{(1)}$ which is the first step of the reduce part 

$$\mathbf{R}^{(1)}=\left[\begin{matrix}\mathbf{R}_1^{(1)}\\\mathbf{R}_2^{(1)}\\\vdots \\\mathbf{R}_m^{(1)}\end{matrix}\right]=\mathbf{Q}^{(2)}\mathbf{R}^{(2)}$$ where

$$\mathbf{Q}^{(2)}=\left[\begin{matrix}\mathbf{Q}^{(2)}_1\\\mathbf{Q}^{(2)}_2\\\vdots\\\mathbf{Q}^{(2)}_m\end{matrix}\right]$$

that can be computed as 


```{r}
# Algorithm 3 Reducer function in step1
Rtemp <- do.call(rbind, lapply(x.block.qr, function (l) l$R))
qrresult <- qr(Rtemp)
Rtemp.qr <- list(Q=qr.Q(qrresult), R=qr.R(qrresult))

R.final <- Rtemp.qr$R
Rtemp.Q.divide <- splitMatByRow(Rtemp.qr$Q, m)


Q.result = list()
for (i in 1:m){
  Q.result[[i]] <- x.block.qr[[i]]$Q %*% Rtemp.Q.divide[[i]]
}
```

Define – as step 2 of the reduce part

$$\mathbf{Q}^{(3)}_j=\mathbf{Q}^{(2)}_j\mathbf{Q}^{(1)}_j$$

and

$$\mathbf{V}_j=\mathbf{Q}^{(3)T}_j\mathbf{y}_j$$

```{r}
# Algorithm 4 Reduce function in step2
V = list()
for (i in 1:m){
  V[[i]]  <-  crossprod(Q.result[[i]], y.block[[i]])  
}
```


and finally set – as the step 3 of the reduce part

$$\widehat{\mathbf{\beta}}=[\mathbf{R}^{(2)}]^{-1}\sum_{j=1}^m\mathbf{V}_j$$


```{r}
# Algorithm 5 Reduce function in step3
V.sum <- Reduce('+', V)
beta <- as.numeric(solve(R.final) %*% V.sum)
```


Let us compare the results

```{r}
cbind(lm=coef(mod), parallel=beta)

# error
sum((beta - coef(mod)) ** 2)
```

<!--chapter:end:01-paralelizacion.Rmd-->

# Linear models

Consider the study of the effect of a new drug for hemophilia, by analyzing the level of blood coagulation after the administration of various amounts of the new drug. Researchers  may be interested in knowing whether the drug affects on the level of blood coagulation. The simplest statistical model to address this scientific question is the linear regression model

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i,
$$

where $i = 1, \ldots, n$ and $i$ denotes the $i$-th observation. 

However, the response variable ($y$) can be explained by a different independent variables ($x$). In other words, the response variable can be the sum of effects of some independent factors ($x=(x_1,\dots,x_p)$). 

The model assumption implies that the expected response is the sum of the factors effects:

$$
\begin{align}
  E[y]=x_1 \beta_1 + \dots + x_p \beta_p = \sum_{j=1}^p x_j \beta_j = x'\beta .
  \tag{7.1}
\end{align}
$$

Clearly, there may be other factors that affect the the level of blood coagulation. We thus introduce an error term, denoted by $\epsilon$, to capture the effects of all unmodeled factors and measurement error. The implied generative process of a sample of $i = 1, \ldots, n$ observations is thus:

$$
\begin{align}
  y_i = x_i'\beta + \varepsilon_i = \sum_j x_{i,j} \beta_j + \varepsilon_i , i=1,\dots,n .
  \tag{7.2}
\end{align}
$$

or in matrix notation

$$
\begin{align}
  y = X \beta + \varepsilon .
  \tag{7.3}
\end{align}
$$

This figure illustrates the linear regression fit of our problem 

```{r, echo=FALSE}
data(mtcars)
mtcars <- mtcars %>% mutate(mpg=mpg*10)
simple_model <- lm(mpg ~ wt, data = mtcars)
mtcars_new <- mtcars %>% 
  mutate(cars = rownames(mtcars),
         fitted = fitted(simple_model),
         residuals = residuals(simple_model)) %>%
  dplyr::select(cars, mpg, wt, fitted, residuals)
ggplot(mtcars_new, aes(x = wt, y = mpg)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +   
  geom_segment(aes(xend = wt, yend = fitted), alpha = .2) + 
  geom_point() +
  geom_point(aes(y = fitted), shape = 1) +
  xlab("Drug dose (mg)") + ylab("Blood coagulation level (IU/mL)")
  ggtitle("Blood coagulation as a function of drug doses")

```

Model parameters can be estimated by solving the Ordinary Least Squares (OLS) problem 


$$
\begin{align}
  \hat \beta= \text{argmin}_\beta \{ \sum_i (y_i-x_i'\beta)^2 \},
  \tag{7.4}
\end{align}
$$


and in matrix notation


$$
\begin{align}
  \hat \beta= \text{argmin}_\beta \{ \Vert y-X\beta \Vert^2_2 \}.
  \tag{7.5}
\end{align}
$$

This minimization problem has an unique solution given by:


$$\widehat{\mathbf{\beta}}=[\mathbf{X}^T\mathbf{X}]^{-1}\mathbf{X}^T\mathbf{y}=\mathbf{R}^{-1}\mathbf{Q}^T\mathbf{y}$$

## OLS estimation in R

We are now ready to estimate some linear models with R. We will use the `airquality` data from the `datasets` package (installed by default), that contains daily air quality measurements in New York, May to September 1973.

```{r}
head(airquality)
``` 

We carry out the OLS estimation to investigate whether temperature has any influence on ozone levels

```{r}
mod <- lm(Ozone ~ Temp, data=airquality)
summary(mod)
```

We can visualize our results using `ggplot2` as follows:

```{r}
library(ggplot2)
p <- ggplot(data = airquality, aes(x = Temp, y = Ozone)) +
  geom_smooth(method = "lm", se=FALSE, color="darkred") +
  xlab("Temperature") + ylab("Ozone") +
  geom_point()
p
```


# Generalized linear models

Let us consider other real data problems. 

**Problem 1**: Consider the relation between cigarettes smoked, and the occurrence of lung cancer. Do we expect the probability of cancer to be linear in the number of cigarettes? I do not think so. In those situations, mainly when the outcome variable does not follow a Normal distribution, generalized linear models (GLMs) are required. 

**Problem 2**: Consider the relation between the travel times to the distance traveled. Do you agree that the longer the distance traveled, then not only the travel times get longer, but they also get more variable?

In the linear models, we assumed the generative process to be linear in the effects of the predictors. Let us now write that same linear model, slightly differently in order to facilitate its extension:

$$
y|x \sim \mathcal{N}(x'\beta, \sigma^2).
$$
This model not allow for the non-linear relations of Problem 1, nor does it allow for the distribution of $\epsilon$ to change with $x$ as in the Problem 2. This is how GLMs works by generalizing linear models to address these two limitations. 

For Problem 1, we would like something like

$$
y|x \sim Binom(1,p(x))
$$

and for Problem 2

$$
y|x \sim \mathcal{N}(x'\beta,\sigma^2(x)),
$$

or more generally

$$
y|x \sim \mathcal{N}(\mu(x),\sigma^2(x)),
$$

or maybe not Gaussian

$$
y|x \sim Pois(\lambda(x)).
$$

Even more generally, for some distribution $F(\theta)$, with a parameter $\theta$, we would like to assume that the data is generated via

$$
\begin{align}
  \tag{8.1}
  y|x \sim F(\theta(x))
\end{align}
$$

GLMs assume the data distribution $F$ to be in a “well-behaved” family known as the [Natural Exponential Family](https://en.wikipedia.org/wiki/Natural_exponential_family) of distributions. This family includes the Gaussian, Gamma, Binomial, Poisson, and Negative Binomial distributions. These five include as special cases the exponential, chi-squared, Rayleigh, Weibull, Bernoulli, and geometric distributions.

GLMs also assume that the distribution’s parameter, $\theta$, is some simple function of a linear combination of the effects. In our cigarettes example this amounts to assuming that each cigarette has an additive effect, but not on the probability of cancer, but rather, on some simple function of it. Formally

$$
g(\theta(x))=x'\beta,
$$

and we recall that

$$
x'\beta=\beta_0 + \sum_j x_j \beta_j.
$$

that corresponds to the linear predictor, and $g$ function is known as the **link** function that for Normal data is the identity, for binomial data (i.e. binary) is the logistic function and for count data (Poisson) is the logarithm function. 


## Logistic regression with R

The `predimed` data set provides information on the PREDIMED trial (Prevención con Dieta Mediterránea) which is a randomized, parallel and multicentric cohort with more than 7,000 participants who were randomly assigned to three diet groups (olive oil + mediterranean  diet, nuts + mediterranean diet, and low-fat diet -control group-). It also includes information about whether the individual suffered from different adverse events (stroke, cardiovascular, myocardial infarction) after a 7 years follow-up.

```{r}
library(compareGroups)
data(predimed)
head(predimed)

table(predimed$group)
```

Let us fit a logistic regression model to investigate whether mediterranean diet (variable `group`) is associated with the risk of having and adverse event (variable `event`)

```{r}
mod <- glm(event ~ group, data=predimed, family="binomial")
summary(mod)
```
Let us see how the results change after adjusting by other covariates

```{r}
mod.adj <- glm(event ~ group + sex + age + smoke,
               data=predimed, family="binomial")
summary(mod.adj)
```


<!--chapter:end:02-glm.Rmd-->

# Model Fitting

This chapter covers a range of additional topics related to model fitting, such as variable selection, model comparison, cross-validation and missing data imputation.  It culminates with a discussion of ridge and LASSO regression, two useful regression-based machine learning techniques for automatically selecting variables in high dimensional data so as to balance the bias-variance trade-off. The concepts and methods discussed here apply to both linear and logistic regression. 

Additional resource:

- [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/index.html).  See chapters 5 and 6. 


## Getting started

Before starting showing how to perform data modelling in the context of linear regression (NOTE: everything applies to logistic regression), let us start by implementing some functions that will be required to evaluate model performance. 

A multivariate linear model with an outcome, $y$, and $p$ predictors $x$ can be written as:

$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i,
$$
where $i = 1, \ldots, n.$

The subscript in this equation, $i$, indexes the $n$ observations in the dataset. (Think of $i$ as a row number.) The equation can be read as follows:  the value of $i^{th}$ outcome variable, $y_i$, is defined by an intercept, $\beta_0$, plus a slope, $\beta_1$, multiplied by the $i^{th}$ predictor variable, $x_i$.   These elements define the *systematic* or *deterministic* portion of the model. However, because the world is uncertain, containing randomness, we know that the model will be wrong (as George Box said).  To fully describe the data we need an error term, $\epsilon_i$, which is also indexed by row. The error term is the *stochastic* portion of the model.   $\epsilon_i$ measures the distance between the fitted or expected values of the model---calculated from the deterministic portion of the model---and the actual values. The errors in a linear model---also known as  model residuals---are the part of the data that remains unexplained  by the deterministic portion of the model.  One of the key assumptions of a linear model is that the residuals are normally distributed with mean = 0 and variance = $\sigma^2$, which we denote, in matrix notation, as $N(0,\sigma^2)$. 


The model performance can be summarized with




$$
\operatorname{RSS} = \sum_{i=1}^n ((\beta_0 + \beta_1x_i) - y_i)^2 = \sum_{i=1}^n (\hat{y}_i - y_i)^2
$$

A related measure is root mean squared error (RMSE), the square root of the average of the squared errors: 


$$
\operatorname{RMSE}= \sqrt{\frac{\sum_{i=1}^n ((\beta_0 + \beta_1x_i) - y_i)^2}{n}} 
$$

$$
= \sqrt{\frac{\sum_{i=1}^n (\hat{y}_i - y_i)^2}{n}}
$$

The nice thing about RMSE is that, unlike RSS, it returns a value that is on the scale of the outcome.

$R^2$ is another measure of model fit that is convenient because it is a standardized measure---scaled between 0 and 1---and is therefore comparable across contexts.

$$
R^2 = 1 - \frac{SS_\text{resid}}{SS_\text{tot}}, 
$$
where $SS_\text{tot}=\sum_i (y_i-\bar{y})^2$ and $SS_\text{res}=\sum_i (y_i - \hat{y}_i)^2$.  In words:  $R^2$ represents the variation in the outcome variable explained by the model as a proportion of the total variation.  In the plot below, the left hand panel, TSS, serves as the denominator in calculating $R^2$, and the right hand panel, RSS, is the numerator.



Next R code illustrates how to implement such measurements and one example with Hitters dataset which contains information of the performance statistics and salaries of major league baseball players in the 1986 season.  It includes information about Salary, which is our outcome, and some predictor: hits, years in the league, home runs, RBIs, walks and assists.


```{r}
library(ISLR)
data(Hitters)


rss <- function(fitted, actual){
  sum((fitted - actual)^2)
}

rmse <- function(fitted, actual){
  sqrt(mean((fitted - actual)^2))
}

R2 <- function(fitted, actual){
  tss <- sum((actual - mean(actual))^2)
  rss <- sum((actual - fitted)^2)
  1 - rss/tss
}

display(h <- lm(Salary ~ Hits, data= Hitters))

rss(fitted(h), na.omit(Hitters$Salary))
rmse(fitted(h), na.omit(Hitters$Salary))
R2(fitted(h), na.omit(Hitters$Salary))
```




## General rules for variable selection 

How do we know which variables belong in a model?  The short answer is:  we often don't.  Here are some rules of thumb when thinking about variable selection:^[For further discussion, see Gelman chapter 4, page 69.]

- *Think about the data*.  What variables does it make sense to include given the situation?  Does any published literature offer guidance? If we are in descriptive mode then we may only care about certain variables and use the others as controls.  If we are in predictive mode then we include all variables that, for substantive reasons, might be important in predicting the outcome.  This is very general guidance, however, as different contexts demand different approaches to model fitting.

- *Include quadratic terms if there is evidence from bivariate plots of a non-linear relationship between predictor and outcome.*  In general, we don't include polynomial terms with degrees greater than 2. To do so risks overfitting.

- *Look for possible interactions  among variables with the largest main effects.*  In general we don't include higher order interactions (greater than 2) unless we have a sensible rationale and can explain it (to ourselves and to our audience). 2 way interactions are hard enough to explain.

- *Consider combining separate predictors into a single predictor---a "total score"---by summing or averaging them.*

- *Keep it simple.* Parsimonious models are almost always better---they are more interpretable and tend to have lower variance.


## Stepwise variable selection

The traditional technique in statistics for selecting variables is *stepwise selection*.

With *forward selection* we start with a null model (intercept only) and add one variable at a time. If the added variable improves the model, then we keep it in and add another.  We continue until all variables have been tested. See this figure


![Forward selection](figures/fwd_stepwise.png){width=40%}

With *backward selection* we start with a full model (all available terms), and serially remove variables.  If the model is better after a variable has been removed, then we leave it out.  We continue until all variables have been tested. See this figure

![Backward selection](figures/bwd_stepwise.png){width=40%}


*Forward selection followed by backward selection*. Select forward then backward.

Unfortunately these hand-fitting procedures are flawed. They depend on the order in which variables are added or excluded and often will not select the best model.  Furthermore, in the Boston data there are $k$ = 13 predictor variables, which means there are $2^k$ or `r 2^13` possible models we could fit, not even including interactions or polynomial terms. This is an extremely large space to search through to find the best model, and the search is computationally expensive and time consuming. Conducting such a search manually would be impossible.  


## Comparing models

We are already familiar with $R^2$, RMSE and RSS as tools for comparing models.  In general, if we add a variable and $R^2$ goes up and RMSE/RSS goes down, then the model with the additional variable is better. The amount of unexplained variance has decreased.  However, there is a danger of overfitting. As we've seen, adjusted $R^2$ penalizes the fit for the number of predictors. Likewise, information criterion methods like AIC (Akaike Information Criterion) penalize the fit for model complexity, defined as the number of predictors. 

$$\mathrm{AIC} = - 2\ln(L) + 2k$$
where $k$ the number of estimated parameters in the model, $L$ is the maximized value of the likelihood function for the model, and $ln$ is the natural log. Given a set of candidate models for the data, the preferred model is the one with the lowest AIC value.  In penalizing for larger $k$ (ensured by the final term, $+2k$), AIC attempts to guard against overfitting. It is possible, then, to see $R^2$ go up with the addition of predictors, while AIC goes down.  

We can also compare models with a formal statistic test using the likelihood ratio test (LRT):  
$$
2 \times [ \ln(L_{a}) - \ln(L_{c}) ]
$$
where $\ln(L_{c})$ is the log likelihood of the current model and $\ln(L_{a})$ is the log likelihood of the alternative model with additional predictors. The `lrtest()` function in the lmtest package implements the LRT.  The `anova()` function in base R will also compare models using an f-test, with results that will be virtually identical to the LRT.

Here are some examples of model comparison using the Hitters data from the ISLR package. We start with a null model of Salary:

```{r}
library(ISLR); data(Hitters)
display(null <- lm(Salary ~ 1, data = Hitters))
round(mean(Hitters$Salary, na.rm = T),2)
```

A null model consists only in an intercept, the coefficient of which, as we can see, is just the mean of Salary.  (Note that in order to calculate the mean of Salary we needed to remove the missing values. `lm()` silently removes the missing values: `display()` reports $n = 263$, whereas the dataset has 322 rows.) The key question as we make a model more complex is whether that complexity is justified, whether adding predictors not only lowers the bias but does so without unduly increasing the potential variance. Let's add predictors.

```{r}
library(lmtest)
display(h1 <- lm(Salary ~ Hits, data = Hitters))
lrtest(null, h1)
anova(null, h1)
as.matrix(AIC(null, h1))
```

Hits is statistically significant, since the 95% CI does not include 0 (4.39 $\pm$ 2 x .56).  These three methods agree that the model with Hits is an improvement over the null model.  In the case of `lrtest()` and `anova()` the p-value represents the results of a statistical test (chi-squared test and f-test, respectively) for whether the second, more complex model is a better fit to the data.  Does adding an additional predictor, AtBat, improve the model further?

```{r}
display(h2 <- lm(Salary ~ Hits + AtBat, data = Hitters))
lrtest(h1, h2)
anova(h1, h2)
as.matrix(AIC(h1, h2))
```

The results are ambiguous.  R-squared goes up, while AIC, log likelihood and RSS go down, but the decline in the latter two cases is not statistically significant.  (This result is consistent with the fact that AtBat is not itself statistically significant, since the 95% CI for AtBat includes 0: -1.22 $pm$ 2 x .64.) Should we leave AtBat in the model? It doesn't improve the fit much, if at all, while adding complexity.  So, we should take it out.  Unfortunately such choices are often not clear, which is why model fitting sometimes seems more like an art than a science.

To implement forward selection, we would keep adding variables and comparing models using `lrtest()` or `anova()` trying to find the best possible fit.  One problem with this procedure, however, is that the order in which we step through predictors will impact our selection decisions because each predictor's impact on model fit is contingent on  the presence of the others. For example, suppose we had added AtBat later in the selection process:

```{r}
h3 <- lm(Salary ~ Hits + Years + HmRun + RBI + Walks + Assists, data = Hitters)

h4 <- lm(Salary ~ Hits + Years + HmRun + RBI + Walks + Assists + AtBat, data = Hitters)

lrtest(h3, h4)
as.matrix(AIC(h3, h4))
```

Now AtBat clearly improves the fit, but we would never have discovered that had we already thrown it out.  This is troubling.  Is there a better way?  Perhaps.

## Automatic variable selection

Algorithms have been developed to search model space efficiently for the optimal model. A caution about automatic variable selection is in order at the outset, however.  *Choosing variables should not be a mechanical process.*  We should, instead, seek to understand the data generating process.  Indeed, the greatest benefit of manual stepwise selection consists less in producing a good model than in the understanding gained by fitting many models, and seeing, through trial and error, which predictors are most reactive with the outcome. Especially when it comes to description, automatic variable selection algorithms are just tools for exploring your data and thinking about models. 

The `step()` function in base R automates stepwise variable selection using AIC.

```{r}

display(step(lm(Salary ~ ., data = Hitters), trace = F, direction = "forward"))

```
Forward selection settled on 19 predictors with model $R^2$ of .55.


```{r}

display(step(lm(Salary ~ ., data = Hitters), trace = F, direction = "backward"))

```

Backward selection settled on 10 predictors with $R^2$ of .54.  (The default setting in `step()` for direction is "both," which returns the same result as the above.)  This function certainly simplifies stepwise variable selection, but even the automated stepwise algorithm is not guaranteed to return the optimal model, as the result still depends on the sequence in which variables are entered into the model.  Moreover, the fact that backward selection returned such a different model is concerning.  Ideally, we do not want our model to depend on a methodological choice---we just want the best model.  And in this case, while the larger model has a marginally higher $R^2$, it is also much more complicated:  does the better fit justify the additional complication?  Probably not.  With the bigger model we have likely crossed the line into overfitting, an issue we will take up when we discuss cross-validation.

The `regsubsets()` function in the leaps package performs exhaustive search of the model space using the leaps algorithm for variable selection.

```{r}
library(leaps)

plot(regsubsets(Salary ~ ., data = Hitters, method = "exhaustive", nbest = 1))


```

The plot presents multiple candidate models organized by BIC on the y-axis.  Like AIC, BIC penalizes for model complexity.^[$\mathrm{BIC} = {\ln(n)k - 2\ln({L})},$ where $L$ is the maximum likelihood value, $n$ is the number of observations, $k$ is the number of parameters, and $ln$ is the natural log.]  Lower BIC is better.  The model with the lowest BIC is the rather simple one at the top of the plot:  Intercept, AtBat, Hits, Walks, CRBI, DivisionW and PutOuts.  If we refit a model with these predictors using `lm()` we find it has an $R^2$ of .51.  

Is this model really better?  The algorithm did an exhaustive search of the model space yet returned a model with lower $R^2$!  How could that be better?  But it probably is better.   While the bias in this model will be higher than in the larger model selected by the `step()` function, the variance is likely lower.  Remember:  bias refers to in-sample model performance and variance refers to the out-of-sample model performance---how the model does when it encounters new data.  If the model performs poorly on new data, with a big discrepancy between  in-sample and out-of-sample performance, then it is overfitting.  AIC, BIC, and adjusted R-squared all penalize for model complexity in order to avoid overfitting and will tend to select models with higher bias and lower variance.

## Cross validation 

Cross validation (CV) is the technique we use to assess whether a model is overfitting and to estimate how it will perform on new data.  

Overfitting is a major hazard in predictive analytics, especially when using machine learning algorithms like random forest which, without proper tuning, can learn sample data almost perfectly, essentially fitting noise.  When such a model is used to predict new data, with different noise, model performance can be shockingly bad.  We use CV to help us identify and avoid such situations.  How so?  Many machine learning algorithms require the user to specify certain parameters.  In the case of random forest, for example, we need to specify values for $m$, the number of randomly chosen predictors to be used at each tree split.  The lower the $m$, the simpler the tree. We can use CV to choose the value of $m$ that  minimizes variance and reduces overfitting.  Linear regression has no user-specified parameters, but CV still helps us assess how much a model might be overfitting the sample data.  

The simplest version of CV is the so-called validation set method, consisting in the following steps:


1. *Split the sample data into two parts: a train set and a test set.*  Researchers use different proportions, but it is common to randomly select 70% of the data as the train set and 30% as the test or validation set.  (Obviously, we must have enough data in the sample to fit a model after splitting the data.)  Because CV relies on random sampling, our results will vary unless we use `set.seed()`. We will demonstrate using the Hitters data, using only complete cases.


```{r}
set.seed(123)
Hitters_complete <- Hitters[complete.cases(Hitters), ]
rows <- sample(nrow(Hitters_complete), .7 * nrow(Hitters_complete))
train <- Hitters_complete[rows, ]
test <- Hitters_complete[-rows, ]
```

2.  *Fit a model on the training set* using an appropriate variable selection procedure. We will create two models for comparison:  one with all the variables, then one with just the variables chosen by `regsubsets()`.

```{r}
full_model <- lm(Salary ~., data = train)

select_model <- lm(Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts, data = train)
```

3.  *Use that model to predict on the testing set.*  Performance on the test set is the CV estimate for the model's out-of-sample performance. 


```{r}
results <- data.frame(Model = c("Full model in-sample",
                                "Select model in-sample",
                                "Full model out-of-sample",
                                "Select model out-of-sample"),
                      RMSE = round(c(rmse(fitted(full_model), train$Salary),
                               rmse(fitted(select_model), train$Salary),
                               rmse(predict(full_model, newdata = test), test$Salary), 
                               rmse(predict(select_model, newdata = test), test$Salary)),1))

results
```
We can see that the full model is overfitting---in-sample RMSE is worse than out-of-sample RMSE---while the select model chosen by `regsubsets()` using BIC is not overfitting.  In fact, the select model actually does better out-of-sample than in-sample, though this particular result is likely a matter of chance, a function of random split we happen to be using. Generally, though, these results illustrate the danger of model complexity, and why it makes sense to choose predictors using measures of model fit that penalize for complexity.  Simple models tend to generalize better.  This figure depicts these relationships:

![](figures/overfit.png)

As model complexity increases, the in-sample fit will likely keep getting better and better.  But the out-of-sample fit starts getting worse at a certain threshold of complexity, as the model begins fitting noise in the sample.  CV is designed to identify that threshold.


## Cross-validation and Bootstrap


The problem with this train-test CV procedure is that results can be quite variable due to the single random split defining the two sets.  $K$-fold CV is designed to solve this problem.  From *Statistical Learning*:

> This approach involves randomly dividing the set of observations into $k$ groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining $k − 1$ folds. The mean squared error, $MSE_1$,is then computed on the observations in the held-out fold. This procedure is repeated $k$ times; each time, a different group of observations is treated as a validation set. This process results in $k$ estimates of the test error, $MSE_1,MSE_2,...,MSE_k$. The $k-fold$ CV estimate is computed by averaging these values:  $CV_k = \sum_{i=1}^{k}MSE_i.$(181)

There are different types of CV that we briefly describe here.

### Leave-one-out cross validation (LOOCV)

This method works as follows:

   - Extract one observation from the data and use the rest to train the model
   - Tests the model with the observation that has been extracted in the previous step and save the error associated with that prediction
   - Repeat the process for all observations
   - Calculate the global prediction error using the average of all the errors estimated in step 2.


We will see later how to do these calculations with a specific library. For now, for you to learn how this methodology works, you must perform the following exercise

>
**EXERCISE** (Deliver at Moodle: Exercise-LOOCV): Upload the R function.
>
Create an R function that performs the LOOCV procedure and estimates the LOOCV value for the full model (e.g object `full_model`) and the selected model (e.g. object `select_model`) in the train data.
>
HINT: use the function `update ()` to re-evaluate the model in a new dataset.
>


### K-fold cross validation (K-fold CV)

The difference with LOOCV is that this method evaluates the behavior of the model in a data set of different size (K). The algorithm is as follows:

   - Separate the data into k-subsets (k-fold) randomly
   - Save one of the subsets of data and train the model with the rest of the individuals
   - Tests the model with the reserved data and saves the average prediction error.
   - Repeat the process until the k subsets have served as test sample.
   - Calculate the average of the k errors that have been saved. This value is the cross-validation error and it helps us to evaluate the behavior of our model as if we were using it in an external database.


The main advantage of this method over LOOCV is the computational cost. Another advantage that is not so obvious is that this method often gives better estimates of model error than LOOCV.

A typical question is how to choose the optimal value of K. Small values of K give biased estimates. On the other hand, large K values are less skewed, but have a lot of variability. In practice, values of k = 5 or k = 10 are normally used, since these values have, empirically, estimated error rates that are not too biased or with too much variance.

As in the previous case, we will see an R package to perform these analyzes efficiently. For now, do the following exercise:


>
**EXERECISE** (Deliver at Moodle: Exercise-Kfold): Upload the R function.
>
Create an R function that performs the K-fold CV procedure and estimates the value of value K-fold CV for the full model (e.g object `full_model`) and the selected model (e.g. object `select_model`) in the train data. The function should have a parameter that depends on K. Give the results for K = 5 and K = 10. 
>
HINT: use the function `update ()` to re-evaluate the model in a new dataset.
>


### Bootstrap 

Instead of dividing our sample into $K$ sub-samples we can carry out a random selection of samples with replacement. These re-samples are called *bootstrap* tambples. This is a technique widely used in statistics to make inference when the distribution of the statistic is unknown. This will be further explained in the next letures, but here you have a simple description of this methodoloty. 

![Boostrap](figures/bootstrap_1.jpg){width=60%}

![Boostrap](figures/bootstrap_2.png){width=60%}


The *bootstrap* procedure applied to regression would be:

  
   - Draw a random sample with replacement of size $ n $ from our data (we have $ n $ observations)
   - Save samples that have not been selected (test data)
   - Train the model with the sample * bootstrap *
   - Tests the model with the test data and saves the average prediction error.
   - Repeat the process $ B $ times
   - Calculate the average of the $ B $ errors that have been saved. This value is the * bootstrap * error and it helps us to evaluate the behavior of our model.

>
**EXERCISE** (Deliver at Moodle: Exercise-bootstrap): Upload the R function.
>
Create a function R that implements the *bootstrap* procedure and estimate the value of this method for  the full model (e.g object `full_model`) and the selected model (e.g. object `select_model`) in the train data.  The function should have a parameter that depends on $B$. Provide the the results for B = 25, B = 50, and B = 100. 
>
HINT: use the function `update ()` to re-evaluate the model in a new dataset.
>



## Example with `caret` library

The `caret` R package (short for Classification And REgression Training) contains functions to streamline the model training process for complex regression and classification problems. [Here](https://topepo.github.io/caret/) there is an excellent bookdown describing how to do machine learning with R using different methods. 

By default `caret` uses 25 bootstrap samples rather than folds to perform model evaluation.  Some data points will be left out of each bootstrap sample; caret uses those as the test set for estimating out-of-sample predictive error.

```{r}
library(caret)
set.seed(123)
train(Salary ~ ., 
      data = train, 
      method = "lm")
```

The output that caret prints to the screen is *not* in-sample RMSE and $R^2$ but is rather the CV estimate of out-of-sample error.  Estimated out-of-sample RMSE for the full model is 391.19.  Let's compare this result to the one for the select model.

```{r}
set.seed(123)
train(Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts, 
      data = train, 
      method = "lm")
```

Estimated out-of-sample RMSE for the select model is 362.17, which roughly agrees with the result we obtained using the validation set method:  the simpler model has lower variance.  And why do we care about lower variance?  Because models that perform better on new data are less yoked to the idiosyncrasies of sample data and presumably doing a better job of describing the characteristics of the population.  Such models are better at both inference and prediction.

>
**EXERCISE** (Deliver Moodle: Exercise-create model):
>
Load Boston data set (from MASS package) in R by executing

```
data("Boston", package = "MASS")
```
>
Our aim is to create a model to predict the median house value (mdev), in Boston Suburbs, using the following predictor variables:
>
* crim: per capita crime rate by town.
* zn: proportion of residential land zoned for lots over 25,000 sq.ft.
* indus: proportion of non-retail business acres per town.
* chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
* nox: nitrogen oxides concentration (parts per 10 million).
* rm: average number of rooms per dwelling.
* age: proportion of owner-occupied units built prior to 1940.
* dis: weighted mean of distances to five Boston employment centres.
* rad: index of accessibility to radial highways.
* tax: full-value property-tax rate per \$10,000.
* ptratio: pupil-teacher ratio by town.
* black: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
* lstat: lower status of the population (percent).
* medv: median value of owner-occupied homes in \$1000s.
>
- Split the data in 70% (train) and 30% (test)
- Create a predictive model using a stepwise procedure
- Provide a goodness-of-fit value of the model using cross-validation in the train set
- Validate the model in the test dataset and compare model performance with the value obtained in the previous step
>
**TO DELIVER**: Do the analyses using R Markdonw and upload the pdf (note: if you can only create the html file, use a web browser to export it to pdf.)
>


## Missing data imputation

Real-world datasets often have missing observations.  The `lm()` function, for better or worse, silently removes rows with missing observations.  Should we remove these rows or impute the missing observations?  We are almost always better off imputing.^[Practically speaking, though, imputing a few missing observations may not be worth the trouble since removing them will not usually change the fit at all.]  While we can choose whether to impute in the case of linear regression, many machine learning applications require complete datasets: so we must impute.  Missing data imputation is a large and complicated topic; the following discussion is very introductory.

Types of missing values:

- *Missing completely at random (MCAR)*: the probability that an observation is missing is the same for all cases. Deleting missing cases in this instance will not cause bias, though we may lose information. 

- *Missing at random (MAR)*: the probability that an observation is missing depends on a known mechanism.  For example, some groups are less likely to answer surveys. If we know  group membership we can delete the missing observations provided we include group as a factor in a regression. However,  we can generally do better than just deleting such cases. 

- *Missing not at random (MNAR)*:  the probability that an observation is missing depends on some unknown mechanism---an unobserved variable. Dealing with MNAR problems is difficult or even impossible. 


In this discussion we we will focus on MAR problems.  A simple solution is to fill in or *impute* the MAR values.  There are two major strategies:

**Single imputation** replaces missing values based on a univariate statistic or a multivariable regression model. The caret package will do single imputation with medians, KNN regression or random forest. The missForest package will do single imputation using random forest. In single imputation using medians we impute missing data using the median of the univariate column vector. (The median is better than the mean when the column data are skewed.) In single imputation using KNN or random forest we create a multivariable model of the missing observations using the other column vectors and use that model to predict the missing values. 

The problem with single imputation, theoretically, is that the variability of the imputed variable is lower than the variability in the actual variable would have been, creating a bias towards 0 in the coefficients. Thus, while deletion loses information, single imputation can cause bias. (It is not clear to me, however, how big a problem this actually is in practice.)


**Multiple imputation** addresses these problems by imputing missing values with a multivariable model but adding the variability back in by re-including the error variation that we would normally see in the data. The "multiple" in multiple imputation refers to the multiple datasets created in the process of estimating regression coefficients. The steps are as follows:

1. Create $m$ complete datasets with imputed missing values. Imputations are done by randomly drawing from distributions of plausible values for each column vector.
2. Fit a linear model on each imputed dataset,and store $\hat\beta$s and SEs.
3. Average the $\hat\beta$s and combine the SEs to produce coefficients based on multiply imputed datasets.^[Specifically,  $\hat\beta_{j} = \frac{1}{m} \sum_{i} \hat\beta_{ij}$ and $s^2_j = \frac{1}{m} \sum_{i} s^2_{ij} + var \hat\beta_{ij} (1 + 1/m)$, where $\hat\beta_{ij}$ and $s_{ij}$ are the estimates of and standard errors for the $i^{th}$ imputed result for $i = 1,..., m$ and for the $j^{th}$ parameter.] 

Multiple imputation works better for description than  for prediction, and is probably preferrable to single imputation if we only want to estimate coefficients. For prediction it will usually be necessary to use single imputation. 

We will demonstrate imputation methods using the Carseats data from the ISLR package. This is a simulated dataset of carseat sales, from which we will randomly remove 25% of the observations using the `prodNA()` function in the missForest package (taking care to leave the outcome variable, Sales, intact).

```{r}
data(Carseats)
levels(Carseats$ShelveLoc) <- c("Bad","Medium","Good") # Relevel the factor
library(missForest)
set.seed(123)
carseats_missx <- prodNA(Carseats[,-1], noNA=.25)
carseats_miss <- cbind(Sales=Carseats[, 1], carseats_missx)
glimpse(carseats_miss)
```

There are now many missing observations.  When we fit a regression model of Sales, notice that `lm()` silently removes the rows with NAs, producing a model based on a very small subset of the data.

```{r}
display(lm(Sales ~ CompPrice + Income + Advertising + Population + Price, data = carseats_miss))
```

Out of an original dataset of 400 we now only have 82 rows!  

We will demonstrate multiple imputation using  the `mice()` function from the mice package. (mice stands for "multiple imputation using chained equations.")

```{r}
library(mice)
names(Carseats)
mice_imp <- mice(carseats_miss, printFlag = F)
```

The `carseats_imp` object created by `mice()` includes (among many other things) $m$ imputed datasets (the default setting in mice is m = 5).  The imputed datasets differ because the imputations are randomly drawn from distributions of plausible values. We can visualize the variability of the predictors in these imputed datasets using the `densityplot()` function.

```{r}
library(lattice)
densityplot(mice_imp)
```

The solid blue lines depict the actual distribution of the predictors, while the red lines show the imputed distributions.  The next step is to use these imputed datasets to average the $\hat\beta$s and SEs using mice's `pool()` function.


```{r}
mice_model_imp <- with(data = mice_imp, 
     exp = lm(Sales ~ CompPrice + Income + Advertising + Population + Price))

(mi <- summary(pool(mice_model_imp))[, 2:6])
```

These coefficients are similar to the ones from the earlier model fitted using the non-imputed data, but they should be closer to population values because, rather than just removing the incomplete cases, instead uses distributional information to make educated guesses about missing data.  Multiple imputation works best for purposes of description---estimating coefficients to report in an academic paper, for example---but using it for prediction on new data is awkward or impossible, for the following reasons:

- If the new data is complete then we can use the coefficient estimates derived from multiple imputation in a regression equation for prediction. But this is a pain.    We use the original Carseats data for illustration.  

```{r}
preds <- mi[1, 2] + 
  mi[2, 2]*Carseats$CompPrice +
  mi[3, 2]*Carseats$Income +
  mi[4, 2]*Carseats$Advertising +
  mi[5, 2]*Carseats$Population +
  mi[6, 2]*Carseats$Price
  
   
head(preds)

```

- If the new data is not complete then these multiply imputed coefficients are useless for predicting on rows with missing observations. This, for example, is the result of trying to predict using the carseats data with missing observations.

```{r}
preds <- mi[1, 2] + 
  mi[2, 2]*carseats_miss$CompPrice +
  mi[3, 2]*carseats_miss$Income +
  mi[4, 2]*carseats_miss$Advertising +
  mi[5, 2]*carseats_miss$Population +
  mi[6, 2]*carseats_miss$Price
  
   
head(preds)

```

- Multiple imputation thus doesn't solve the major problem we often face with missing data, which is that although we may have successfully fit a model on the train set, the test set may also have missing observations, and our predictions using that data will also therefore be incomplete.

- We could use one of the imputed datasets produced by mice, but then we are not doing multiple imputation anymore but single imputation.  At that point, the methods available in the mice package offer no special advantage over those in the caret and the missForest packages. Indeed, they might be worse since `mice()` was designed not to produce the single best imputation but rather a range of plausible imputations. 

Using caret, we can do single imputation using knnImpute, medianImpute, or bagImpute (random forest).  While it is possible to impute inside the `train()` function using  `preProcess()`, it is more straightforward to create a new dataset with imputed observatons. These methods only work for numeric variables, so we will create a custom function to turn the factors---Shelveloc, Urban and US---into integers. (When using the imputed dataset for regression we could leave these variables as integers, as long as the integer values correspond to the factor levels.)

```{r}
make_df_numeric <- function(df){
  data.frame(sapply(df, function(x) as.numeric(x)))
  }

carseats_miss_num <- make_df_numeric(carseats_miss)

med_imp <- predict(preProcess(carseats_miss_num, method = c("medianImpute")), carseats_miss_num)

knn_imp <- predict(preProcess(carseats_miss_num, method = c("knnImpute")), carseats_miss_num)

bag_imp <- predict(preProcess(carseats_miss_num, method = c("bagImpute")), carseats_miss_num)

```

The missForest package offers yet another single imputation solution, which is simpler than the caret functions because it handles categorical data automatically.  While missForest works well for small datasets, and provides good quality imputations using multivariable random forest models, it will be very slow on large datasets. In fact, the same will be true for caret's `bagImpute()` function, which also uses random forest.  In such cases it might make sense to use caret's `medianImpute()` function instead.

```{r}
mf_imp <- missForest(carseats_miss, verbose = F)
```

The imputed dataset is stored in a list object (under "ximp").    

Let's compare the errors associated with these different imputation methods. We can do this because, having created the missing observations in the first place, we can compare the imputed observations against the true observations by computing the sum of squares of the difference. For the imputations using `mice()` we calculate errors for each of the 5 imputed datasets.  The results from `knnImpute()` are not comparable  because the function automatically centers and scales variables; they have been omitted.

```{r}

comparison <- data.frame(Method = c("mice 1", 
                                    "mice 2", 
                                    "mice 3", 
                                    "mice 4", 
                                    "mice 5", 
                                    "medianImpute", 
                                    "bagImpute", 
                                    "missForest"),
                         RSS = c(rss(make_df_numeric(complete(mice_imp, 1)), make_df_numeric(Carseats)),
                                 rss(make_df_numeric(complete(mice_imp, 2)), make_df_numeric(Carseats)),
                                 rss(make_df_numeric(complete(mice_imp, 3)), make_df_numeric(Carseats)),
                                 rss(make_df_numeric(complete(mice_imp, 4)), make_df_numeric(Carseats)),
                                 rss(make_df_numeric(complete(mice_imp, 5)), make_df_numeric(Carseats)),
                                 rss(med_imp, make_df_numeric(Carseats)),
                                 rss(bag_imp, make_df_numeric(Carseats)),
                                 rss(make_df_numeric(mf_imp$ximp), make_df_numeric(Carseats))))
                         
comparison %>%
  mutate(RSS = round(RSS)) %>%
  arrange(RSS)

```

Missforest does the best, though medianImpute compares very well!  Mice does not do well, probably for the reasons mentioned above:  it is designed for multiple, not single, imputation.

## Regularization

Selecting variables using AIC or $R^2$ is a discrete process: a variable is either in or out of the model. By contrast, methods are available that regularize or  *shrink*  coefficients towards zero and thereby achieve the same objective as discrete variable selection but in a continuous manner. The method  works particularly well when there are large numbers of predictors. (In the wrong conditions---small number of predictors, for example---regularized models will actually do worse than ordinary least squares regression or OLS regression.)   We will discuss two  methods:  *ridge regression*, which  shrinks coefficients towards each other and towards zero, and *lasso*, which does the same thing but shrinks some coefficients all the way to  zero, effectively taking those predictors out of the model.^[Lasso stands for Least Absolute Selection and Shrinkage Operator.] Ridge regression never completely removes predictors.

Why would we want to shrink coefficients?  Large coefficients tend to be artifacts of chance---of the fact that we happened to get this sample rather than another one. The world is a complex place, with many intersecting influences; it does not abound in strong relationships. Shrinking large coefficients will generally produce a model with higher bias but lower variance.  We select a *worse* model in-sample so as to have a *better* model out-of-sample. Regularized models are particularly well-suited, consequently, for prediction problems.

Ridge regression shrinks  regression coefficients towards each other and towards zero by constraining their size. Remember: the least squares line in OLS regression is defined by  the $\beta_0$ and $\beta_j$ that minimize RSS:


$$
\min_{ \beta_0, \beta_j }\left\{  \frac{1}{N} \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} )^2 \right\} = \min_{ \beta_0, \beta_j }\left\{RSS\right\}
$$ 

We can think of the least squares algorithm as searching a large space of possibilities for the values of  $\beta_0$ and $\beta_j$ that produce the lowest RSS.  Ridge regression does the same thing thing but imposes a *shrinkage* penalty on RSS. 

$$
\min_{ \beta_0, \beta_j }\left\{RSS + \lambda \sum_{j=1}^p \beta{_j^2} \right\}
$$

where $\lambda$ is a tuning parameter.  From *Statistical Learning*:

> As with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the $RSS$ small. However, the second term, $\lambda \sum_j \beta{_j^2}$, called a shrinkage penalty,is small when $\beta_1, ... , \beta_j$ are close to zero, and so it has the effect of shrinking the estimates of $\beta_j$ towards zero. The tuning parameter $\lambda$ serves to control the relative impact of these two terms on the regression coefficient estimates. When $\lambda$ = 0, the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as $\lambda \rightarrow \infty$, the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero. Unlike least squares, which generates only one set of coefficient estimates, ridge regression will produce a different set of coefficient estimates, $\hat\beta^r_\lambda$, for each value of $\lambda$. Selecting a good value for $\lambda$ is critical. [For that we use cross- validation.] (215)

Let's examine how shrinkage works in practice.  Consider a simple regression model with $\beta_0$ = -1 and $\beta_1$ = 2.


```{r}

x <- c(1,2,3,4)
y <- c(1,4,3,8)
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point() +
  stat_smooth(method = "lm", se = F) +
  ggtitle("OLS line: intercept = -1, slope = 2")


```

The following table relates $\beta_1$ to  RSS for three models:  the OLS model from above (column 2) and then two different ridge models of the same data with different $\lambda$ (columns 3 and 4):

```{r}
tab <- data.frame(Slope = seq(1.75,2.25,.05), rss = 0, rss2 = 0, rss3 = 0)

names(tab)[2:4] <- c("OLS RSS", "Ridge RSS (lambda = 1)",  "Ridge RSS (lambda = 2)")

for(i in 1:nrow(tab)){tab[i,2] <- round(sum((-1 + tab$Slope[i]*x - y)^2) + 0*tab$Slope[i]^2, 2)}

for(i in 1:nrow(tab)){tab[i,3] <- round(sum((-1 + tab$Slope[i]*x - y)^2) + 1*tab$Slope[i]^2, 2)}

for(i in 1:nrow(tab)){tab[i,4] <- round(sum((-1 + tab$Slope[i]*x - y)^2) + 2*tab$Slope[i]^2 , 2)}


tab
```
The $\beta_1$ that minimizes RSS for the OLS model is 2.  (OLS is identical to a ridge model with $\lambda$ = 0.)  For the ridge models we can see that as $\lambda$ increases from 1 to 2, the shrinkage penalty grows, which has the effect of selecting smaller $\beta_1$s.  When $\lambda$ = 1 the optimal $\beta_1$ is 1.95, and when $\lambda$ = 2 the optimal $\beta_1$ somewhere between 1.85 and 1.9.

```{r}

x <- c(1,2,3,4)
y <- c(1,4,3,8)
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point() +
  stat_smooth(method = "lm", se = F) +
  geom_abline(slope = 1.95, intercept = -1, lty = 2) +
  geom_abline(slope = 1.9, intercept = -1, lty = 2) +
  ggtitle("OLS line compared to ridge estimates for lambda = 1 and lambda = 2")


```

Another way to think about ridge regression is that it minimizes RSS subject to a constraint, $t$, on the size of the square root of the squared and summed $\beta$ coefficients:

$$
\min_{ \beta_0, \beta_j }\left\{RSS \right\} \text{ subject to } \sum_{j=1}^p ||\beta_j||_2 \leq t
$$

$||\beta_j||_2$ is the $L_2$ or Euclidean norm:  $\left\| \boldsymbol{x} \right\|_2 := \sqrt{x_1^2 + \cdots + x_n^2}$. The constraint is like a budget that ensures the $\beta$ coefficients never get larger than a certain size. We pick the optimal $t$, just as we would the optimal $\lambda$, through cross validation.  We seek the value of $t$ that minimizes estimated out-of-sample penalized error.

Lasso regression also shrinks  regression coefficients by constraining their size, but uses absolute value of $\beta_j$ in the penalty term. In technical terms:  lasso uses the $L_1$ norm instead of the $L_2$ norm. The $L_1$ norm is just the absolute value of the summed $\beta_j$s rather than the squares. 

$$
\min_{ \beta_0, \beta_j }\left\{  \frac{1}{N} \sum_{i=1}^N (y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij} )^2 + \lambda \sum_{j=1}^p |\beta{_j}|_1 \right\} =
$$ 

$$
\min_{ \beta_0, \beta_j }\left\{RSS + \lambda \sum_{j=1}^p|\beta{_j|_1} \right\}
$$

where $\lambda \geq 0$ is again a tuning parameter, which we choose using CV.  Or, just as with ridge, we can think about lasso as minimizing RSS subject to a constraint, $t$, on the size of the absolute value of the summed $\beta$ coefficients:

$$
\min_{ \beta_0, \beta_j }\left\{RSS \right\} \text{ subject to } \sum_{j=1}^p |\beta_j|_1 \leq t
$$

The difference between the $L_2$ norm (used for ridge regression) and the $L_1$ norm (used for lasso) may seem trivial but it accounts for the fact that lasso does not just shrink coefficients towards zero but actually sets some coefficients at zero.  Say, for example, that the constraint  on the coefficients for a model with two predictors is $t =1$. For lasso this means that $|\hat\beta_1|$ + $|\hat\beta_2| \leq$ 1. Examples: 

- $|1| + |0| = 1$
- $|.5| + |.5| = 1$
- $|0| + |1| = 1$

We can generalize and say that the shape  of the lasso constraint for any $\hat\beta_1$ + $\hat\beta_2$ subject to $t \leq 1$ will be a square, whereas the shape of the ridge constraint will be a circle. Examples: 

- $1^2 + 0 = 1$
- $.71^2 + .71^2 = .5 + .5 = 1$
- $0 + 1^2 = 1$

<!-- This plot suggests the difference: -->

<!-- ```{r } -->
<!-- points_lasso <- data.frame(beta1 = c(0, .5, 1), -->
<!--                      beta2 = c(1, .5, 0)) -->

<!-- points_ridge <- data.frame(beta1 = c(0, .71, 1), -->
<!--                      beta2 = c(1, .71, 0)) -->

<!-- ggplot(points_lasso, aes(beta1, beta2)) + -->
<!--   geom_path() + -->
<!--   geom_point() + -->
<!--   geom_path(data = points_ridge, mapping = aes(beta1, beta2), col = "red")+ -->
<!--   geom_point(data = points_ridge, mapping = aes(beta1, beta2), col = "red")+ -->
<!--   ggtitle("Shape of lasso constraint (black) vs. ridge constraint (red) when t = 1") -->

<!-- ``` -->

Ridge coefficients will never equal 0 because, due to the circular shape of the constraint, they will always intersect the constraint at points where $\hat\beta_1$ and $\hat\beta_2$ are either greater than or less than 0.  Not so for lasso.  The following graphic from *Statistical Learning* shows the difference.

![](figures/reg.png)

The possible values for $\hat\beta$ will touch the corners of the square (will equal 0) in the case of lasso, but never for ridge: the constraint will always intersect the possibilities for $\hat\beta$  at some non-zero point.

Both lasso and ridge regression models are simple to fit in caret using the `glmnet()` function.  We must center and scale variables to use these methods.

```{r}
set.seed(123)
(glmnet_model <- train(Salary ~ ., 
                   data = train,
                   preProcess = c("center", "scale"),
                   method = "glmnet"))
```

There are two user-specified parameters that caret sets using CV:  lambda and alpha.  Lambda is the shrinkage penalty.  Caret searches over a small set of possibilities in this case---.5, 5, and 50---to find the lambda associated with the best out-of-sample performance, here 48.44.  (We can specify a wider grid search for optimal lambda.)  Alpha represents the "mixing percentage" between ridge and lasso.  By default, `glmnet()` combines ridge and lasso in optimal proportions. We can force `glmnet()` to fit a ridge or lasso regression by specifying alpha = 0 (ridge) or alpha = 1 (lasso). 

```{r}
set.seed(156)
(ridge_model <- train(Salary ~ ., 
                   data = train,
                   preProcess = c("center", "scale"),
                   method = "glmnet",
                   tuneGrid = expand.grid(
                     alpha = 0,
                     lambda = seq(150,200, 10))))

```

Extracting the coefficients for a `glmnet()` model from caret is sort of a pain.  We first need to find the optimal lambda selected through CV, and then use that to pick out the best final model object.  Here is the code:

```{r}
glmnet_model$finalModel$tuneValue
coef(glmnet_model$finalModel, glmnet_model$finalModel$tuneValue$lambda)

```

Two things are going on here.  First, the coefficients for all predictors have been shrunk towards 0, and, second, some predictors have been completely removed from the model by having their coefficients shrunk to all the way to 0.  Is this continuous version of automatic variable selection better than the discrete version we used earlier with `regsubsets()`?  Let's compare predictions on the test set.

```{r}
rmse(predict(select_model, newdata = test), test$Salary)
rmse(predict(glmnet_model, newdata = test), test$Salary)

```


Unfortunately, `glmnet()` did not live up to its billing in this case.  Regularization tends to work best in high dimensional settings where manual variable selection is not possible, or where automatic discrete variable selection does not provide enough flexibility. 

Let's try using `glmnet()` to predict on a more challenging, high-dimensional dataset, the [communities and crime dataset](http://archive.ics.uci.edu/ml/datasets/communities+and+crime) from UC Irvine's machine learning repository.  The data dictionary notes, "the data combines socio-economic data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey, and crime data from the 1995 FBI UCR." There are 147 variables in the dataset with 2215 rows.  We won't bother to add in predictor names.  The final variable in the dataset, ViolentCrimesPerPop, is the outcome. We will exclude the first two columns which function as row names representing the cities and states with crime statistics in this dataset.

```{r}
crime_data <- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/00211/CommViolPredUnnormalizedData.txt", header = F, sep = ",", quote = "\"", dec = ".", fill = TRUE, comment.char = "", na.strings = "?", strip.white=TRUE, stringsAsFactors = F)

any(is.na(crime_data))

```

There are missing observations.  We could use `missForest()` for imputation, but given the high dimensionality of the data, this method will be very slow, if it works at all.  We will instead use caret's `medianImpute()` function for speed.

```{r}
crime_data <- predict(preProcess(crime_data[, -c(1:2)], method = c("medianImpute")), crime_data[, -c(1:2)])

all(complete.cases(crime_data))

set.seed(512)
rows <- sample(nrow(crime_data), .7*nrow(crime_data), replace = F)
crime_train <- crime_data[rows,]
crime_test <- crime_data[-rows,]

crime_lm <- lm(V147 ~., data = crime_train)

crime_glmnet <- train(V147 ~.,
                      data = crime_train,
                      preProcess = c("center", "scale"),
                      method = "glmnet")

rmse(predict(crime_lm, newdata = crime_test), crime_test$V147)
rmse(predict(crime_glmnet, newdata = crime_test), crime_test$V147)

```

In this case the regularized model outperforms the linear model.  But does it outperform a model with discrete automatic variable selection? Exhaustive search using `regsubsets()` would not be computationally feasible.  The model space consists in  $2^{145}$ models. From that perspective, lasso and ridge regression seem like pretty good alternatives. Using the `step()` function, however, remains possible.

```{r}
step_selection <- step(crime_lm, data = crime_train, trace = 0)
rmse(predict(step_selection, newdata = crime_test), crime_test$V147)

```

In this instance, regularized regression outperforms step selection also, which in this case is worse than the linear model with all predictors.  How, additionally, does the regularized model compare to other popular machine learning algorithms like gradient boosting?

```{r}
crime_gbm <- train(V147 ~.,
                   data = crime_train,
                   method = "gbm",
                   verbose = F)

rmse(predict(crime_gbm, newdata = crime_test), crime_test$V147)
```

It does better.  In high dimensional settings, then, regularization is a good choice.


-----------------------------------------------------
**EXERCISE** (Deliver Moodle: Exercise-lasso):

DNA methylation (DNAm) is a biological process by which methyl groups are added to the DNA molecule. Methylation can change the activity of a DNA segment without changing the sequence. In mammals, DNAm is almost exclusively found in CpG dinucleotides, with the cytosines on both strands being usually methylated. There are arrays that can measure 27K CpG sites for a given individual (current methods can obtain information up to 450K or 1M). The CpG variables are continuous variables with values between 0 (hypo-methylation) and 1 (hyper-methylation).

Horvath hypothesized that DNAm age measures the cumulative effect of an epigenetic maintenance system and that it can predict all-cause mortality in later life. 

Using healthy individuals we can create a model to estimate age from DNAm data. Then, this model can be used to predict DNAm age in another individual and use the difference of this value with the biological age (i.e. age acceleration) as a biomarker of death. This task aims to illustrate how to perform such data modelling.

The file `methy_train.txt` (espace delimited) contains information of 261 healthy individuals, their biological age and DNAm data corresponding to 25,978 CpG sites (variables starting by "cg"). Use this data to train a model to predict the age using DNAm data. You can use whatever technique you think is appropriate. You also have access to another file called `methy_test.txt` having the same information for 172 independent individuals. Both data files are available at: https://github.com/isglobal-brge/Master_Modelling/blob/main/data/methy_data.zip 
-----------------------------------------------------

<!--chapter:end:03-model_fitting.Rmd-->

# DataSHIELD

```{r include = FALSE, eval=FALSE}
download.file("https://github.com/isglobal-brge/resource_bookdown/raw/master/02-DataSHIELD.Rmd", "rmd_datashield/DataSHIELD.Rmd")
```

```{r child='rmd_datashield/DataSHIELD.Rmd', include=TRUE}
```


```{r include = FALSE, eval=FALSE}
download.file("https://github.com/isglobal-brge/resource_bookdown/raw/master/01-Opal.Rmd", "rmd_datashield/opal.Rmd")
```

```{r child='rmd_datashield/opal.Rmd', include=TRUE}
```



```{r include = FALSE, eval=FALSE}
download.file("https://github.com/isglobal-brge/resource_bookdown/raw/master/04-resources.Rmd", "rmd_datashield/resources.Rmd")
```

```{r child='rmd_datashield/resources.Rmd', include=TRUE}
```



```{r include = FALSE, eval=FALSE}
download.file("https://github.com/isglobal-brge/resource_bookdown/raw/master/07-workflow2_simple_analyses.Rmd", "rmd_datashield/analyses.Rmd")
```

```{r child='rmd_datashield/analyses.Rmd', include=TRUE}
```

```{r include = FALSE, eval=FALSE}
download.file("https://github.com/isglobal-brge/resource_bookdown/raw/master/13-Useful_information.Rmd", "rmd_datashield/info.Rmd")
```

```{r child='rmd_datashield/info.Rmd', include=TRUE}
```

<!--chapter:end:04-DataSHIELD.Rmd-->

